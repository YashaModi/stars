{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) Ensemble Training Notebook\n",
    "\n",
    "This notebook handles data generation, feature scaling, model architecture definition, and training for the multi-model ensemble. It is designed to be run sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. TensorFlow version: <module 'tensorflow._api.v2.version' from '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/_api/v2/version/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# --- 1. IMPORTS AND SETUP ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Ensure your stars_utils.py file is in the same directory\n",
    "import stars_utils\n",
    "\n",
    "# Set seed globally for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "print('Setup complete. TensorFlow version: ' + str(tf.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. MODEL CONFIGURATION ---\n",
    "TARGET_PLANET = 'jupiter'\n",
    "NUM_MODELS = 1\n",
    "EPOCHS = 5000   \n",
    "TEST_SIZE = 0.2  \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Time Range for Data Generation (Training Data)\n",
    "START_DATE = datetime(1970, 1, 1)\n",
    "END_DATE = datetime(2025, 1, 1)\n",
    "TIME_STEP = timedelta(days=1) \n",
    "\n",
    "# File Path for Scaler Persistence\n",
    "MODEL_DIR = 'models'\n",
    "SCALER_FILEPATH = os.path.join(MODEL_DIR, f'feature_scaler_{TARGET_PLANET}.pkl')\n",
    "\n",
    "print('Configuration loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec28271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generating data for Jupiter...\n",
      "Note: Using 'jupiter barycenter' for target lookup.\n",
      "Dataset for Jupiter created successfully with 20090 data points.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_UTC</th>\n",
       "      <th>Julian_Date</th>\n",
       "      <th>RA_deg</th>\n",
       "      <th>Dec_deg</th>\n",
       "      <th>Distance_AU</th>\n",
       "      <th>X_au</th>\n",
       "      <th>Y_au</th>\n",
       "      <th>Z_au</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2.440588e+06</td>\n",
       "      <td>210.963608</td>\n",
       "      <td>-11.290574</td>\n",
       "      <td>5.744630</td>\n",
       "      <td>-4.830654</td>\n",
       "      <td>-2.898375</td>\n",
       "      <td>-1.124711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>2.440589e+06</td>\n",
       "      <td>211.094562</td>\n",
       "      <td>-11.334160</td>\n",
       "      <td>5.729532</td>\n",
       "      <td>-4.810606</td>\n",
       "      <td>-2.901321</td>\n",
       "      <td>-1.126029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-03</td>\n",
       "      <td>2.440590e+06</td>\n",
       "      <td>211.223534</td>\n",
       "      <td>-11.376928</td>\n",
       "      <td>5.714343</td>\n",
       "      <td>-4.790610</td>\n",
       "      <td>-2.903987</td>\n",
       "      <td>-1.127226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-04</td>\n",
       "      <td>2.440591e+06</td>\n",
       "      <td>211.350491</td>\n",
       "      <td>-11.418872</td>\n",
       "      <td>5.699067</td>\n",
       "      <td>-4.770670</td>\n",
       "      <td>-2.906374</td>\n",
       "      <td>-1.128302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>2.440592e+06</td>\n",
       "      <td>211.475399</td>\n",
       "      <td>-11.459981</td>\n",
       "      <td>5.683707</td>\n",
       "      <td>-4.750791</td>\n",
       "      <td>-2.908484</td>\n",
       "      <td>-1.129258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time_UTC   Julian_Date      RA_deg    Dec_deg  Distance_AU      X_au  \\\n",
       "0 1970-01-01  2.440588e+06  210.963608 -11.290574     5.744630 -4.830654   \n",
       "1 1970-01-02  2.440589e+06  211.094562 -11.334160     5.729532 -4.810606   \n",
       "2 1970-01-03  2.440590e+06  211.223534 -11.376928     5.714343 -4.790610   \n",
       "3 1970-01-04  2.440591e+06  211.350491 -11.418872     5.699067 -4.770670   \n",
       "4 1970-01-05  2.440592e+06  211.475399 -11.459981     5.683707 -4.750791   \n",
       "\n",
       "       Y_au      Z_au  \n",
       "0 -2.898375 -1.124711  \n",
       "1 -2.901321 -1.126029  \n",
       "2 -2.903987 -1.127226  \n",
       "3 -2.906374 -1.128302  \n",
       "4 -2.908484 -1.129258  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. DATA GENERATION, SPLIT, AND SCALING ---\n",
    "\n",
    "print('1. Generating data for ' + TARGET_PLANET.capitalize() + '...')\n",
    "\n",
    "# Use the utility function to get the raw ephemeris data\n",
    "df_raw = stars_utils.generate_planetary_ephemeris_df(\n",
    "    target_planet=TARGET_PLANET, \n",
    "    start_date=START_DATE, \n",
    "    end_date=END_DATE, \n",
    "    time_step=TIME_STEP\n",
    ")\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e8bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated jupiter's Synodic Period with Earth: 398.88 days.\n",
      "Added dynamic features (Time Index, Polynomial, Earth Cycle, Target Cycle, Synodic Cycle, Interaction) to the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "jup_df = stars_utils.add_astronomy_features(df_raw, TARGET_PLANET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ea23261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time_UTC', 'Julian_Date', 'RA_deg', 'Dec_deg', 'Distance_AU', 'X_au',\n",
      "       'Y_au', 'Z_au', 'Time_Index', 'Time_Index_2', 'Time_Index_3',\n",
      "       'Sin_Year', 'Cos_Year', 'Sin_Jupiter', 'Cos_Jupiter', 'Sin_Synodic',\n",
      "       'Cos_Synodic', 'Sin_Year_Sin_Synodic', 'Sin_Year_Cos_Synodic',\n",
      "       'Cos_Year_Sin_Synodic', 'Cos_Year_Cos_Synodic'],\n",
      "      dtype='object')\n",
      "(20090, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_UTC</th>\n",
       "      <th>Julian_Date</th>\n",
       "      <th>RA_deg</th>\n",
       "      <th>Dec_deg</th>\n",
       "      <th>Distance_AU</th>\n",
       "      <th>X_au</th>\n",
       "      <th>Y_au</th>\n",
       "      <th>Z_au</th>\n",
       "      <th>Time_Index</th>\n",
       "      <th>Time_Index_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Sin_Year</th>\n",
       "      <th>Cos_Year</th>\n",
       "      <th>Sin_Jupiter</th>\n",
       "      <th>Cos_Jupiter</th>\n",
       "      <th>Sin_Synodic</th>\n",
       "      <th>Cos_Synodic</th>\n",
       "      <th>Sin_Year_Sin_Synodic</th>\n",
       "      <th>Sin_Year_Cos_Synodic</th>\n",
       "      <th>Cos_Year_Sin_Synodic</th>\n",
       "      <th>Cos_Year_Cos_Synodic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>2.440588e+06</td>\n",
       "      <td>210.963608</td>\n",
       "      <td>-11.290574</td>\n",
       "      <td>5.744630</td>\n",
       "      <td>-4.830654</td>\n",
       "      <td>-2.898375</td>\n",
       "      <td>-1.124711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>2.440589e+06</td>\n",
       "      <td>211.094562</td>\n",
       "      <td>-11.334160</td>\n",
       "      <td>5.729532</td>\n",
       "      <td>-4.810606</td>\n",
       "      <td>-2.901321</td>\n",
       "      <td>-1.126029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.999876</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.999728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-03</td>\n",
       "      <td>2.440590e+06</td>\n",
       "      <td>211.223534</td>\n",
       "      <td>-11.376928</td>\n",
       "      <td>5.714343</td>\n",
       "      <td>-4.790610</td>\n",
       "      <td>-2.903987</td>\n",
       "      <td>-1.127226</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034398</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.999504</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.034381</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.998912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-04</td>\n",
       "      <td>2.440591e+06</td>\n",
       "      <td>211.350491</td>\n",
       "      <td>-11.418872</td>\n",
       "      <td>5.699067</td>\n",
       "      <td>-4.770670</td>\n",
       "      <td>-2.906374</td>\n",
       "      <td>-1.128302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051584</td>\n",
       "      <td>0.998669</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.047239</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.051527</td>\n",
       "      <td>0.047176</td>\n",
       "      <td>0.997554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>2.440592e+06</td>\n",
       "      <td>211.475399</td>\n",
       "      <td>-11.459981</td>\n",
       "      <td>5.683707</td>\n",
       "      <td>-4.750791</td>\n",
       "      <td>-2.908484</td>\n",
       "      <td>-1.129258</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068755</td>\n",
       "      <td>0.997634</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.062967</td>\n",
       "      <td>0.998016</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.068619</td>\n",
       "      <td>0.062818</td>\n",
       "      <td>0.995654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time_UTC   Julian_Date      RA_deg    Dec_deg  Distance_AU      X_au  \\\n",
       "0 1970-01-01  2.440588e+06  210.963608 -11.290574     5.744630 -4.830654   \n",
       "1 1970-01-02  2.440589e+06  211.094562 -11.334160     5.729532 -4.810606   \n",
       "2 1970-01-03  2.440590e+06  211.223534 -11.376928     5.714343 -4.790610   \n",
       "3 1970-01-04  2.440591e+06  211.350491 -11.418872     5.699067 -4.770670   \n",
       "4 1970-01-05  2.440592e+06  211.475399 -11.459981     5.683707 -4.750791   \n",
       "\n",
       "       Y_au      Z_au  Time_Index  Time_Index_2  ...  Sin_Year  Cos_Year  \\\n",
       "0 -2.898375 -1.124711         0.0           0.0  ...  0.000000  1.000000   \n",
       "1 -2.901321 -1.126029         1.0           1.0  ...  0.017202  0.999852   \n",
       "2 -2.903987 -1.127226         2.0           4.0  ...  0.034398  0.999408   \n",
       "3 -2.906374 -1.128302         3.0           9.0  ...  0.051584  0.998669   \n",
       "4 -2.908484 -1.129258         4.0          16.0  ...  0.068755  0.997634   \n",
       "\n",
       "   Sin_Jupiter  Cos_Jupiter  Sin_Synodic  Cos_Synodic  Sin_Year_Sin_Synodic  \\\n",
       "0     0.000000     1.000000     0.000000     1.000000              0.000000   \n",
       "1     0.001450     0.999999     0.015752     0.999876              0.000271   \n",
       "2     0.002900     0.999996     0.031499     0.999504              0.001084   \n",
       "3     0.004351     0.999991     0.047239     0.998884              0.002437   \n",
       "4     0.005801     0.999983     0.062967     0.998016              0.004329   \n",
       "\n",
       "   Sin_Year_Cos_Synodic  Cos_Year_Sin_Synodic  Cos_Year_Cos_Synodic  \n",
       "0              0.000000              0.000000              1.000000  \n",
       "1              0.017199              0.015749              0.999728  \n",
       "2              0.034381              0.031481              0.998912  \n",
       "3              0.051527              0.047176              0.997554  \n",
       "4              0.068619              0.062818              0.995654  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(jup_df.columns)\n",
    "print(jup_df.shape)\n",
    "jup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved successfully to: models/feature_scaler_jupiter.pkl\n",
      "Data split and scaled. Scaler saved.\n"
     ]
    }
   ],
   "source": [
    "FEATURES = [\n",
    "    'Time_Index', 'Time_Index_2', 'Time_Index_3',\n",
    "    'Sin_Year', 'Cos_Year',\n",
    "    'Sin_Jupiter', 'Cos_Jupiter',\n",
    "    'Sin_Synodic', 'Cos_Synodic',\n",
    "    # 'Sin_Year_Cos_Synodic','Cos_Year_Sin_Synodic',\n",
    "    'Sin_Year_Sin_Synodic','Cos_Year_Cos_Synodic'\n",
    "]\n",
    "TARGETS = ['X_au', 'Y_au', 'Z_au']\n",
    "\n",
    "X = jup_df[FEATURES] \n",
    "y = jup_df[TARGETS] \n",
    "\n",
    "# Chronological Train-Test Split\n",
    "split_index = int(len(jup_df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Scaling the features (Julian Date)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "INPUT_SHAPE = X_train_scaled.shape[1] \n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "# Save the fitted scaler for later use in prediction/analysis notebooks\n",
    "stars_utils.save_scaler(scaler, SCALER_FILEPATH)\n",
    "\n",
    "# Create robust, optimized TensorFlow Datasets\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# raw_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train.values))\n",
    "# raw_test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test.values))\n",
    "# test_dataset = raw_test_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "print('Data split and scaled. Scaler saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. MODEL CONFIGURATION AND CALLBACKS ---\n",
    "models = []\n",
    "y_pred_list = []\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=150,\n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "lr_on_plateau_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5,\n",
    "    patience=50,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, lr_on_plateau_callback]\n",
    "best_ensemble_rmse = float('inf')\n",
    "\n",
    "print('Callbacks defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Building and training 1 diverse MLP models...\n",
      "--- Training MLP Model 1 (Seed: 42) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890us/step - loss: 0.0204 - val_loss: 0.0126 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.0041 - val_loss: 0.0127 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0037 - val_loss: 0.0155 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0034 - val_loss: 0.0102 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.0029 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0025 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0023 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0021 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.0015 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 0.0019 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0012 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.0010 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0011 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 8.5856e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 9.0855e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 7.4300e-04 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 7.1955e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 6.5885e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 5.5561e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 7.3076e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.8792e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.9287e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.6971e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 4.8418e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.8303e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 3.8791e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2802e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 3.5191e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.3455e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 3.7340e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 2.8525e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.0809e-04 - val_loss: 8.4553e-04 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 2.9130e-04 - val_loss: 8.7874e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 3.3901e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 3.9349e-04 - val_loss: 7.6958e-04 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 2.4701e-04 - val_loss: 9.7089e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 2.8144e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 3.0102e-04 - val_loss: 7.9015e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 3.2518e-04 - val_loss: 9.7106e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 2.9688e-04 - val_loss: 8.8821e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 2.7859e-04 - val_loss: 6.7039e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 2.5717e-04 - val_loss: 5.6951e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 2.6487e-04 - val_loss: 6.9323e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 2.8374e-04 - val_loss: 6.9020e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 2.4145e-04 - val_loss: 6.7199e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 2.5109e-04 - val_loss: 4.9968e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 2.2101e-04 - val_loss: 5.5881e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 2.7995e-04 - val_loss: 4.7755e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 2.6646e-04 - val_loss: 4.3809e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 2.0282e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.7844e-04 - val_loss: 4.0799e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.6113e-04 - val_loss: 3.9072e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.6017e-04 - val_loss: 3.9510e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.7586e-04 - val_loss: 3.5335e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.6967e-04 - val_loss: 3.8523e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.6213e-04 - val_loss: 5.7548e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.8965e-04 - val_loss: 3.1764e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.5163e-04 - val_loss: 4.2675e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 1.5887e-04 - val_loss: 5.3869e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.7315e-04 - val_loss: 3.7563e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.7274e-04 - val_loss: 3.6133e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.4621e-04 - val_loss: 4.2702e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.5675e-04 - val_loss: 3.7642e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 1.5559e-04 - val_loss: 3.8597e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 1.5124e-04 - val_loss: 3.5561e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.5021e-04 - val_loss: 3.4560e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.5621e-04 - val_loss: 4.8388e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.4625e-04 - val_loss: 4.6352e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.4564e-04 - val_loss: 5.8971e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.4701e-04 - val_loss: 4.5243e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 1.4647e-04 - val_loss: 3.2273e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.3714e-04 - val_loss: 2.8142e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.7050e-04 - val_loss: 3.7897e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2987e-04 - val_loss: 4.1853e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.3863e-04 - val_loss: 2.4944e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.4040e-04 - val_loss: 2.3751e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.5006e-04 - val_loss: 3.2148e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 1.3499e-04 - val_loss: 2.6823e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.4603e-04 - val_loss: 8.5649e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.3987e-04 - val_loss: 3.3306e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 1.3044e-04 - val_loss: 2.3399e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2958e-04 - val_loss: 2.4552e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.4373e-04 - val_loss: 3.3902e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.3711e-04 - val_loss: 3.7025e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.3486e-04 - val_loss: 3.1787e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.3388e-04 - val_loss: 2.9132e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.3272e-04 - val_loss: 2.9902e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.3200e-04 - val_loss: 2.6085e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.2950e-04 - val_loss: 1.7807e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 1.3202e-04 - val_loss: 2.1839e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.3268e-04 - val_loss: 2.4188e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.2984e-04 - val_loss: 2.6351e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 1.3280e-04 - val_loss: 2.5573e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 1.2662e-04 - val_loss: 2.2239e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2075e-04 - val_loss: 1.9304e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 1.3805e-04 - val_loss: 2.4346e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.2663e-04 - val_loss: 2.4900e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 1.2912e-04 - val_loss: 3.8346e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2652e-04 - val_loss: 2.4497e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 1.1880e-04 - val_loss: 1.7945e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.3690e-04 - val_loss: 1.8555e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.2245e-04 - val_loss: 2.3382e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 1.2552e-04 - val_loss: 2.3423e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 1.2392e-04 - val_loss: 2.4291e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 1.1645e-04 - val_loss: 1.7164e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 1.4063e-04 - val_loss: 2.1249e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.2056e-04 - val_loss: 2.7339e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.2153e-04 - val_loss: 1.9869e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 1.2307e-04 - val_loss: 3.0331e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 1.2606e-04 - val_loss: 2.1819e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 1.1772e-04 - val_loss: 4.0682e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.1968e-04 - val_loss: 4.3472e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.1788e-04 - val_loss: 1.5704e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.1677e-04 - val_loss: 1.7269e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.2758e-04 - val_loss: 1.8278e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1883e-04 - val_loss: 2.5543e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 1.0958e-04 - val_loss: 1.4329e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.2362e-04 - val_loss: 3.1060e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.2683e-04 - val_loss: 2.6021e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.3372e-04 - val_loss: 1.9830e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.1421e-04 - val_loss: 1.6954e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.0800e-04 - val_loss: 1.6310e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.3694e-04 - val_loss: 1.9474e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 1.1745e-04 - val_loss: 2.9655e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.2537e-04 - val_loss: 1.5608e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.1718e-04 - val_loss: 2.4739e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.1990e-04 - val_loss: 1.4546e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.0458e-04 - val_loss: 1.3874e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.2601e-04 - val_loss: 2.4162e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 1.2100e-04 - val_loss: 1.3336e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.0487e-04 - val_loss: 1.4025e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 1.2887e-04 - val_loss: 1.3728e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.0726e-04 - val_loss: 1.3619e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.2460e-04 - val_loss: 1.2767e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.0685e-04 - val_loss: 1.4315e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 1.1986e-04 - val_loss: 1.7477e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.1509e-04 - val_loss: 2.2083e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.1200e-04 - val_loss: 1.4080e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.0724e-04 - val_loss: 1.4862e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.1797e-04 - val_loss: 1.5710e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.0823e-04 - val_loss: 1.2849e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 1.1916e-04 - val_loss: 1.4999e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.0507e-04 - val_loss: 1.2491e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 1.1710e-04 - val_loss: 1.5168e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1086e-04 - val_loss: 1.3427e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 1.1034e-04 - val_loss: 1.1892e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 1.2553e-04 - val_loss: 1.3341e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 9.7047e-05 - val_loss: 1.2703e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 1.1338e-04 - val_loss: 1.4585e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.0847e-04 - val_loss: 1.4303e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 151/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 1.0832e-04 - val_loss: 1.6797e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 152/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.1204e-04 - val_loss: 1.2873e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 153/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.1489e-04 - val_loss: 1.4897e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 154/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 9.9898e-05 - val_loss: 1.4892e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 155/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 1.1141e-04 - val_loss: 1.7759e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 156/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 1.1505e-04 - val_loss: 1.6434e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 157/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 1.1272e-04 - val_loss: 3.1301e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 158/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.1509e-04 - val_loss: 1.1556e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 159/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 1.0302e-04 - val_loss: 1.2488e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 160/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 1.0911e-04 - val_loss: 1.1392e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 161/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 9.8483e-05 - val_loss: 1.3772e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 162/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2103e-04 - val_loss: 1.2093e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 163/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 9.7244e-05 - val_loss: 1.5344e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 164/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 1.0400e-04 - val_loss: 1.2677e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 165/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 1.0508e-04 - val_loss: 1.5608e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 166/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 1.0145e-04 - val_loss: 1.2773e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 167/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 1.1306e-04 - val_loss: 1.0516e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 168/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 8.7391e-05 - val_loss: 1.0631e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 169/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 8.8206e-05 - val_loss: 1.0273e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 170/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 9.0436e-05 - val_loss: 1.1187e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 171/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 9.2503e-05 - val_loss: 1.0517e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 172/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 9.1953e-05 - val_loss: 1.0521e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 173/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 9.1187e-05 - val_loss: 1.0966e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 174/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 9.0560e-05 - val_loss: 1.1589e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 175/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 8.8354e-05 - val_loss: 1.2832e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 176/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 8.8680e-05 - val_loss: 1.1309e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 177/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 8.9083e-05 - val_loss: 1.1319e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 178/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 9.0703e-05 - val_loss: 1.1459e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 179/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 8.7646e-05 - val_loss: 1.0706e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 180/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 9.0020e-05 - val_loss: 1.0407e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 181/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 8.9099e-05 - val_loss: 1.0286e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 182/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 8.8629e-05 - val_loss: 1.0372e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 183/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 8.9097e-05 - val_loss: 1.0340e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 184/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 8.9722e-05 - val_loss: 1.0695e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 185/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 8.6740e-05 - val_loss: 1.0708e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 186/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 8.7456e-05 - val_loss: 1.0429e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 187/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 8.8571e-05 - val_loss: 9.9392e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 188/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 8.9243e-05 - val_loss: 9.9387e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 189/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 8.6457e-05 - val_loss: 9.8662e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 190/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 8.8001e-05 - val_loss: 9.8642e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 191/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 8.6937e-05 - val_loss: 1.0033e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 192/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 8.7086e-05 - val_loss: 9.9880e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 193/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 8.6744e-05 - val_loss: 1.0062e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 194/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 8.6832e-05 - val_loss: 9.9433e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 195/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 8.6194e-05 - val_loss: 1.0309e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 196/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 8.6419e-05 - val_loss: 1.0089e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 197/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 8.6153e-05 - val_loss: 1.0044e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 198/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 8.5825e-05 - val_loss: 9.8571e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 199/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 8.5597e-05 - val_loss: 9.7126e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 200/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 8.5437e-05 - val_loss: 9.5139e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 201/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 8.5175e-05 - val_loss: 9.4000e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 202/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 8.5262e-05 - val_loss: 9.1840e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 203/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 8.5071e-05 - val_loss: 9.0902e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 204/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 8.4856e-05 - val_loss: 9.0602e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 205/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 8.6106e-05 - val_loss: 9.0859e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 206/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 8.3729e-05 - val_loss: 9.1283e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 207/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 8.6098e-05 - val_loss: 1.0422e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 208/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 8.8020e-05 - val_loss: 9.2282e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 209/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 8.4008e-05 - val_loss: 8.8933e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 210/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 8.3128e-05 - val_loss: 9.8617e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 211/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 8.5314e-05 - val_loss: 1.0685e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 212/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 9.2873e-05 - val_loss: 9.5051e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 213/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 8.2974e-05 - val_loss: 1.0474e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 214/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 8.5806e-05 - val_loss: 1.0499e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 215/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 8.4407e-05 - val_loss: 1.0961e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 216/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 8.3829e-05 - val_loss: 1.0218e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 217/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 8.2109e-05 - val_loss: 9.8018e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 218/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.8116e-05 - val_loss: 9.9315e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 219/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.8208e-05 - val_loss: 9.7405e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 220/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.8398e-05 - val_loss: 9.9993e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 221/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 7.9303e-05 - val_loss: 9.9123e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 222/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.9086e-05 - val_loss: 9.9419e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 223/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 7.8907e-05 - val_loss: 9.9186e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 224/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 7.8897e-05 - val_loss: 9.8832e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 225/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 7.8675e-05 - val_loss: 9.9346e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 226/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.8709e-05 - val_loss: 9.9913e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 227/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.8402e-05 - val_loss: 1.0019e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 228/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 7.8510e-05 - val_loss: 1.0017e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 229/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 7.8176e-05 - val_loss: 1.0092e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 230/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.8365e-05 - val_loss: 1.0131e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 231/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.7860e-05 - val_loss: 1.0203e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 232/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.8032e-05 - val_loss: 1.0151e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 233/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 7.7770e-05 - val_loss: 1.0241e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 234/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 7.7601e-05 - val_loss: 1.0316e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 235/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.7359e-05 - val_loss: 1.0433e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 236/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.7148e-05 - val_loss: 1.0575e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 237/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.7070e-05 - val_loss: 1.0688e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 238/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 7.6921e-05 - val_loss: 1.0484e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 239/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.6759e-05 - val_loss: 1.0290e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 240/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 7.6619e-05 - val_loss: 1.0119e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 241/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.6437e-05 - val_loss: 9.8772e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 242/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 7.6237e-05 - val_loss: 9.6556e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 243/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.6128e-05 - val_loss: 9.4305e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 244/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.5979e-05 - val_loss: 9.1998e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 245/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.5849e-05 - val_loss: 9.0607e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 246/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 7.5702e-05 - val_loss: 8.9733e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 247/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 7.5596e-05 - val_loss: 8.8518e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 248/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.5494e-05 - val_loss: 8.7178e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 249/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.5383e-05 - val_loss: 8.5983e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 250/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 7.5294e-05 - val_loss: 8.4747e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 251/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.5215e-05 - val_loss: 8.4381e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 252/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 7.5099e-05 - val_loss: 8.3978e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 253/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.5015e-05 - val_loss: 8.3785e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 254/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.4945e-05 - val_loss: 8.3742e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 255/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 7.4837e-05 - val_loss: 8.4181e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 256/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.4811e-05 - val_loss: 8.5177e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 257/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 7.4826e-05 - val_loss: 8.6920e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 258/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.4639e-05 - val_loss: 8.7857e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 259/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 7.4500e-05 - val_loss: 8.8228e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 260/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.4335e-05 - val_loss: 8.8231e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 261/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.4194e-05 - val_loss: 8.7267e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 262/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 7.4011e-05 - val_loss: 8.4703e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 263/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 7.3824e-05 - val_loss: 8.2079e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 264/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.3869e-05 - val_loss: 8.1749e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 265/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.3817e-05 - val_loss: 8.1693e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 266/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 7.3631e-05 - val_loss: 8.1470e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 267/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 7.3520e-05 - val_loss: 8.1437e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 268/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.1999e-05 - val_loss: 8.3485e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 269/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.1940e-05 - val_loss: 8.3039e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 270/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.1983e-05 - val_loss: 8.3294e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 271/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 7.1919e-05 - val_loss: 8.3331e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 272/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 7.1863e-05 - val_loss: 8.3208e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 273/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.1804e-05 - val_loss: 8.3009e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 274/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 7.1739e-05 - val_loss: 8.3015e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 275/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 7.1665e-05 - val_loss: 8.2945e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 276/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 7.1586e-05 - val_loss: 8.2952e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 277/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.1501e-05 - val_loss: 8.2785e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 278/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.1425e-05 - val_loss: 8.2586e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 279/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 7.1361e-05 - val_loss: 8.2468e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 280/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 7.1297e-05 - val_loss: 8.2471e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 281/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.1223e-05 - val_loss: 8.2430e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 282/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.1146e-05 - val_loss: 8.2205e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 283/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 7.1067e-05 - val_loss: 8.1956e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 284/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 7.0993e-05 - val_loss: 8.1742e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 285/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.0921e-05 - val_loss: 8.1457e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 286/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.0846e-05 - val_loss: 8.1303e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 287/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.0764e-05 - val_loss: 8.1151e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 288/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 7.0687e-05 - val_loss: 8.1013e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 289/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.0619e-05 - val_loss: 8.0896e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 290/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.0549e-05 - val_loss: 8.0808e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 291/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.0463e-05 - val_loss: 8.0575e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 292/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 7.0374e-05 - val_loss: 8.0293e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 293/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 7.0289e-05 - val_loss: 8.0067e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 294/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.0198e-05 - val_loss: 7.9913e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 295/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.0125e-05 - val_loss: 7.9925e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 296/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 7.0049e-05 - val_loss: 7.9968e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 297/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 6.9959e-05 - val_loss: 8.0077e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 298/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.9883e-05 - val_loss: 8.0029e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 299/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.9822e-05 - val_loss: 8.0132e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 300/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.9754e-05 - val_loss: 8.0210e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 301/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 6.9671e-05 - val_loss: 8.0131e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 302/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.9583e-05 - val_loss: 7.9936e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 303/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 6.9481e-05 - val_loss: 7.9625e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 304/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.9376e-05 - val_loss: 7.9475e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 305/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 6.9286e-05 - val_loss: 7.9378e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 306/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.9187e-05 - val_loss: 7.9358e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 307/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.9094e-05 - val_loss: 7.9289e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 308/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.9013e-05 - val_loss: 7.9189e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 309/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 6.8942e-05 - val_loss: 7.9152e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 310/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.8868e-05 - val_loss: 7.9206e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 311/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.8788e-05 - val_loss: 7.9274e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 312/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.8713e-05 - val_loss: 7.9192e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 313/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.8629e-05 - val_loss: 7.9087e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 314/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 6.8548e-05 - val_loss: 7.9115e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 315/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.8468e-05 - val_loss: 7.9199e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 316/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.8386e-05 - val_loss: 7.9335e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 317/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 6.8307e-05 - val_loss: 7.9395e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 318/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.7716e-05 - val_loss: 8.0380e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 319/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 6.7716e-05 - val_loss: 8.1267e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 320/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.7694e-05 - val_loss: 8.1902e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 321/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 6.7643e-05 - val_loss: 8.2052e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 322/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.7588e-05 - val_loss: 8.2109e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 323/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 6.7535e-05 - val_loss: 8.2172e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 324/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.7487e-05 - val_loss: 8.2176e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 325/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 6.7437e-05 - val_loss: 8.2267e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 326/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.7386e-05 - val_loss: 8.2449e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 327/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.7335e-05 - val_loss: 8.2545e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 328/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.7285e-05 - val_loss: 8.2689e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 329/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 6.7234e-05 - val_loss: 8.2840e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 330/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.7185e-05 - val_loss: 8.2932e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 331/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.7137e-05 - val_loss: 8.3038e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 332/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.7090e-05 - val_loss: 8.3213e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 333/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.7044e-05 - val_loss: 8.3368e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 334/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 6.6997e-05 - val_loss: 8.3481e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 335/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 6.6951e-05 - val_loss: 8.3590e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 336/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.6904e-05 - val_loss: 8.3724e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 337/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 6.6858e-05 - val_loss: 8.3784e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 338/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.6813e-05 - val_loss: 8.3781e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 339/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.6767e-05 - val_loss: 8.3859e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 340/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.6722e-05 - val_loss: 8.3962e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 341/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 6.6676e-05 - val_loss: 8.4079e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 342/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.6633e-05 - val_loss: 8.4088e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 343/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.6587e-05 - val_loss: 8.4091e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 344/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.6539e-05 - val_loss: 8.4025e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 345/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 6.6492e-05 - val_loss: 8.3999e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 346/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.6446e-05 - val_loss: 8.3994e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 347/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.6401e-05 - val_loss: 8.3764e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 348/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 6.6354e-05 - val_loss: 8.3630e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 349/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.6305e-05 - val_loss: 8.3411e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 350/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.6257e-05 - val_loss: 8.3283e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 351/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.6210e-05 - val_loss: 8.3179e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 352/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 6.6163e-05 - val_loss: 8.3224e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 353/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.6114e-05 - val_loss: 8.3261e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 354/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.6068e-05 - val_loss: 8.3319e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 355/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.6023e-05 - val_loss: 8.3387e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 356/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 6.5979e-05 - val_loss: 8.3431e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 357/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 6.5932e-05 - val_loss: 8.3477e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 358/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.5886e-05 - val_loss: 8.3473e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 359/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.5841e-05 - val_loss: 8.3388e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 360/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 6.5798e-05 - val_loss: 8.3427e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 361/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.5756e-05 - val_loss: 8.3511e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 362/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.5714e-05 - val_loss: 8.3475e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 363/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 6.5671e-05 - val_loss: 8.3569e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 364/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 6.5631e-05 - val_loss: 8.3567e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 365/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.5591e-05 - val_loss: 8.3594e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 366/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.5551e-05 - val_loss: 8.3585e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 367/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.5508e-05 - val_loss: 8.3571e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 368/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 6.5251e-05 - val_loss: 8.1417e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 369/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.5248e-05 - val_loss: 8.1433e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 370/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 6.5230e-05 - val_loss: 8.1538e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 371/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 6.5206e-05 - val_loss: 8.1642e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 372/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.5179e-05 - val_loss: 8.1683e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 373/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.5149e-05 - val_loss: 8.1739e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 374/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.5120e-05 - val_loss: 8.1826e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 375/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 6.5089e-05 - val_loss: 8.1902e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 376/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.5057e-05 - val_loss: 8.1928e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 377/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.5026e-05 - val_loss: 8.2039e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 378/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.4995e-05 - val_loss: 8.2098e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 379/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 6.4964e-05 - val_loss: 8.2143e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 380/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 6.4933e-05 - val_loss: 8.2227e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 381/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.4903e-05 - val_loss: 8.2298e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 382/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.4874e-05 - val_loss: 8.2353e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 383/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.4844e-05 - val_loss: 8.2428e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 384/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 6.4815e-05 - val_loss: 8.2455e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 385/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.4786e-05 - val_loss: 8.2500e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 386/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.4756e-05 - val_loss: 8.2521e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 387/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.4726e-05 - val_loss: 8.2568e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 388/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.4697e-05 - val_loss: 8.2625e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 389/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 6.4667e-05 - val_loss: 8.2724e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 390/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 6.4638e-05 - val_loss: 8.2779e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 391/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.4609e-05 - val_loss: 8.2840e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 392/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.4580e-05 - val_loss: 8.2882e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 393/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 6.4551e-05 - val_loss: 8.2889e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 394/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.4522e-05 - val_loss: 8.2804e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 395/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.4492e-05 - val_loss: 8.2842e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 396/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 6.4460e-05 - val_loss: 8.2815e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 397/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 6.4430e-05 - val_loss: 8.2879e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 398/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.4398e-05 - val_loss: 8.2870e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 399/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 6.4366e-05 - val_loss: 8.2945e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 400/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 6.4334e-05 - val_loss: 8.3027e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 401/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 6.4300e-05 - val_loss: 8.3095e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 402/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.4268e-05 - val_loss: 8.3190e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 403/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 6.4237e-05 - val_loss: 8.3242e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 404/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 6.4205e-05 - val_loss: 8.3331e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 405/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.4173e-05 - val_loss: 8.3414e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 406/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.4142e-05 - val_loss: 8.3445e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 407/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.4111e-05 - val_loss: 8.3517e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 408/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 6.4079e-05 - val_loss: 8.3588e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 409/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.4048e-05 - val_loss: 8.3629e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 410/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.4016e-05 - val_loss: 8.3675e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 411/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 6.3986e-05 - val_loss: 8.3728e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 412/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.3956e-05 - val_loss: 8.3760e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 413/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.3925e-05 - val_loss: 8.3781e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 414/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 6.3894e-05 - val_loss: 8.3768e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 415/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.3864e-05 - val_loss: 8.3780e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 416/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.3835e-05 - val_loss: 8.3741e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 417/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 6.3806e-05 - val_loss: 8.3778e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 418/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.3684e-05 - val_loss: 8.1854e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 419/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.3661e-05 - val_loss: 8.1749e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 420/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 6.3648e-05 - val_loss: 8.1643e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 421/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 6.3633e-05 - val_loss: 8.1555e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 422/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.3617e-05 - val_loss: 8.1492e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 423/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 6.3601e-05 - val_loss: 8.1446e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 424/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 6.3585e-05 - val_loss: 8.1398e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 425/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.3568e-05 - val_loss: 8.1351e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 426/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.3550e-05 - val_loss: 8.1315e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 427/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.3533e-05 - val_loss: 8.1294e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 428/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 6.3516e-05 - val_loss: 8.1273e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 429/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.3499e-05 - val_loss: 8.1282e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 430/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.3482e-05 - val_loss: 8.1283e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 431/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 6.3464e-05 - val_loss: 8.1296e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 432/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.3447e-05 - val_loss: 8.1301e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 433/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 6.3430e-05 - val_loss: 8.1297e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 434/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.3413e-05 - val_loss: 8.1298e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 435/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 6.3396e-05 - val_loss: 8.1280e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 436/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.3379e-05 - val_loss: 8.1265e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 437/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 6.3362e-05 - val_loss: 8.1259e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 438/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 6.3345e-05 - val_loss: 8.1266e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 439/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.3328e-05 - val_loss: 8.1264e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 440/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.3311e-05 - val_loss: 8.1281e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 441/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 6.3293e-05 - val_loss: 8.1298e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 442/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.3277e-05 - val_loss: 8.1304e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 443/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.3259e-05 - val_loss: 8.1318e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 444/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.3242e-05 - val_loss: 8.1325e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 445/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.3225e-05 - val_loss: 8.1360e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 446/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.3208e-05 - val_loss: 8.1341e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 447/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 6.3191e-05 - val_loss: 8.1328e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 448/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.3174e-05 - val_loss: 8.1313e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 449/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.3158e-05 - val_loss: 8.1319e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 450/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 6.3141e-05 - val_loss: 8.1331e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 451/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.3124e-05 - val_loss: 8.1359e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 452/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.3107e-05 - val_loss: 8.1369e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 453/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.3090e-05 - val_loss: 8.1382e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 454/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 6.3074e-05 - val_loss: 8.1377e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 455/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 6.3057e-05 - val_loss: 8.1372e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 456/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.3041e-05 - val_loss: 8.1357e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 457/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 6.3024e-05 - val_loss: 8.1361e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 458/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.3008e-05 - val_loss: 8.1333e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 459/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 6.2991e-05 - val_loss: 8.1320e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 460/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.2975e-05 - val_loss: 8.1295e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 461/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 6.2958e-05 - val_loss: 8.1325e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 462/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.2941e-05 - val_loss: 8.1331e-05 - learning_rate: 7.8125e-06\n",
      "Epoch 463/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.2924e-05 - val_loss: 8.1354e-05 - learning_rate: 7.8125e-06\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step\n",
      "\n",
      "--- Model Evaluation (Neural Network Test Set) ---\n",
      "Overall Averaged RMSE: 0.008893 AU\n",
      "X-coordinate RMSE: 0.012450 AU\n",
      "Y-coordinate RMSE: 0.012386 AU\n",
      "Z-coordinate RMSE: 0.005364 AU\n"
     ]
    }
   ],
   "source": [
    "# --- 5. BUILD AND TRAIN THE INDIVIDUAL MLP MODELS ---\n",
    "print('2. Building and training ' + str(NUM_MODELS) + ' diverse MLP models...')\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    # CRITICAL: Clear session to ensure fresh start for each model\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Use a different random seed and shuffle seed for diversity\n",
    "    current_seed = RANDOM_SEED + i\n",
    "    tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.00001), \n",
    "              input_shape=(INPUT_SHAPE,)), \n",
    "        \n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.00001)),\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.00001)), \n",
    "        \n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "        Dense(3, activation='linear') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    print('--- Training MLP Model ' + str(i+1) + ' (Seed: ' + str(current_seed) + ') ---')\n",
    "    model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train_scaled,\n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(X_test_scaled, y_test_scaled),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "    # get inverse value (becuase of the scaling)\n",
    "    y_pred_mlp_au = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Calculate the loss (MSE) on the test set\n",
    "    y_test_np = y_test.values\n",
    "    loss_mse = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "    rmse_au = np.sqrt(loss_mse) \n",
    "\n",
    "    # Calculate RMSE for each coordinate individually\n",
    "    rmse_x = np.sqrt(mean_squared_error(y_test_np[:, 0], y_pred_mlp_au[:, 0]))\n",
    "    rmse_y = np.sqrt(mean_squared_error(y_test_np[:, 1], y_pred_mlp_au[:, 1]))\n",
    "    rmse_z = np.sqrt(mean_squared_error(y_test_np[:, 2], y_pred_mlp_au[:, 2]))\n",
    "\n",
    "    print(\"\\n--- Model Evaluation (Neural Network Test Set) ---\")\n",
    "    print(f\"Overall Averaged RMSE: {rmse_au:.6f} AU\")\n",
    "    print(f\"X-coordinate RMSE: {rmse_x:.6f} AU\")\n",
    "    print(f\"Y-coordinate RMSE: {rmse_y:.6f} AU\")\n",
    "    print(f\"Z-coordinate RMSE: {rmse_z:.6f} AU\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the individual model (using the modern .keras format)\n",
    "model_save_name = os.path.join(MODEL_DIR, f'{TARGET_PLANET}_position_predictor_mm' + str(i) + '.keras')\n",
    "model.save(model_save_name)\n",
    "print('Saved model to: ' + model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. ENSEMBLE PREDICTION AND EVALUATION ---\n",
    "print('3. Final Ensemble Evaluation...')\n",
    "\n",
    "# The ensemble prediction is the simple average of all individual model predictions.\n",
    "y_ensemble_pred_au = np.mean(y_pred_list, axis=0)\n",
    "\n",
    "# Evaluate the ensemble performance\n",
    "overall_ensemble_rmse = np.sqrt(mean_squared_error(y_test.values, y_ensemble_pred_au))\n",
    "\n",
    "print('Overall Ensemble RMSE: ' + '{:.6f}'.format(overall_ensemble_rmse) + ' AU')\n",
    "\n",
    "print(\"Training and evaluation complete. Models and scaler saved to the 'models' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
