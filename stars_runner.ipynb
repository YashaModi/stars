{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skyfield.api import load, Topos, Star, utc\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import stars_utils\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "MODELS_LIS = ['mm0','mm1','mm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from directory: models\n",
      "Attempting to load: models/mars_position_predictor_mm0.keras\n",
      "Attempting to load: models/mars_position_predictor_mm1.keras\n",
      "File not found at models/mars_position_predictor_mm2.keras. Falling back to: models/mars_position_predictor_mm2.h5\n",
      "Successfully loaded 3 models.\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'models'\n",
    "MODELS_LIS = ['mm0','mm1','mm2']\n",
    "\n",
    "models = stars_utils.models_loader(MODEL_DIR,MODELS_LIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JPL ephemeris data file\n",
    "planets = load('de421.bsp')\n",
    "ts = load.timescale()\n",
    "mars = planets['mars']\n",
    "earth = planets['earth']\n",
    "\n",
    "# 2. Define Time Range (e.g., 50 years of data, 1970 to 2020)\n",
    "start_date = datetime(1970, 1, 1)\n",
    "end_date = datetime(2025, 1, 1)\n",
    "time_step = timedelta(days=1)  # Data point every week\n",
    "\n",
    "# Generate a list of dates\n",
    "dates = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    dates.append(current_date)\n",
    "    current_date += time_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created successfully with 20090 data points.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert Python datetimes to Skyfield time objects\n",
    "timezone_aware_dates = [d.replace(tzinfo=utc) for d in dates]\n",
    "t = ts.utc(timezone_aware_dates)\n",
    "\n",
    "# 3. Calculate Geocentric Position (Position as seen from Earth)\n",
    "# astrometric_position is the calculation of a body's position in space\n",
    "# as seen from a specific location (Earth).\n",
    "astrometric = earth.at(t).observe(mars)\n",
    "\n",
    "# Get Right Ascension and Declination\n",
    "# These are your primary target variables for prediction!\n",
    "ra, dec, distance = astrometric.radec()\n",
    "\n",
    "# 4. Create the DataFrame (Your Dataset)\n",
    "data = {\n",
    "    'Time_UTC': dates,\n",
    "    'Julian_Date': t.tdb,\n",
    "    'RA_deg': ra.degrees,      # Right Ascension (Target Variable)\n",
    "    'Dec_deg': dec.degrees     # Declination (Target Variable)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 5. Save the Dataset\n",
    "df.to_csv('mars_ephemeris_data.csv', index=False)\n",
    "print(\"Dataset created successfully with\", len(df), \"data points.\")\n",
    "\n",
    "mars_sf_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the jpl file, rename and clean the colums, and remove redundants\n",
    "\n",
    "# mars_jpl_df = pd.read_csv(\"/Users/yasha/Desktop/projects/stars/mars_1970_2025.csv\", skiprows=7)\n",
    "# mars_jpl_df = mars_jpl_df.rename(columns={mars_jpl_df.columns[0]: 'Date__(UT)__HR:MN'})\n",
    "# mars_jpl_df.columns = [col.strip().replace('\\\\', '') for col in mars_jpl_df.columns]\n",
    "# cols_to_drop = ['', '.1', '/r']\n",
    "# mars_jpl_df = mars_jpl_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "# mars_jpl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get JD_min as the reference point\n",
    "JD_min = mars_sf_df.Julian_Date.min()\n",
    "mars_sf_df[\"Time_Index\"] = mars_sf_df.Julian_Date - JD_min\n",
    "time_index = mars_sf_df[\"Time_Index\"]\n",
    "\n",
    "# Add features to the dataframe\n",
    "PERIOD_YEAR = 365.25\n",
    "PERIOD_SYNODIC = 780.0\n",
    "\n",
    "# Earth's Annual Cycle Features\n",
    "mars_sf_df[\"Sin_Year\"] = np.sin(2*np.pi*time_index/PERIOD_YEAR)\n",
    "mars_sf_df[\"Cos_Year\"] = np.cos(2*np.pi*time_index/PERIOD_YEAR)\n",
    "\n",
    "# Mars-Earth Synodic Cycle Features\n",
    "mars_sf_df[\"Sin_Synodic\"] = np.sin(2*np.pi*time_index/PERIOD_SYNODIC)\n",
    "mars_sf_df[\"Cos_Synodic\"] = np.cos(2*np.pi*time_index/PERIOD_SYNODIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20090, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_sf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 20090\n",
      "Training period ends at index 16072 (Date: 2014-01-02 00:00:00)\n",
      "Testing period starts at index 16072 (Date: 2014-01-02 00:00:00)\n"
     ]
    }
   ],
   "source": [
    "# Define features (x) and targets (y) for the model \n",
    "FEATURES = [\n",
    "    'Time_Index',\n",
    "    'Sin_Year', 'Cos_Year',\n",
    "    'Sin_Synodic', 'Cos_Synodic'\n",
    "]\n",
    "TARGETS = ['RA_deg', 'Dec_deg']\n",
    "\n",
    "X = mars_sf_df[FEATURES]\n",
    "y = mars_sf_df[TARGETS]\n",
    "\n",
    "# 80/20 split of the data\n",
    "split_point = int(len(mars_sf_df)*0.8)\n",
    "\n",
    "X_train = X[:split_point]\n",
    "X_test = X[split_point:]\n",
    "y_train = y[:split_point]\n",
    "y_test = y[split_point:]\n",
    "\n",
    "print(f\"Total data points: {len(X)}\")\n",
    "print(f\"Training period ends at index {split_point} (Date: {df.iloc[split_point]['Time_UTC']})\")\n",
    "print(f\"Testing period starts at index {split_point} (Date: {df.iloc[split_point]['Time_UTC']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# RF model training\n",
    "# - 100 trees, depth of each tree is 15\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting model training...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation (Test Set) ---\n",
      "RA (Right Ascension) RMSE: 92.835463 degrees\n",
      "Dec (Declination) RMSE: 13.216534 degrees\n",
      "Mean Angular Separation Error: 65.860966 degrees\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the unseen test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE for each target Right Ascension and Declination\n",
    "rmse_ra = np.sqrt(mean_squared_error(y_test['RA_deg'], y_pred[:, 0]))\n",
    "rmse_dec = np.sqrt(mean_squared_error(y_test['Dec_deg'], y_pred[:, 1]))\n",
    "\n",
    "print(\"\\n--- Model Evaluation (Test Set) ---\")\n",
    "print(f\"RA (Right Ascension) RMSE: {rmse_ra:.6f} degrees\")\n",
    "print(f\"Dec (Declination) RMSE: {rmse_dec:.6f} degrees\")\n",
    "\n",
    "# Calculate the average error distance (Hypotenuse of the error)\n",
    "total_error = np.sqrt(\n",
    "    (y_test['RA_deg'] - y_pred[:, 0])**2 + \n",
    "    (y_test['Dec_deg'] - y_pred[:, 1])**2\n",
    ")\n",
    "print(f\"Mean Angular Separation Error: {total_error.mean():.6f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this does not work well :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the astronomic xyz coordinates of mars from earth's pov\n",
    "astrometric = earth.at(t).observe(mars)\n",
    "pos_vector_au = astrometric.xyz.au #pos_vector_au is a 3xN NumPy array, where N is the number of time steps (t).\n",
    "\n",
    "data = {\n",
    "    'Time_UTC': dates,\n",
    "    'Julian_Date': t.tdb,\n",
    "    \n",
    "    # Extract the X, Y, and Z components\n",
    "    'X_au': pos_vector_au[0],  # X-coordinate\n",
    "    'Y_au': pos_vector_au[1],  # Y-coordinate\n",
    "    'Z_au': pos_vector_au[2]   # Z-coordinate\n",
    "}\n",
    "\n",
    "mars_sf_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_min = mars_sf_df.Julian_Date.min()\n",
    "mars_sf_df[\"Time_Index\"] = mars_sf_df.Julian_Date - JD_min\n",
    "time_index = mars_sf_df[\"Time_Index\"]\n",
    "\n",
    "mars_sf_df[\"Time_Index_2\"] = time_index ** 2\n",
    "mars_sf_df[\"Time_Index_3\"] = time_index ** 3\n",
    "\n",
    "# Add features to the dataframe\n",
    "PERIOD_YEAR = 365.25\n",
    "PERIOD_MARS = 686.98\n",
    "PERIOD_SYNODIC = 780.0\n",
    "PERIOD_JUPITER = 4332.6 # mars cycles are influenced by jupiters movement. added this for fixing.\n",
    "\n",
    "# Earth's Annual Cycle Features\n",
    "mars_sf_df[\"Sin_Year\"] = np.sin(2*np.pi*time_index/PERIOD_YEAR)\n",
    "mars_sf_df[\"Cos_Year\"] = np.cos(2*np.pi*time_index/PERIOD_YEAR)\n",
    "\n",
    "# Mars's Orbital Cycle Features (NEW and CRUCIAL)\n",
    "mars_sf_df['Sin_Mars'] = np.sin(2 * np.pi * time_index / PERIOD_MARS)\n",
    "mars_sf_df['Cos_Mars'] = np.cos(2 * np.pi * time_index / PERIOD_MARS)\n",
    "\n",
    "# Mars-Earth Synodic Cycle Features\n",
    "mars_sf_df[\"Sin_Synodic\"] = np.sin(2*np.pi*time_index/PERIOD_SYNODIC)\n",
    "mars_sf_df[\"Cos_Synodic\"] = np.cos(2*np.pi*time_index/PERIOD_SYNODIC)\n",
    "\n",
    "# Jupiter's Annual Cycle Features\n",
    "mars_sf_df['Sin_Jupiter'] = np.sin(2 * np.pi * time_index / PERIOD_JUPITER)\n",
    "mars_sf_df['Cos_Jupiter'] = np.cos(2 * np.pi * time_index / PERIOD_JUPITER)\n",
    "\n",
    "mars_sf_df['Sin_Year_Sin_Synodic'] = mars_sf_df['Sin_Year'] * mars_sf_df['Sin_Synodic']\n",
    "mars_sf_df['Sin_Year_Cos_Synodic'] = mars_sf_df['Sin_Year'] * mars_sf_df['Cos_Synodic']\n",
    "mars_sf_df['Cos_Year_Sin_Synodic'] = mars_sf_df['Cos_Year'] * mars_sf_df['Sin_Synodic']\n",
    "mars_sf_df['Cos_Year_Cos_Synodic'] = mars_sf_df['Cos_Year'] * mars_sf_df['Cos_Synodic']\n",
    "\n",
    "# Define features (x) and targets (y) for the model \n",
    "FEATURES = [\n",
    "    'Time_Index', 'Time_Index_2', 'Time_Index_3', \n",
    "    'Sin_Year', 'Cos_Year',\n",
    "    'Sin_Mars', 'Cos_Mars',\n",
    "    'Sin_Synodic', 'Cos_Synodic',\n",
    "]\n",
    "TARGETS = ['X_au', 'Y_au', 'Z_au']\n",
    "\n",
    "FEATURES_XGB = [\n",
    "    'Time_Index', 'Time_Index_2', 'Time_Index_3', \n",
    "    'Sin_Year', 'Cos_Year',\n",
    "    'Sin_Mars', 'Cos_Mars',\n",
    "    'Sin_Synodic', 'Cos_Synodic',\n",
    "    'Sin_Year_Sin_Synodic', 'Sin_Year_Cos_Synodic','Cos_Year_Sin_Synodic','Cos_Year_Cos_Synodic'\n",
    "]\n",
    "TARGETS = ['X_au', 'Y_au', 'Z_au']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 20090\n",
      "Training period ends at index 16072 (Date: 2014-01-02 00:00:00)\n",
      "Testing period starts at index 16072 (Date: 2014-01-02 00:00:00)\n"
     ]
    }
   ],
   "source": [
    "X = mars_sf_df[FEATURES]\n",
    "X_xgb = mars_sf_df[FEATURES_XGB]\n",
    "y = mars_sf_df[TARGETS]\n",
    "\n",
    "# 80/20 split of the data\n",
    "split_point = int(len(mars_sf_df)*0.8)\n",
    "\n",
    "X_train = X[:split_point]\n",
    "X_test = X[split_point:]\n",
    "\n",
    "X_xgb_train = X_xgb[:split_point]\n",
    "X_xgb_test = X_xgb[split_point:]\n",
    "\n",
    "y_train = y[:split_point]\n",
    "y_test = y[split_point:]\n",
    "\n",
    "# apply standart scalar\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_xgb_train_scaled = scaler.fit_transform(X_xgb_train)\n",
    "X_xgb_test_scaled = scaler.transform(X_xgb_test)\n",
    "\n",
    "# Fit and transform the training targets\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "# Transform the testing targets\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "print(f\"Total data points: {len(X)}\")\n",
    "print(f\"Training period ends at index {split_point} (Date: {df.iloc[split_point]['Time_UTC']})\")\n",
    "print(f\"Testing period starts at index {split_point} (Date: {df.iloc[split_point]['Time_UTC']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# RF model training\n",
    "# - 150 trees, depth of each tree is 15\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=150,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting model training...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation (Test Set) ---\n",
      "X_au RMSE: 0.056263 AU\n",
      "Y_au RMSE: 0.049758 AU\n",
      "Z_au RMSE: 0.022668 AU\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the unseen test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the NumPy prediction array back to a DataFrame for easy calculation\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=TARGETS, index=y_test.index)\n",
    "\n",
    "# Calculate RMSE for each target coordinate\n",
    "rmse_x = np.sqrt(mean_squared_error(y_test['X_au'], y_pred_df['X_au']))\n",
    "rmse_y = np.sqrt(mean_squared_error(y_test['Y_au'], y_pred_df['Y_au']))\n",
    "rmse_z = np.sqrt(mean_squared_error(y_test['Z_au'], y_pred_df['Z_au']))\n",
    "\n",
    "print(\"\\n--- Model Evaluation (Test Set) ---\")\n",
    "print(f\"X_au RMSE: {rmse_x:.6f} AU\")\n",
    "print(f\"Y_au RMSE: {rmse_y:.6f} AU\")\n",
    "print(f\"Z_au RMSE: {rmse_z:.6f} AU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Neural Network training (100 epochs)...\n",
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 886us/step - loss: 0.0532 - val_loss: 0.0392 - learning_rate: 0.0010\n",
      "Epoch 2/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0209 - val_loss: 0.0257 - learning_rate: 0.0010\n",
      "Epoch 3/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.0129 - val_loss: 0.0178 - learning_rate: 0.0010\n",
      "Epoch 4/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.0084 - val_loss: 0.0131 - learning_rate: 0.0010\n",
      "Epoch 5/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.0056 - val_loss: 0.0149 - learning_rate: 0.0010\n",
      "Epoch 6/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0043 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 7/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0032 - val_loss: 0.0087 - learning_rate: 0.0010\n",
      "Epoch 8/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0028 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 9/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.0022 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 10/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.0021 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 11/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.0018 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 12/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.0016 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 13/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 14/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.0014 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 15/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 0.0017 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 16/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 17/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0013 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 18/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0013 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 19/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 20/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0013 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 21/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 22/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 23/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 24/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 25/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0011 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 26/10000\n",
      "\u001b[1m260/503\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.0013"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# use 10000 spochs with batch size of 32\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Neural Network training (100 epochs)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_scaled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m              \u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel training complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# MLP NN Implemantation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# use early stopping for when there is no improvement for 150 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=150, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Reduce the LR by 50% (factor=0.5), If no improvement for 50 epochs, drop the LR, Don't let the LR drop below 1e-7 (0.0000001)\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,       \n",
    "    patience=50,      \n",
    "    min_lr=1e-7       \n",
    ")\n",
    "\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) \n",
    "regularizer_strength = 0.001 \n",
    "\n",
    "model = Sequential([\n",
    "    # Apply kernel_regularizer to hidden layers\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0001), \n",
    "          input_shape=(X_train_scaled.shape[1],)), \n",
    "    \n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0001)), \n",
    "    \n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(3, activation='linear') \n",
    "])\n",
    "\n",
    "model.compile(optimizer=custom_optimizer, loss='mse') \n",
    "\n",
    "# use 10000 spochs with batch size of 32\n",
    "print(\"\\nStarting Neural Network training (100 epochs)...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train_scaled, \n",
    "    epochs=10000,             \n",
    "    batch_size=32,          \n",
    "    validation_data=(X_test_scaled, y_test_scaled),\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    verbose=1              \n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step\n",
      "\n",
      "--- Model Evaluation (Neural Network Test Set) ---\n",
      "Overall Averaged RMSE: 0.019528 AU\n",
      "X-coordinate RMSE: 0.003944 AU\n",
      "Y-coordinate RMSE: 0.003822 AU\n",
      "Z-coordinate RMSE: 0.001565 AU\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# get inverse value (becuase of the scaling)\n",
    "y_pred_mlp_au = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate the loss (MSE) on the test set\n",
    "y_test_np = y_test.values\n",
    "loss_mse = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "rmse_au = np.sqrt(loss_mse) \n",
    "\n",
    "# Calculate RMSE for each coordinate individually\n",
    "rmse_x = np.sqrt(mean_squared_error(y_test_np[:, 0], y_pred_mlp_au[:, 0]))\n",
    "rmse_y = np.sqrt(mean_squared_error(y_test_np[:, 1], y_pred_mlp_au[:, 1]))\n",
    "rmse_z = np.sqrt(mean_squared_error(y_test_np[:, 2], y_pred_mlp_au[:, 2]))\n",
    "\n",
    "print(\"\\n--- Model Evaluation (Neural Network Test Set) ---\")\n",
    "print(f\"Overall Averaged RMSE: {rmse_au:.6f} AU\")\n",
    "print(f\"X-coordinate RMSE: {rmse_x:.6f} AU\")\n",
    "print(f\"Y-coordinate RMSE: {rmse_y:.6f} AU\")\n",
    "print(f\"Z-coordinate RMSE: {rmse_z:.6f} AU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to models/mars_position_predictor_final.keras\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_FILENAME = 'mars_position_predictor_final.keras' \n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# os.makedirs(MODEL_DIR, exist_ok=True) \n",
    "\n",
    "# Save the model\n",
    "model.save(os.path.join(MODEL_DIR, MODEL_FILENAME))\n",
    "\n",
    "print(f\"Model saved successfully to {os.path.join(MODEL_DIR, MODEL_FILENAME)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Predicted RA/Dec Coordinates (Degrees) ---\n",
      "   Predicted_RA_deg  Predicted_Dec_deg\n",
      "0        191.896774          -2.759812\n",
      "1        192.311584          -2.923764\n",
      "2        192.723251          -3.086056\n",
      "3        193.131683          -3.246643\n",
      "4        193.522125          -3.398860\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the conversion function\n",
    "radec_predictions = stars_utils.xyz_to_radec(y_pred_mlp_au)\n",
    "\n",
    "print(\"--- Predicted RA/Dec Coordinates (Degrees) ---\")\n",
    "print(radec_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting manual, stable XGBoost training (one model per coordinate)...\n",
      "\n",
      "--- Training X-coordinate model ---\n",
      "[0]\tvalidation-rmse:1.24974\n",
      "[1]\tvalidation-rmse:1.23953\n",
      "[2]\tvalidation-rmse:1.22772\n",
      "[3]\tvalidation-rmse:1.21610\n",
      "[4]\tvalidation-rmse:1.20448\n",
      "[5]\tvalidation-rmse:1.19381\n",
      "[6]\tvalidation-rmse:1.18232\n",
      "[7]\tvalidation-rmse:1.17092\n",
      "[8]\tvalidation-rmse:1.15985\n",
      "[9]\tvalidation-rmse:1.14877\n",
      "[10]\tvalidation-rmse:1.13795\n",
      "[11]\tvalidation-rmse:1.12961\n",
      "[12]\tvalidation-rmse:1.12006\n",
      "[13]\tvalidation-rmse:1.10950\n",
      "[14]\tvalidation-rmse:1.09905\n",
      "[15]\tvalidation-rmse:1.08860\n",
      "[16]\tvalidation-rmse:1.07836\n",
      "[17]\tvalidation-rmse:1.06805\n",
      "[18]\tvalidation-rmse:1.05799\n",
      "[19]\tvalidation-rmse:1.04800\n",
      "[20]\tvalidation-rmse:1.03799\n",
      "[21]\tvalidation-rmse:1.02804\n",
      "[22]\tvalidation-rmse:1.01824\n",
      "[23]\tvalidation-rmse:1.00845\n",
      "[24]\tvalidation-rmse:0.99871\n",
      "[25]\tvalidation-rmse:0.98930\n",
      "[26]\tvalidation-rmse:0.98012\n",
      "[27]\tvalidation-rmse:0.97092\n",
      "[28]\tvalidation-rmse:0.96190\n",
      "[29]\tvalidation-rmse:0.95289\n",
      "[30]\tvalidation-rmse:0.94393\n",
      "[31]\tvalidation-rmse:0.93489\n",
      "[32]\tvalidation-rmse:0.92589\n",
      "[33]\tvalidation-rmse:0.91716\n",
      "[34]\tvalidation-rmse:0.90841\n",
      "[35]\tvalidation-rmse:0.89968\n",
      "[36]\tvalidation-rmse:0.89120\n",
      "[37]\tvalidation-rmse:0.88276\n",
      "[38]\tvalidation-rmse:0.87445\n",
      "[39]\tvalidation-rmse:0.86703\n",
      "[40]\tvalidation-rmse:0.85881\n",
      "[41]\tvalidation-rmse:0.85076\n",
      "[42]\tvalidation-rmse:0.84288\n",
      "[43]\tvalidation-rmse:0.83491\n",
      "[44]\tvalidation-rmse:0.82694\n",
      "[45]\tvalidation-rmse:0.81899\n",
      "[46]\tvalidation-rmse:0.81122\n",
      "[47]\tvalidation-rmse:0.80349\n",
      "[48]\tvalidation-rmse:0.79582\n",
      "[49]\tvalidation-rmse:0.78834\n",
      "[50]\tvalidation-rmse:0.78088\n",
      "[51]\tvalidation-rmse:0.77348\n",
      "[52]\tvalidation-rmse:0.76698\n",
      "[53]\tvalidation-rmse:0.75975\n",
      "[54]\tvalidation-rmse:0.75318\n",
      "[55]\tvalidation-rmse:0.74624\n",
      "[56]\tvalidation-rmse:0.73914\n",
      "[57]\tvalidation-rmse:0.73216\n",
      "[58]\tvalidation-rmse:0.72527\n",
      "[59]\tvalidation-rmse:0.71838\n",
      "[60]\tvalidation-rmse:0.71154\n",
      "[61]\tvalidation-rmse:0.70481\n",
      "[62]\tvalidation-rmse:0.69822\n",
      "[63]\tvalidation-rmse:0.69162\n",
      "[64]\tvalidation-rmse:0.68507\n",
      "[65]\tvalidation-rmse:0.67861\n",
      "[66]\tvalidation-rmse:0.67226\n",
      "[67]\tvalidation-rmse:0.66592\n",
      "[68]\tvalidation-rmse:0.65962\n",
      "[69]\tvalidation-rmse:0.65337\n",
      "[70]\tvalidation-rmse:0.64715\n",
      "[71]\tvalidation-rmse:0.64095\n",
      "[72]\tvalidation-rmse:0.63497\n",
      "[73]\tvalidation-rmse:0.62903\n",
      "[74]\tvalidation-rmse:0.62312\n",
      "[75]\tvalidation-rmse:0.61730\n",
      "[76]\tvalidation-rmse:0.61154\n",
      "[77]\tvalidation-rmse:0.60591\n",
      "[78]\tvalidation-rmse:0.60029\n",
      "[79]\tvalidation-rmse:0.59472\n",
      "[80]\tvalidation-rmse:0.58920\n",
      "[81]\tvalidation-rmse:0.58363\n",
      "[82]\tvalidation-rmse:0.57809\n",
      "[83]\tvalidation-rmse:0.57262\n",
      "[84]\tvalidation-rmse:0.56720\n",
      "[85]\tvalidation-rmse:0.56197\n",
      "[86]\tvalidation-rmse:0.55683\n",
      "[87]\tvalidation-rmse:0.55163\n",
      "[88]\tvalidation-rmse:0.54639\n",
      "[89]\tvalidation-rmse:0.54127\n",
      "[90]\tvalidation-rmse:0.53616\n",
      "[91]\tvalidation-rmse:0.53114\n",
      "[92]\tvalidation-rmse:0.52620\n",
      "[93]\tvalidation-rmse:0.52125\n",
      "[94]\tvalidation-rmse:0.51632\n",
      "[95]\tvalidation-rmse:0.51217\n",
      "[96]\tvalidation-rmse:0.50735\n",
      "[97]\tvalidation-rmse:0.50268\n",
      "[98]\tvalidation-rmse:0.49801\n",
      "[99]\tvalidation-rmse:0.49344\n",
      "[100]\tvalidation-rmse:0.48877\n",
      "[101]\tvalidation-rmse:0.48476\n",
      "[102]\tvalidation-rmse:0.48025\n",
      "[103]\tvalidation-rmse:0.47577\n",
      "[104]\tvalidation-rmse:0.47134\n",
      "[105]\tvalidation-rmse:0.46702\n",
      "[106]\tvalidation-rmse:0.46275\n",
      "[107]\tvalidation-rmse:0.45858\n",
      "[108]\tvalidation-rmse:0.45447\n",
      "[109]\tvalidation-rmse:0.45026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/callback.py:386: UserWarning: [16:31:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110]\tvalidation-rmse:0.44600\n",
      "[111]\tvalidation-rmse:0.44183\n",
      "[112]\tvalidation-rmse:0.43774\n",
      "[113]\tvalidation-rmse:0.43374\n",
      "[114]\tvalidation-rmse:0.42968\n",
      "[115]\tvalidation-rmse:0.42584\n",
      "[116]\tvalidation-rmse:0.42187\n",
      "[117]\tvalidation-rmse:0.41806\n",
      "[118]\tvalidation-rmse:0.41422\n",
      "[119]\tvalidation-rmse:0.41029\n",
      "[120]\tvalidation-rmse:0.40655\n",
      "[121]\tvalidation-rmse:0.40305\n",
      "[122]\tvalidation-rmse:0.39943\n",
      "[123]\tvalidation-rmse:0.39571\n",
      "[124]\tvalidation-rmse:0.39238\n",
      "[125]\tvalidation-rmse:0.38876\n",
      "[126]\tvalidation-rmse:0.38517\n",
      "[127]\tvalidation-rmse:0.38162\n",
      "[128]\tvalidation-rmse:0.37816\n",
      "[129]\tvalidation-rmse:0.37465\n",
      "[130]\tvalidation-rmse:0.37115\n",
      "[131]\tvalidation-rmse:0.36779\n",
      "[132]\tvalidation-rmse:0.36443\n",
      "[133]\tvalidation-rmse:0.36104\n",
      "[134]\tvalidation-rmse:0.35769\n",
      "[135]\tvalidation-rmse:0.35438\n",
      "[136]\tvalidation-rmse:0.35111\n",
      "[137]\tvalidation-rmse:0.34825\n",
      "[138]\tvalidation-rmse:0.34512\n",
      "[139]\tvalidation-rmse:0.34191\n",
      "[140]\tvalidation-rmse:0.33873\n",
      "[141]\tvalidation-rmse:0.33564\n",
      "[142]\tvalidation-rmse:0.33254\n",
      "[143]\tvalidation-rmse:0.33000\n",
      "[144]\tvalidation-rmse:0.32699\n",
      "[145]\tvalidation-rmse:0.32396\n",
      "[146]\tvalidation-rmse:0.32132\n",
      "[147]\tvalidation-rmse:0.31837\n",
      "[148]\tvalidation-rmse:0.31543\n",
      "[149]\tvalidation-rmse:0.31252\n",
      "[150]\tvalidation-rmse:0.30966\n",
      "[151]\tvalidation-rmse:0.30691\n",
      "[152]\tvalidation-rmse:0.30406\n",
      "[153]\tvalidation-rmse:0.30134\n",
      "[154]\tvalidation-rmse:0.29886\n",
      "[155]\tvalidation-rmse:0.29637\n",
      "[156]\tvalidation-rmse:0.29368\n",
      "[157]\tvalidation-rmse:0.29100\n",
      "[158]\tvalidation-rmse:0.28836\n",
      "[159]\tvalidation-rmse:0.28605\n",
      "[160]\tvalidation-rmse:0.28345\n",
      "[161]\tvalidation-rmse:0.28098\n",
      "[162]\tvalidation-rmse:0.27846\n",
      "[163]\tvalidation-rmse:0.27592\n",
      "[164]\tvalidation-rmse:0.27343\n",
      "[165]\tvalidation-rmse:0.27091\n",
      "[166]\tvalidation-rmse:0.26844\n",
      "[167]\tvalidation-rmse:0.26603\n",
      "[168]\tvalidation-rmse:0.26373\n",
      "[169]\tvalidation-rmse:0.26139\n",
      "[170]\tvalidation-rmse:0.25904\n",
      "[171]\tvalidation-rmse:0.25680\n",
      "[172]\tvalidation-rmse:0.25452\n",
      "[173]\tvalidation-rmse:0.25224\n",
      "[174]\tvalidation-rmse:0.25000\n",
      "[175]\tvalidation-rmse:0.24775\n",
      "[176]\tvalidation-rmse:0.24547\n",
      "[177]\tvalidation-rmse:0.24327\n",
      "[178]\tvalidation-rmse:0.24111\n",
      "[179]\tvalidation-rmse:0.23893\n",
      "[180]\tvalidation-rmse:0.23684\n",
      "[181]\tvalidation-rmse:0.23472\n",
      "[182]\tvalidation-rmse:0.23267\n",
      "[183]\tvalidation-rmse:0.23055\n",
      "[184]\tvalidation-rmse:0.22848\n",
      "[185]\tvalidation-rmse:0.22641\n",
      "[186]\tvalidation-rmse:0.22438\n",
      "[187]\tvalidation-rmse:0.22235\n",
      "[188]\tvalidation-rmse:0.22070\n",
      "[189]\tvalidation-rmse:0.21873\n",
      "[190]\tvalidation-rmse:0.21679\n",
      "[191]\tvalidation-rmse:0.21493\n",
      "[192]\tvalidation-rmse:0.21306\n",
      "[193]\tvalidation-rmse:0.21117\n",
      "[194]\tvalidation-rmse:0.20930\n",
      "[195]\tvalidation-rmse:0.20742\n",
      "[196]\tvalidation-rmse:0.20558\n",
      "[197]\tvalidation-rmse:0.20378\n",
      "[198]\tvalidation-rmse:0.20197\n",
      "[199]\tvalidation-rmse:0.20018\n",
      "[200]\tvalidation-rmse:0.19839\n",
      "[201]\tvalidation-rmse:0.19669\n",
      "[202]\tvalidation-rmse:0.19504\n",
      "[203]\tvalidation-rmse:0.19333\n",
      "[204]\tvalidation-rmse:0.19169\n",
      "[205]\tvalidation-rmse:0.19033\n",
      "[206]\tvalidation-rmse:0.18877\n",
      "[207]\tvalidation-rmse:0.18712\n",
      "[208]\tvalidation-rmse:0.18551\n",
      "[209]\tvalidation-rmse:0.18390\n",
      "[210]\tvalidation-rmse:0.18231\n",
      "[211]\tvalidation-rmse:0.18074\n",
      "[212]\tvalidation-rmse:0.17918\n",
      "[213]\tvalidation-rmse:0.17764\n",
      "[214]\tvalidation-rmse:0.17617\n",
      "[215]\tvalidation-rmse:0.17464\n",
      "[216]\tvalidation-rmse:0.17314\n",
      "[217]\tvalidation-rmse:0.17168\n",
      "[218]\tvalidation-rmse:0.17024\n",
      "[219]\tvalidation-rmse:0.16876\n",
      "[220]\tvalidation-rmse:0.16742\n",
      "[221]\tvalidation-rmse:0.16603\n",
      "[222]\tvalidation-rmse:0.16461\n",
      "[223]\tvalidation-rmse:0.16354\n",
      "[224]\tvalidation-rmse:0.16214\n",
      "[225]\tvalidation-rmse:0.16075\n",
      "[226]\tvalidation-rmse:0.15938\n",
      "[227]\tvalidation-rmse:0.15808\n",
      "[228]\tvalidation-rmse:0.15678\n",
      "[229]\tvalidation-rmse:0.15580\n",
      "[230]\tvalidation-rmse:0.15450\n",
      "[231]\tvalidation-rmse:0.15325\n",
      "[232]\tvalidation-rmse:0.15197\n",
      "[233]\tvalidation-rmse:0.15067\n",
      "[234]\tvalidation-rmse:0.14938\n",
      "[235]\tvalidation-rmse:0.14810\n",
      "[236]\tvalidation-rmse:0.14685\n",
      "[237]\tvalidation-rmse:0.14578\n",
      "[238]\tvalidation-rmse:0.14462\n",
      "[239]\tvalidation-rmse:0.14341\n",
      "[240]\tvalidation-rmse:0.14219\n",
      "[241]\tvalidation-rmse:0.14101\n",
      "[242]\tvalidation-rmse:0.13983\n",
      "[243]\tvalidation-rmse:0.13865\n",
      "[244]\tvalidation-rmse:0.13751\n",
      "[245]\tvalidation-rmse:0.13637\n",
      "[246]\tvalidation-rmse:0.13527\n",
      "[247]\tvalidation-rmse:0.13416\n",
      "[248]\tvalidation-rmse:0.13310\n",
      "[249]\tvalidation-rmse:0.13203\n",
      "[250]\tvalidation-rmse:0.13099\n",
      "[251]\tvalidation-rmse:0.12993\n",
      "[252]\tvalidation-rmse:0.12891\n",
      "[253]\tvalidation-rmse:0.12783\n",
      "[254]\tvalidation-rmse:0.12683\n",
      "[255]\tvalidation-rmse:0.12578\n",
      "[256]\tvalidation-rmse:0.12477\n",
      "[257]\tvalidation-rmse:0.12388\n",
      "[258]\tvalidation-rmse:0.12291\n",
      "[259]\tvalidation-rmse:0.12193\n",
      "[260]\tvalidation-rmse:0.12094\n",
      "[261]\tvalidation-rmse:0.11993\n",
      "[262]\tvalidation-rmse:0.11899\n",
      "[263]\tvalidation-rmse:0.11808\n",
      "[264]\tvalidation-rmse:0.11718\n",
      "[265]\tvalidation-rmse:0.11630\n",
      "[266]\tvalidation-rmse:0.11535\n",
      "[267]\tvalidation-rmse:0.11441\n",
      "[268]\tvalidation-rmse:0.11348\n",
      "[269]\tvalidation-rmse:0.11258\n",
      "[270]\tvalidation-rmse:0.11180\n",
      "[271]\tvalidation-rmse:0.11091\n",
      "[272]\tvalidation-rmse:0.11006\n",
      "[273]\tvalidation-rmse:0.10921\n",
      "[274]\tvalidation-rmse:0.10834\n",
      "[275]\tvalidation-rmse:0.10748\n",
      "[276]\tvalidation-rmse:0.10663\n",
      "[277]\tvalidation-rmse:0.10580\n",
      "[278]\tvalidation-rmse:0.10501\n",
      "[279]\tvalidation-rmse:0.10420\n",
      "[280]\tvalidation-rmse:0.10341\n",
      "[281]\tvalidation-rmse:0.10261\n",
      "[282]\tvalidation-rmse:0.10185\n",
      "[283]\tvalidation-rmse:0.10105\n",
      "[284]\tvalidation-rmse:0.10027\n",
      "[285]\tvalidation-rmse:0.09950\n",
      "[286]\tvalidation-rmse:0.09873\n",
      "[287]\tvalidation-rmse:0.09798\n",
      "[288]\tvalidation-rmse:0.09725\n",
      "[289]\tvalidation-rmse:0.09650\n",
      "[290]\tvalidation-rmse:0.09576\n",
      "[291]\tvalidation-rmse:0.09503\n",
      "[292]\tvalidation-rmse:0.09431\n",
      "[293]\tvalidation-rmse:0.09362\n",
      "[294]\tvalidation-rmse:0.09296\n",
      "[295]\tvalidation-rmse:0.09226\n",
      "[296]\tvalidation-rmse:0.09156\n",
      "[297]\tvalidation-rmse:0.09093\n",
      "[298]\tvalidation-rmse:0.09027\n",
      "[299]\tvalidation-rmse:0.08962\n",
      "[300]\tvalidation-rmse:0.08894\n",
      "[301]\tvalidation-rmse:0.08828\n",
      "[302]\tvalidation-rmse:0.08763\n",
      "[303]\tvalidation-rmse:0.08709\n",
      "[304]\tvalidation-rmse:0.08647\n",
      "[305]\tvalidation-rmse:0.08585\n",
      "[306]\tvalidation-rmse:0.08524\n",
      "[307]\tvalidation-rmse:0.08461\n",
      "[308]\tvalidation-rmse:0.08401\n",
      "[309]\tvalidation-rmse:0.08340\n",
      "[310]\tvalidation-rmse:0.08279\n",
      "[311]\tvalidation-rmse:0.08220\n",
      "[312]\tvalidation-rmse:0.08161\n",
      "[313]\tvalidation-rmse:0.08105\n",
      "[314]\tvalidation-rmse:0.08063\n",
      "[315]\tvalidation-rmse:0.08006\n",
      "[316]\tvalidation-rmse:0.07951\n",
      "[317]\tvalidation-rmse:0.07896\n",
      "[318]\tvalidation-rmse:0.07842\n",
      "[319]\tvalidation-rmse:0.07788\n",
      "[320]\tvalidation-rmse:0.07735\n",
      "[321]\tvalidation-rmse:0.07684\n",
      "[322]\tvalidation-rmse:0.07631\n",
      "[323]\tvalidation-rmse:0.07578\n",
      "[324]\tvalidation-rmse:0.07526\n",
      "[325]\tvalidation-rmse:0.07474\n",
      "[326]\tvalidation-rmse:0.07422\n",
      "[327]\tvalidation-rmse:0.07373\n",
      "[328]\tvalidation-rmse:0.07323\n",
      "[329]\tvalidation-rmse:0.07275\n",
      "[330]\tvalidation-rmse:0.07228\n",
      "[331]\tvalidation-rmse:0.07181\n",
      "[332]\tvalidation-rmse:0.07136\n",
      "[333]\tvalidation-rmse:0.07090\n",
      "[334]\tvalidation-rmse:0.07042\n",
      "[335]\tvalidation-rmse:0.06996\n",
      "[336]\tvalidation-rmse:0.06954\n",
      "[337]\tvalidation-rmse:0.06909\n",
      "[338]\tvalidation-rmse:0.06865\n",
      "[339]\tvalidation-rmse:0.06821\n",
      "[340]\tvalidation-rmse:0.06777\n",
      "[341]\tvalidation-rmse:0.06742\n",
      "[342]\tvalidation-rmse:0.06699\n",
      "[343]\tvalidation-rmse:0.06657\n",
      "[344]\tvalidation-rmse:0.06615\n",
      "[345]\tvalidation-rmse:0.06575\n",
      "[346]\tvalidation-rmse:0.06531\n",
      "[347]\tvalidation-rmse:0.06490\n",
      "[348]\tvalidation-rmse:0.06450\n",
      "[349]\tvalidation-rmse:0.06412\n",
      "[350]\tvalidation-rmse:0.06373\n",
      "[351]\tvalidation-rmse:0.06335\n",
      "[352]\tvalidation-rmse:0.06297\n",
      "[353]\tvalidation-rmse:0.06259\n",
      "[354]\tvalidation-rmse:0.06222\n",
      "[355]\tvalidation-rmse:0.06185\n",
      "[356]\tvalidation-rmse:0.06147\n",
      "[357]\tvalidation-rmse:0.06109\n",
      "[358]\tvalidation-rmse:0.06073\n",
      "[359]\tvalidation-rmse:0.06036\n",
      "[360]\tvalidation-rmse:0.06001\n",
      "[361]\tvalidation-rmse:0.05965\n",
      "[362]\tvalidation-rmse:0.05931\n",
      "[363]\tvalidation-rmse:0.05898\n",
      "[364]\tvalidation-rmse:0.05863\n",
      "[365]\tvalidation-rmse:0.05830\n",
      "[366]\tvalidation-rmse:0.05796\n",
      "[367]\tvalidation-rmse:0.05764\n",
      "[368]\tvalidation-rmse:0.05731\n",
      "[369]\tvalidation-rmse:0.05707\n",
      "[370]\tvalidation-rmse:0.05676\n",
      "[371]\tvalidation-rmse:0.05645\n",
      "[372]\tvalidation-rmse:0.05615\n",
      "[373]\tvalidation-rmse:0.05586\n",
      "[374]\tvalidation-rmse:0.05555\n",
      "[375]\tvalidation-rmse:0.05525\n",
      "[376]\tvalidation-rmse:0.05494\n",
      "[377]\tvalidation-rmse:0.05466\n",
      "[378]\tvalidation-rmse:0.05439\n",
      "[379]\tvalidation-rmse:0.05411\n",
      "[380]\tvalidation-rmse:0.05383\n",
      "[381]\tvalidation-rmse:0.05355\n",
      "[382]\tvalidation-rmse:0.05327\n",
      "[383]\tvalidation-rmse:0.05299\n",
      "[384]\tvalidation-rmse:0.05272\n",
      "[385]\tvalidation-rmse:0.05245\n",
      "[386]\tvalidation-rmse:0.05218\n",
      "[387]\tvalidation-rmse:0.05192\n",
      "[388]\tvalidation-rmse:0.05166\n",
      "[389]\tvalidation-rmse:0.05140\n",
      "[390]\tvalidation-rmse:0.05113\n",
      "[391]\tvalidation-rmse:0.05089\n",
      "[392]\tvalidation-rmse:0.05065\n",
      "[393]\tvalidation-rmse:0.05041\n",
      "[394]\tvalidation-rmse:0.05016\n",
      "[395]\tvalidation-rmse:0.04992\n",
      "[396]\tvalidation-rmse:0.04970\n",
      "[397]\tvalidation-rmse:0.04947\n",
      "[398]\tvalidation-rmse:0.04922\n",
      "[399]\tvalidation-rmse:0.04899\n",
      "[400]\tvalidation-rmse:0.04876\n",
      "[401]\tvalidation-rmse:0.04854\n",
      "[402]\tvalidation-rmse:0.04832\n",
      "[403]\tvalidation-rmse:0.04809\n",
      "[404]\tvalidation-rmse:0.04793\n",
      "[405]\tvalidation-rmse:0.04770\n",
      "[406]\tvalidation-rmse:0.04751\n",
      "[407]\tvalidation-rmse:0.04731\n",
      "[408]\tvalidation-rmse:0.04711\n",
      "[409]\tvalidation-rmse:0.04691\n",
      "[410]\tvalidation-rmse:0.04672\n",
      "[411]\tvalidation-rmse:0.04651\n",
      "[412]\tvalidation-rmse:0.04632\n",
      "[413]\tvalidation-rmse:0.04613\n",
      "[414]\tvalidation-rmse:0.04594\n",
      "[415]\tvalidation-rmse:0.04575\n",
      "[416]\tvalidation-rmse:0.04556\n",
      "[417]\tvalidation-rmse:0.04536\n",
      "[418]\tvalidation-rmse:0.04517\n",
      "[419]\tvalidation-rmse:0.04498\n",
      "[420]\tvalidation-rmse:0.04479\n",
      "[421]\tvalidation-rmse:0.04461\n",
      "[422]\tvalidation-rmse:0.04443\n",
      "[423]\tvalidation-rmse:0.04427\n",
      "[424]\tvalidation-rmse:0.04410\n",
      "[425]\tvalidation-rmse:0.04392\n",
      "[426]\tvalidation-rmse:0.04376\n",
      "[427]\tvalidation-rmse:0.04360\n",
      "[428]\tvalidation-rmse:0.04343\n",
      "[429]\tvalidation-rmse:0.04325\n",
      "[430]\tvalidation-rmse:0.04310\n",
      "[431]\tvalidation-rmse:0.04295\n",
      "[432]\tvalidation-rmse:0.04282\n",
      "[433]\tvalidation-rmse:0.04268\n",
      "[434]\tvalidation-rmse:0.04251\n",
      "[435]\tvalidation-rmse:0.04236\n",
      "[436]\tvalidation-rmse:0.04220\n",
      "[437]\tvalidation-rmse:0.04203\n",
      "[438]\tvalidation-rmse:0.04189\n",
      "[439]\tvalidation-rmse:0.04174\n",
      "[440]\tvalidation-rmse:0.04158\n",
      "[441]\tvalidation-rmse:0.04143\n",
      "[442]\tvalidation-rmse:0.04133\n",
      "[443]\tvalidation-rmse:0.04118\n",
      "[444]\tvalidation-rmse:0.04103\n",
      "[445]\tvalidation-rmse:0.04089\n",
      "[446]\tvalidation-rmse:0.04075\n",
      "[447]\tvalidation-rmse:0.04063\n",
      "[448]\tvalidation-rmse:0.04050\n",
      "[449]\tvalidation-rmse:0.04037\n",
      "[450]\tvalidation-rmse:0.04024\n",
      "[451]\tvalidation-rmse:0.04012\n",
      "[452]\tvalidation-rmse:0.04002\n",
      "[453]\tvalidation-rmse:0.03991\n",
      "[454]\tvalidation-rmse:0.03979\n",
      "[455]\tvalidation-rmse:0.03967\n",
      "[456]\tvalidation-rmse:0.03955\n",
      "[457]\tvalidation-rmse:0.03942\n",
      "[458]\tvalidation-rmse:0.03930\n",
      "[459]\tvalidation-rmse:0.03919\n",
      "[460]\tvalidation-rmse:0.03907\n",
      "[461]\tvalidation-rmse:0.03895\n",
      "[462]\tvalidation-rmse:0.03883\n",
      "[463]\tvalidation-rmse:0.03872\n",
      "[464]\tvalidation-rmse:0.03862\n",
      "[465]\tvalidation-rmse:0.03851\n",
      "[466]\tvalidation-rmse:0.03841\n",
      "[467]\tvalidation-rmse:0.03830\n",
      "[468]\tvalidation-rmse:0.03820\n",
      "[469]\tvalidation-rmse:0.03809\n",
      "[470]\tvalidation-rmse:0.03798\n",
      "[471]\tvalidation-rmse:0.03788\n",
      "[472]\tvalidation-rmse:0.03778\n",
      "[473]\tvalidation-rmse:0.03767\n",
      "[474]\tvalidation-rmse:0.03757\n",
      "[475]\tvalidation-rmse:0.03747\n",
      "[476]\tvalidation-rmse:0.03737\n",
      "[477]\tvalidation-rmse:0.03727\n",
      "[478]\tvalidation-rmse:0.03718\n",
      "[479]\tvalidation-rmse:0.03709\n",
      "[480]\tvalidation-rmse:0.03702\n",
      "[481]\tvalidation-rmse:0.03693\n",
      "[482]\tvalidation-rmse:0.03685\n",
      "[483]\tvalidation-rmse:0.03675\n",
      "[484]\tvalidation-rmse:0.03666\n",
      "[485]\tvalidation-rmse:0.03658\n",
      "[486]\tvalidation-rmse:0.03649\n",
      "[487]\tvalidation-rmse:0.03641\n",
      "[488]\tvalidation-rmse:0.03632\n",
      "[489]\tvalidation-rmse:0.03624\n",
      "[490]\tvalidation-rmse:0.03617\n",
      "[491]\tvalidation-rmse:0.03609\n",
      "[492]\tvalidation-rmse:0.03600\n",
      "[493]\tvalidation-rmse:0.03592\n",
      "[494]\tvalidation-rmse:0.03584\n",
      "[495]\tvalidation-rmse:0.03577\n",
      "[496]\tvalidation-rmse:0.03570\n",
      "[497]\tvalidation-rmse:0.03562\n",
      "[498]\tvalidation-rmse:0.03553\n",
      "[499]\tvalidation-rmse:0.03546\n",
      "[500]\tvalidation-rmse:0.03538\n",
      "[501]\tvalidation-rmse:0.03531\n",
      "[502]\tvalidation-rmse:0.03526\n",
      "[503]\tvalidation-rmse:0.03518\n",
      "[504]\tvalidation-rmse:0.03511\n",
      "[505]\tvalidation-rmse:0.03504\n",
      "[506]\tvalidation-rmse:0.03496\n",
      "[507]\tvalidation-rmse:0.03489\n",
      "[508]\tvalidation-rmse:0.03483\n",
      "[509]\tvalidation-rmse:0.03476\n",
      "[510]\tvalidation-rmse:0.03470\n",
      "[511]\tvalidation-rmse:0.03464\n",
      "[512]\tvalidation-rmse:0.03458\n",
      "[513]\tvalidation-rmse:0.03452\n",
      "[514]\tvalidation-rmse:0.03445\n",
      "[515]\tvalidation-rmse:0.03440\n",
      "[516]\tvalidation-rmse:0.03435\n",
      "[517]\tvalidation-rmse:0.03429\n",
      "[518]\tvalidation-rmse:0.03423\n",
      "[519]\tvalidation-rmse:0.03417\n",
      "[520]\tvalidation-rmse:0.03411\n",
      "[521]\tvalidation-rmse:0.03405\n",
      "[522]\tvalidation-rmse:0.03400\n",
      "[523]\tvalidation-rmse:0.03396\n",
      "[524]\tvalidation-rmse:0.03390\n",
      "[525]\tvalidation-rmse:0.03386\n",
      "[526]\tvalidation-rmse:0.03380\n",
      "[527]\tvalidation-rmse:0.03375\n",
      "[528]\tvalidation-rmse:0.03370\n",
      "[529]\tvalidation-rmse:0.03364\n",
      "[530]\tvalidation-rmse:0.03359\n",
      "[531]\tvalidation-rmse:0.03357\n",
      "[532]\tvalidation-rmse:0.03353\n",
      "[533]\tvalidation-rmse:0.03349\n",
      "[534]\tvalidation-rmse:0.03344\n",
      "[535]\tvalidation-rmse:0.03339\n",
      "[536]\tvalidation-rmse:0.03336\n",
      "[537]\tvalidation-rmse:0.03332\n",
      "[538]\tvalidation-rmse:0.03327\n",
      "[539]\tvalidation-rmse:0.03322\n",
      "[540]\tvalidation-rmse:0.03319\n",
      "[541]\tvalidation-rmse:0.03315\n",
      "[542]\tvalidation-rmse:0.03312\n",
      "[543]\tvalidation-rmse:0.03308\n",
      "[544]\tvalidation-rmse:0.03302\n",
      "[545]\tvalidation-rmse:0.03298\n",
      "[546]\tvalidation-rmse:0.03295\n",
      "[547]\tvalidation-rmse:0.03291\n",
      "[548]\tvalidation-rmse:0.03289\n",
      "[549]\tvalidation-rmse:0.03286\n",
      "[550]\tvalidation-rmse:0.03282\n",
      "[551]\tvalidation-rmse:0.03280\n",
      "[552]\tvalidation-rmse:0.03276\n",
      "[553]\tvalidation-rmse:0.03272\n",
      "[554]\tvalidation-rmse:0.03266\n",
      "[555]\tvalidation-rmse:0.03262\n",
      "[556]\tvalidation-rmse:0.03258\n",
      "[557]\tvalidation-rmse:0.03255\n",
      "[558]\tvalidation-rmse:0.03251\n",
      "[559]\tvalidation-rmse:0.03247\n",
      "[560]\tvalidation-rmse:0.03244\n",
      "[561]\tvalidation-rmse:0.03241\n",
      "[562]\tvalidation-rmse:0.03238\n",
      "[563]\tvalidation-rmse:0.03234\n",
      "[564]\tvalidation-rmse:0.03230\n",
      "[565]\tvalidation-rmse:0.03226\n",
      "[566]\tvalidation-rmse:0.03223\n",
      "[567]\tvalidation-rmse:0.03220\n",
      "[568]\tvalidation-rmse:0.03218\n",
      "[569]\tvalidation-rmse:0.03215\n",
      "[570]\tvalidation-rmse:0.03212\n",
      "[571]\tvalidation-rmse:0.03210\n",
      "[572]\tvalidation-rmse:0.03206\n",
      "[573]\tvalidation-rmse:0.03203\n",
      "[574]\tvalidation-rmse:0.03199\n",
      "[575]\tvalidation-rmse:0.03196\n",
      "[576]\tvalidation-rmse:0.03192\n",
      "[577]\tvalidation-rmse:0.03192\n",
      "[578]\tvalidation-rmse:0.03189\n",
      "[579]\tvalidation-rmse:0.03186\n",
      "[580]\tvalidation-rmse:0.03182\n",
      "[581]\tvalidation-rmse:0.03179\n",
      "[582]\tvalidation-rmse:0.03178\n",
      "[583]\tvalidation-rmse:0.03175\n",
      "[584]\tvalidation-rmse:0.03171\n",
      "[585]\tvalidation-rmse:0.03168\n",
      "[586]\tvalidation-rmse:0.03166\n",
      "[587]\tvalidation-rmse:0.03163\n",
      "[588]\tvalidation-rmse:0.03160\n",
      "[589]\tvalidation-rmse:0.03157\n",
      "[590]\tvalidation-rmse:0.03155\n",
      "[591]\tvalidation-rmse:0.03152\n",
      "[592]\tvalidation-rmse:0.03149\n",
      "[593]\tvalidation-rmse:0.03147\n",
      "[594]\tvalidation-rmse:0.03145\n",
      "[595]\tvalidation-rmse:0.03142\n",
      "[596]\tvalidation-rmse:0.03140\n",
      "[597]\tvalidation-rmse:0.03137\n",
      "[598]\tvalidation-rmse:0.03134\n",
      "[599]\tvalidation-rmse:0.03132\n",
      "[600]\tvalidation-rmse:0.03129\n",
      "[601]\tvalidation-rmse:0.03126\n",
      "[602]\tvalidation-rmse:0.03123\n",
      "[603]\tvalidation-rmse:0.03121\n",
      "[604]\tvalidation-rmse:0.03121\n",
      "[605]\tvalidation-rmse:0.03119\n",
      "[606]\tvalidation-rmse:0.03116\n",
      "[607]\tvalidation-rmse:0.03113\n",
      "[608]\tvalidation-rmse:0.03111\n",
      "[609]\tvalidation-rmse:0.03108\n",
      "[610]\tvalidation-rmse:0.03106\n",
      "[611]\tvalidation-rmse:0.03104\n",
      "[612]\tvalidation-rmse:0.03102\n",
      "[613]\tvalidation-rmse:0.03100\n",
      "[614]\tvalidation-rmse:0.03097\n",
      "[615]\tvalidation-rmse:0.03094\n",
      "[616]\tvalidation-rmse:0.03092\n",
      "[617]\tvalidation-rmse:0.03090\n",
      "[618]\tvalidation-rmse:0.03088\n",
      "[619]\tvalidation-rmse:0.03085\n",
      "[620]\tvalidation-rmse:0.03083\n",
      "[621]\tvalidation-rmse:0.03082\n",
      "[622]\tvalidation-rmse:0.03080\n",
      "[623]\tvalidation-rmse:0.03078\n",
      "[624]\tvalidation-rmse:0.03078\n",
      "[625]\tvalidation-rmse:0.03076\n",
      "[626]\tvalidation-rmse:0.03074\n",
      "[627]\tvalidation-rmse:0.03072\n",
      "[628]\tvalidation-rmse:0.03073\n",
      "[629]\tvalidation-rmse:0.03071\n",
      "[630]\tvalidation-rmse:0.03069\n",
      "[631]\tvalidation-rmse:0.03067\n",
      "[632]\tvalidation-rmse:0.03065\n",
      "[633]\tvalidation-rmse:0.03063\n",
      "[634]\tvalidation-rmse:0.03063\n",
      "[635]\tvalidation-rmse:0.03062\n",
      "[636]\tvalidation-rmse:0.03060\n",
      "[637]\tvalidation-rmse:0.03059\n",
      "[638]\tvalidation-rmse:0.03057\n",
      "[639]\tvalidation-rmse:0.03055\n",
      "[640]\tvalidation-rmse:0.03054\n",
      "[641]\tvalidation-rmse:0.03053\n",
      "[642]\tvalidation-rmse:0.03051\n",
      "[643]\tvalidation-rmse:0.03049\n",
      "[644]\tvalidation-rmse:0.03047\n",
      "[645]\tvalidation-rmse:0.03045\n",
      "[646]\tvalidation-rmse:0.03043\n",
      "[647]\tvalidation-rmse:0.03042\n",
      "[648]\tvalidation-rmse:0.03040\n",
      "[649]\tvalidation-rmse:0.03039\n",
      "[650]\tvalidation-rmse:0.03038\n",
      "[651]\tvalidation-rmse:0.03036\n",
      "[652]\tvalidation-rmse:0.03035\n",
      "[653]\tvalidation-rmse:0.03034\n",
      "[654]\tvalidation-rmse:0.03035\n",
      "[655]\tvalidation-rmse:0.03034\n",
      "[656]\tvalidation-rmse:0.03033\n",
      "[657]\tvalidation-rmse:0.03031\n",
      "[658]\tvalidation-rmse:0.03030\n",
      "[659]\tvalidation-rmse:0.03029\n",
      "[660]\tvalidation-rmse:0.03027\n",
      "[661]\tvalidation-rmse:0.03026\n",
      "[662]\tvalidation-rmse:0.03025\n",
      "[663]\tvalidation-rmse:0.03023\n",
      "[664]\tvalidation-rmse:0.03022\n",
      "[665]\tvalidation-rmse:0.03021\n",
      "[666]\tvalidation-rmse:0.03020\n",
      "[667]\tvalidation-rmse:0.03019\n",
      "[668]\tvalidation-rmse:0.03018\n",
      "[669]\tvalidation-rmse:0.03016\n",
      "[670]\tvalidation-rmse:0.03017\n",
      "[671]\tvalidation-rmse:0.03015\n",
      "[672]\tvalidation-rmse:0.03013\n",
      "[673]\tvalidation-rmse:0.03011\n",
      "[674]\tvalidation-rmse:0.03010\n",
      "[675]\tvalidation-rmse:0.03009\n",
      "[676]\tvalidation-rmse:0.03008\n",
      "[677]\tvalidation-rmse:0.03008\n",
      "[678]\tvalidation-rmse:0.03006\n",
      "[679]\tvalidation-rmse:0.03006\n",
      "[680]\tvalidation-rmse:0.03004\n",
      "[681]\tvalidation-rmse:0.03003\n",
      "[682]\tvalidation-rmse:0.03002\n",
      "[683]\tvalidation-rmse:0.03003\n",
      "[684]\tvalidation-rmse:0.03002\n",
      "[685]\tvalidation-rmse:0.03000\n",
      "[686]\tvalidation-rmse:0.02999\n",
      "[687]\tvalidation-rmse:0.02998\n",
      "[688]\tvalidation-rmse:0.02998\n",
      "[689]\tvalidation-rmse:0.02997\n",
      "[690]\tvalidation-rmse:0.02996\n",
      "[691]\tvalidation-rmse:0.02995\n",
      "[692]\tvalidation-rmse:0.02994\n",
      "[693]\tvalidation-rmse:0.02992\n",
      "[694]\tvalidation-rmse:0.02992\n",
      "[695]\tvalidation-rmse:0.02991\n",
      "[696]\tvalidation-rmse:0.02990\n",
      "[697]\tvalidation-rmse:0.02988\n",
      "[698]\tvalidation-rmse:0.02988\n",
      "[699]\tvalidation-rmse:0.02987\n",
      "[700]\tvalidation-rmse:0.02986\n",
      "[701]\tvalidation-rmse:0.02984\n",
      "[702]\tvalidation-rmse:0.02983\n",
      "[703]\tvalidation-rmse:0.02982\n",
      "[704]\tvalidation-rmse:0.02981\n",
      "[705]\tvalidation-rmse:0.02980\n",
      "[706]\tvalidation-rmse:0.02980\n",
      "[707]\tvalidation-rmse:0.02979\n",
      "[708]\tvalidation-rmse:0.02978\n",
      "[709]\tvalidation-rmse:0.02977\n",
      "[710]\tvalidation-rmse:0.02977\n",
      "[711]\tvalidation-rmse:0.02975\n",
      "[712]\tvalidation-rmse:0.02974\n",
      "[713]\tvalidation-rmse:0.02974\n",
      "[714]\tvalidation-rmse:0.02973\n",
      "[715]\tvalidation-rmse:0.02973\n",
      "[716]\tvalidation-rmse:0.02972\n",
      "[717]\tvalidation-rmse:0.02971\n",
      "[718]\tvalidation-rmse:0.02969\n",
      "[719]\tvalidation-rmse:0.02968\n",
      "[720]\tvalidation-rmse:0.02968\n",
      "[721]\tvalidation-rmse:0.02967\n",
      "[722]\tvalidation-rmse:0.02966\n",
      "[723]\tvalidation-rmse:0.02966\n",
      "[724]\tvalidation-rmse:0.02965\n",
      "[725]\tvalidation-rmse:0.02964\n",
      "[726]\tvalidation-rmse:0.02963\n",
      "[727]\tvalidation-rmse:0.02962\n",
      "[728]\tvalidation-rmse:0.02961\n",
      "[729]\tvalidation-rmse:0.02960\n",
      "[730]\tvalidation-rmse:0.02959\n",
      "[731]\tvalidation-rmse:0.02959\n",
      "[732]\tvalidation-rmse:0.02958\n",
      "[733]\tvalidation-rmse:0.02957\n",
      "[734]\tvalidation-rmse:0.02956\n",
      "[735]\tvalidation-rmse:0.02956\n",
      "[736]\tvalidation-rmse:0.02956\n",
      "[737]\tvalidation-rmse:0.02955\n",
      "[738]\tvalidation-rmse:0.02953\n",
      "[739]\tvalidation-rmse:0.02953\n",
      "[740]\tvalidation-rmse:0.02952\n",
      "[741]\tvalidation-rmse:0.02952\n",
      "[742]\tvalidation-rmse:0.02951\n",
      "[743]\tvalidation-rmse:0.02951\n",
      "[744]\tvalidation-rmse:0.02950\n",
      "[745]\tvalidation-rmse:0.02950\n",
      "[746]\tvalidation-rmse:0.02949\n",
      "[747]\tvalidation-rmse:0.02948\n",
      "[748]\tvalidation-rmse:0.02948\n",
      "[749]\tvalidation-rmse:0.02947\n",
      "[750]\tvalidation-rmse:0.02946\n",
      "[751]\tvalidation-rmse:0.02945\n",
      "[752]\tvalidation-rmse:0.02944\n",
      "[753]\tvalidation-rmse:0.02943\n",
      "[754]\tvalidation-rmse:0.02943\n",
      "[755]\tvalidation-rmse:0.02941\n",
      "[756]\tvalidation-rmse:0.02941\n",
      "[757]\tvalidation-rmse:0.02940\n",
      "[758]\tvalidation-rmse:0.02939\n",
      "[759]\tvalidation-rmse:0.02939\n",
      "[760]\tvalidation-rmse:0.02938\n",
      "[761]\tvalidation-rmse:0.02937\n",
      "[762]\tvalidation-rmse:0.02937\n",
      "[763]\tvalidation-rmse:0.02937\n",
      "[764]\tvalidation-rmse:0.02936\n",
      "[765]\tvalidation-rmse:0.02936\n",
      "[766]\tvalidation-rmse:0.02935\n",
      "[767]\tvalidation-rmse:0.02935\n",
      "[768]\tvalidation-rmse:0.02934\n",
      "[769]\tvalidation-rmse:0.02934\n",
      "[770]\tvalidation-rmse:0.02934\n",
      "[771]\tvalidation-rmse:0.02934\n",
      "[772]\tvalidation-rmse:0.02933\n",
      "[773]\tvalidation-rmse:0.02932\n",
      "[774]\tvalidation-rmse:0.02932\n",
      "[775]\tvalidation-rmse:0.02931\n",
      "[776]\tvalidation-rmse:0.02931\n",
      "[777]\tvalidation-rmse:0.02930\n",
      "[778]\tvalidation-rmse:0.02929\n",
      "[779]\tvalidation-rmse:0.02929\n",
      "[780]\tvalidation-rmse:0.02928\n",
      "[781]\tvalidation-rmse:0.02928\n",
      "[782]\tvalidation-rmse:0.02927\n",
      "[783]\tvalidation-rmse:0.02927\n",
      "[784]\tvalidation-rmse:0.02927\n",
      "[785]\tvalidation-rmse:0.02926\n",
      "[786]\tvalidation-rmse:0.02926\n",
      "[787]\tvalidation-rmse:0.02926\n",
      "[788]\tvalidation-rmse:0.02925\n",
      "[789]\tvalidation-rmse:0.02925\n",
      "[790]\tvalidation-rmse:0.02925\n",
      "[791]\tvalidation-rmse:0.02924\n",
      "[792]\tvalidation-rmse:0.02923\n",
      "[793]\tvalidation-rmse:0.02923\n",
      "[794]\tvalidation-rmse:0.02924\n",
      "[795]\tvalidation-rmse:0.02923\n",
      "[796]\tvalidation-rmse:0.02922\n",
      "[797]\tvalidation-rmse:0.02922\n",
      "[798]\tvalidation-rmse:0.02921\n",
      "[799]\tvalidation-rmse:0.02921\n",
      "[800]\tvalidation-rmse:0.02921\n",
      "[801]\tvalidation-rmse:0.02920\n",
      "[802]\tvalidation-rmse:0.02920\n",
      "[803]\tvalidation-rmse:0.02919\n",
      "[804]\tvalidation-rmse:0.02918\n",
      "[805]\tvalidation-rmse:0.02917\n",
      "[806]\tvalidation-rmse:0.02918\n",
      "[807]\tvalidation-rmse:0.02917\n",
      "[808]\tvalidation-rmse:0.02917\n",
      "[809]\tvalidation-rmse:0.02916\n",
      "[810]\tvalidation-rmse:0.02916\n",
      "[811]\tvalidation-rmse:0.02915\n",
      "[812]\tvalidation-rmse:0.02915\n",
      "[813]\tvalidation-rmse:0.02914\n",
      "[814]\tvalidation-rmse:0.02914\n",
      "[815]\tvalidation-rmse:0.02914\n",
      "[816]\tvalidation-rmse:0.02913\n",
      "[817]\tvalidation-rmse:0.02913\n",
      "[818]\tvalidation-rmse:0.02913\n",
      "[819]\tvalidation-rmse:0.02912\n",
      "[820]\tvalidation-rmse:0.02912\n",
      "[821]\tvalidation-rmse:0.02911\n",
      "[822]\tvalidation-rmse:0.02911\n",
      "[823]\tvalidation-rmse:0.02910\n",
      "[824]\tvalidation-rmse:0.02910\n",
      "[825]\tvalidation-rmse:0.02909\n",
      "[826]\tvalidation-rmse:0.02909\n",
      "[827]\tvalidation-rmse:0.02908\n",
      "[828]\tvalidation-rmse:0.02908\n",
      "[829]\tvalidation-rmse:0.02908\n",
      "[830]\tvalidation-rmse:0.02907\n",
      "[831]\tvalidation-rmse:0.02906\n",
      "[832]\tvalidation-rmse:0.02906\n",
      "[833]\tvalidation-rmse:0.02906\n",
      "[834]\tvalidation-rmse:0.02905\n",
      "[835]\tvalidation-rmse:0.02905\n",
      "[836]\tvalidation-rmse:0.02905\n",
      "[837]\tvalidation-rmse:0.02904\n",
      "[838]\tvalidation-rmse:0.02904\n",
      "[839]\tvalidation-rmse:0.02903\n",
      "[840]\tvalidation-rmse:0.02903\n",
      "[841]\tvalidation-rmse:0.02902\n",
      "[842]\tvalidation-rmse:0.02903\n",
      "[843]\tvalidation-rmse:0.02902\n",
      "[844]\tvalidation-rmse:0.02902\n",
      "[845]\tvalidation-rmse:0.02901\n",
      "[846]\tvalidation-rmse:0.02901\n",
      "[847]\tvalidation-rmse:0.02900\n",
      "[848]\tvalidation-rmse:0.02900\n",
      "[849]\tvalidation-rmse:0.02899\n",
      "[850]\tvalidation-rmse:0.02899\n",
      "[851]\tvalidation-rmse:0.02899\n",
      "[852]\tvalidation-rmse:0.02899\n",
      "[853]\tvalidation-rmse:0.02899\n",
      "[854]\tvalidation-rmse:0.02898\n",
      "[855]\tvalidation-rmse:0.02898\n",
      "[856]\tvalidation-rmse:0.02898\n",
      "[857]\tvalidation-rmse:0.02897\n",
      "[858]\tvalidation-rmse:0.02897\n",
      "[859]\tvalidation-rmse:0.02897\n",
      "[860]\tvalidation-rmse:0.02897\n",
      "[861]\tvalidation-rmse:0.02896\n",
      "[862]\tvalidation-rmse:0.02896\n",
      "[863]\tvalidation-rmse:0.02896\n",
      "[864]\tvalidation-rmse:0.02895\n",
      "[865]\tvalidation-rmse:0.02895\n",
      "[866]\tvalidation-rmse:0.02894\n",
      "[867]\tvalidation-rmse:0.02894\n",
      "[868]\tvalidation-rmse:0.02894\n",
      "[869]\tvalidation-rmse:0.02894\n",
      "[870]\tvalidation-rmse:0.02893\n",
      "[871]\tvalidation-rmse:0.02892\n",
      "[872]\tvalidation-rmse:0.02892\n",
      "[873]\tvalidation-rmse:0.02892\n",
      "[874]\tvalidation-rmse:0.02891\n",
      "[875]\tvalidation-rmse:0.02890\n",
      "[876]\tvalidation-rmse:0.02890\n",
      "[877]\tvalidation-rmse:0.02890\n",
      "[878]\tvalidation-rmse:0.02890\n",
      "[879]\tvalidation-rmse:0.02889\n",
      "[880]\tvalidation-rmse:0.02889\n",
      "[881]\tvalidation-rmse:0.02889\n",
      "[882]\tvalidation-rmse:0.02889\n",
      "[883]\tvalidation-rmse:0.02889\n",
      "[884]\tvalidation-rmse:0.02888\n",
      "[885]\tvalidation-rmse:0.02888\n",
      "[886]\tvalidation-rmse:0.02888\n",
      "[887]\tvalidation-rmse:0.02887\n",
      "[888]\tvalidation-rmse:0.02887\n",
      "[889]\tvalidation-rmse:0.02887\n",
      "[890]\tvalidation-rmse:0.02887\n",
      "[891]\tvalidation-rmse:0.02887\n",
      "[892]\tvalidation-rmse:0.02886\n",
      "[893]\tvalidation-rmse:0.02887\n",
      "[894]\tvalidation-rmse:0.02886\n",
      "[895]\tvalidation-rmse:0.02886\n",
      "[896]\tvalidation-rmse:0.02886\n",
      "[897]\tvalidation-rmse:0.02886\n",
      "[898]\tvalidation-rmse:0.02885\n",
      "[899]\tvalidation-rmse:0.02884\n",
      "[900]\tvalidation-rmse:0.02884\n",
      "[901]\tvalidation-rmse:0.02884\n",
      "[902]\tvalidation-rmse:0.02883\n",
      "[903]\tvalidation-rmse:0.02882\n",
      "[904]\tvalidation-rmse:0.02882\n",
      "[905]\tvalidation-rmse:0.02882\n",
      "[906]\tvalidation-rmse:0.02882\n",
      "[907]\tvalidation-rmse:0.02881\n",
      "[908]\tvalidation-rmse:0.02882\n",
      "[909]\tvalidation-rmse:0.02881\n",
      "[910]\tvalidation-rmse:0.02881\n",
      "[911]\tvalidation-rmse:0.02880\n",
      "[912]\tvalidation-rmse:0.02880\n",
      "[913]\tvalidation-rmse:0.02879\n",
      "[914]\tvalidation-rmse:0.02879\n",
      "[915]\tvalidation-rmse:0.02878\n",
      "[916]\tvalidation-rmse:0.02878\n",
      "[917]\tvalidation-rmse:0.02877\n",
      "[918]\tvalidation-rmse:0.02877\n",
      "[919]\tvalidation-rmse:0.02877\n",
      "[920]\tvalidation-rmse:0.02876\n",
      "[921]\tvalidation-rmse:0.02876\n",
      "[922]\tvalidation-rmse:0.02876\n",
      "[923]\tvalidation-rmse:0.02875\n",
      "[924]\tvalidation-rmse:0.02875\n",
      "[925]\tvalidation-rmse:0.02875\n",
      "[926]\tvalidation-rmse:0.02874\n",
      "[927]\tvalidation-rmse:0.02874\n",
      "[928]\tvalidation-rmse:0.02874\n",
      "[929]\tvalidation-rmse:0.02873\n",
      "[930]\tvalidation-rmse:0.02873\n",
      "[931]\tvalidation-rmse:0.02873\n",
      "[932]\tvalidation-rmse:0.02872\n",
      "[933]\tvalidation-rmse:0.02871\n",
      "[934]\tvalidation-rmse:0.02871\n",
      "[935]\tvalidation-rmse:0.02871\n",
      "[936]\tvalidation-rmse:0.02870\n",
      "[937]\tvalidation-rmse:0.02870\n",
      "[938]\tvalidation-rmse:0.02870\n",
      "[939]\tvalidation-rmse:0.02869\n",
      "[940]\tvalidation-rmse:0.02869\n",
      "[941]\tvalidation-rmse:0.02870\n",
      "[942]\tvalidation-rmse:0.02869\n",
      "[943]\tvalidation-rmse:0.02869\n",
      "[944]\tvalidation-rmse:0.02869\n",
      "[945]\tvalidation-rmse:0.02868\n",
      "[946]\tvalidation-rmse:0.02868\n",
      "[947]\tvalidation-rmse:0.02867\n",
      "[948]\tvalidation-rmse:0.02867\n",
      "[949]\tvalidation-rmse:0.02867\n",
      "[950]\tvalidation-rmse:0.02867\n",
      "[951]\tvalidation-rmse:0.02866\n",
      "[952]\tvalidation-rmse:0.02866\n",
      "[953]\tvalidation-rmse:0.02867\n",
      "[954]\tvalidation-rmse:0.02866\n",
      "[955]\tvalidation-rmse:0.02866\n",
      "[956]\tvalidation-rmse:0.02866\n",
      "[957]\tvalidation-rmse:0.02866\n",
      "[958]\tvalidation-rmse:0.02866\n",
      "[959]\tvalidation-rmse:0.02865\n",
      "[960]\tvalidation-rmse:0.02865\n",
      "[961]\tvalidation-rmse:0.02865\n",
      "[962]\tvalidation-rmse:0.02865\n",
      "[963]\tvalidation-rmse:0.02865\n",
      "[964]\tvalidation-rmse:0.02865\n",
      "[965]\tvalidation-rmse:0.02864\n",
      "[966]\tvalidation-rmse:0.02864\n",
      "[967]\tvalidation-rmse:0.02864\n",
      "[968]\tvalidation-rmse:0.02864\n",
      "[969]\tvalidation-rmse:0.02864\n",
      "[970]\tvalidation-rmse:0.02864\n",
      "[971]\tvalidation-rmse:0.02864\n",
      "[972]\tvalidation-rmse:0.02864\n",
      "[973]\tvalidation-rmse:0.02863\n",
      "[974]\tvalidation-rmse:0.02863\n",
      "[975]\tvalidation-rmse:0.02863\n",
      "[976]\tvalidation-rmse:0.02863\n",
      "[977]\tvalidation-rmse:0.02863\n",
      "[978]\tvalidation-rmse:0.02862\n",
      "[979]\tvalidation-rmse:0.02862\n",
      "[980]\tvalidation-rmse:0.02862\n",
      "[981]\tvalidation-rmse:0.02862\n",
      "[982]\tvalidation-rmse:0.02861\n",
      "[983]\tvalidation-rmse:0.02861\n",
      "[984]\tvalidation-rmse:0.02861\n",
      "[985]\tvalidation-rmse:0.02861\n",
      "[986]\tvalidation-rmse:0.02861\n",
      "[987]\tvalidation-rmse:0.02860\n",
      "[988]\tvalidation-rmse:0.02860\n",
      "[989]\tvalidation-rmse:0.02860\n",
      "[990]\tvalidation-rmse:0.02860\n",
      "[991]\tvalidation-rmse:0.02860\n",
      "[992]\tvalidation-rmse:0.02860\n",
      "[993]\tvalidation-rmse:0.02859\n",
      "[994]\tvalidation-rmse:0.02859\n",
      "[995]\tvalidation-rmse:0.02859\n",
      "[996]\tvalidation-rmse:0.02859\n",
      "[997]\tvalidation-rmse:0.02859\n",
      "[998]\tvalidation-rmse:0.02859\n",
      "[999]\tvalidation-rmse:0.02858\n",
      "[1000]\tvalidation-rmse:0.02858\n",
      "[1001]\tvalidation-rmse:0.02858\n",
      "[1002]\tvalidation-rmse:0.02858\n",
      "[1003]\tvalidation-rmse:0.02858\n",
      "[1004]\tvalidation-rmse:0.02858\n",
      "[1005]\tvalidation-rmse:0.02857\n",
      "[1006]\tvalidation-rmse:0.02858\n",
      "[1007]\tvalidation-rmse:0.02857\n",
      "[1008]\tvalidation-rmse:0.02857\n",
      "[1009]\tvalidation-rmse:0.02857\n",
      "[1010]\tvalidation-rmse:0.02857\n",
      "[1011]\tvalidation-rmse:0.02857\n",
      "[1012]\tvalidation-rmse:0.02857\n",
      "[1013]\tvalidation-rmse:0.02856\n",
      "[1014]\tvalidation-rmse:0.02856\n",
      "[1015]\tvalidation-rmse:0.02856\n",
      "[1016]\tvalidation-rmse:0.02856\n",
      "[1017]\tvalidation-rmse:0.02856\n",
      "[1018]\tvalidation-rmse:0.02855\n",
      "[1019]\tvalidation-rmse:0.02856\n",
      "[1020]\tvalidation-rmse:0.02856\n",
      "[1021]\tvalidation-rmse:0.02856\n",
      "[1022]\tvalidation-rmse:0.02855\n",
      "[1023]\tvalidation-rmse:0.02855\n",
      "[1024]\tvalidation-rmse:0.02855\n",
      "[1025]\tvalidation-rmse:0.02854\n",
      "[1026]\tvalidation-rmse:0.02855\n",
      "[1027]\tvalidation-rmse:0.02854\n",
      "[1028]\tvalidation-rmse:0.02854\n",
      "[1029]\tvalidation-rmse:0.02854\n",
      "[1030]\tvalidation-rmse:0.02854\n",
      "[1031]\tvalidation-rmse:0.02854\n",
      "[1032]\tvalidation-rmse:0.02853\n",
      "[1033]\tvalidation-rmse:0.02853\n",
      "[1034]\tvalidation-rmse:0.02853\n",
      "[1035]\tvalidation-rmse:0.02853\n",
      "[1036]\tvalidation-rmse:0.02853\n",
      "[1037]\tvalidation-rmse:0.02853\n",
      "[1038]\tvalidation-rmse:0.02852\n",
      "[1039]\tvalidation-rmse:0.02852\n",
      "[1040]\tvalidation-rmse:0.02852\n",
      "[1041]\tvalidation-rmse:0.02853\n",
      "[1042]\tvalidation-rmse:0.02852\n",
      "[1043]\tvalidation-rmse:0.02852\n",
      "[1044]\tvalidation-rmse:0.02852\n",
      "[1045]\tvalidation-rmse:0.02852\n",
      "[1046]\tvalidation-rmse:0.02852\n",
      "[1047]\tvalidation-rmse:0.02852\n",
      "[1048]\tvalidation-rmse:0.02851\n",
      "[1049]\tvalidation-rmse:0.02851\n",
      "[1050]\tvalidation-rmse:0.02851\n",
      "[1051]\tvalidation-rmse:0.02851\n",
      "[1052]\tvalidation-rmse:0.02851\n",
      "[1053]\tvalidation-rmse:0.02850\n",
      "[1054]\tvalidation-rmse:0.02850\n",
      "[1055]\tvalidation-rmse:0.02850\n",
      "[1056]\tvalidation-rmse:0.02850\n",
      "[1057]\tvalidation-rmse:0.02850\n",
      "[1058]\tvalidation-rmse:0.02849\n",
      "[1059]\tvalidation-rmse:0.02849\n",
      "[1060]\tvalidation-rmse:0.02849\n",
      "[1061]\tvalidation-rmse:0.02849\n",
      "[1062]\tvalidation-rmse:0.02849\n",
      "[1063]\tvalidation-rmse:0.02848\n",
      "[1064]\tvalidation-rmse:0.02848\n",
      "[1065]\tvalidation-rmse:0.02848\n",
      "[1066]\tvalidation-rmse:0.02848\n",
      "[1067]\tvalidation-rmse:0.02847\n",
      "[1068]\tvalidation-rmse:0.02847\n",
      "[1069]\tvalidation-rmse:0.02847\n",
      "[1070]\tvalidation-rmse:0.02847\n",
      "[1071]\tvalidation-rmse:0.02847\n",
      "[1072]\tvalidation-rmse:0.02846\n",
      "[1073]\tvalidation-rmse:0.02846\n",
      "[1074]\tvalidation-rmse:0.02846\n",
      "[1075]\tvalidation-rmse:0.02846\n",
      "[1076]\tvalidation-rmse:0.02846\n",
      "[1077]\tvalidation-rmse:0.02846\n",
      "[1078]\tvalidation-rmse:0.02846\n",
      "[1079]\tvalidation-rmse:0.02846\n",
      "[1080]\tvalidation-rmse:0.02846\n",
      "[1081]\tvalidation-rmse:0.02845\n",
      "[1082]\tvalidation-rmse:0.02845\n",
      "[1083]\tvalidation-rmse:0.02845\n",
      "[1084]\tvalidation-rmse:0.02845\n",
      "[1085]\tvalidation-rmse:0.02845\n",
      "[1086]\tvalidation-rmse:0.02844\n",
      "[1087]\tvalidation-rmse:0.02844\n",
      "[1088]\tvalidation-rmse:0.02844\n",
      "[1089]\tvalidation-rmse:0.02844\n",
      "[1090]\tvalidation-rmse:0.02844\n",
      "[1091]\tvalidation-rmse:0.02844\n",
      "[1092]\tvalidation-rmse:0.02843\n",
      "[1093]\tvalidation-rmse:0.02843\n",
      "[1094]\tvalidation-rmse:0.02843\n",
      "[1095]\tvalidation-rmse:0.02843\n",
      "[1096]\tvalidation-rmse:0.02843\n",
      "[1097]\tvalidation-rmse:0.02843\n",
      "[1098]\tvalidation-rmse:0.02843\n",
      "[1099]\tvalidation-rmse:0.02843\n",
      "[1100]\tvalidation-rmse:0.02842\n",
      "[1101]\tvalidation-rmse:0.02842\n",
      "[1102]\tvalidation-rmse:0.02844\n",
      "[1103]\tvalidation-rmse:0.02844\n",
      "[1104]\tvalidation-rmse:0.02844\n",
      "[1105]\tvalidation-rmse:0.02843\n",
      "[1106]\tvalidation-rmse:0.02843\n",
      "[1107]\tvalidation-rmse:0.02843\n",
      "[1108]\tvalidation-rmse:0.02843\n",
      "[1109]\tvalidation-rmse:0.02843\n",
      "[1110]\tvalidation-rmse:0.02843\n",
      "[1111]\tvalidation-rmse:0.02843\n",
      "[1112]\tvalidation-rmse:0.02843\n",
      "[1113]\tvalidation-rmse:0.02843\n",
      "[1114]\tvalidation-rmse:0.02843\n",
      "[1115]\tvalidation-rmse:0.02843\n",
      "[1116]\tvalidation-rmse:0.02843\n",
      "[1117]\tvalidation-rmse:0.02843\n",
      "[1118]\tvalidation-rmse:0.02843\n",
      "[1119]\tvalidation-rmse:0.02842\n",
      "[1120]\tvalidation-rmse:0.02842\n",
      "[1121]\tvalidation-rmse:0.02842\n",
      "[1122]\tvalidation-rmse:0.02842\n",
      "[1123]\tvalidation-rmse:0.02842\n",
      "[1124]\tvalidation-rmse:0.02842\n",
      "[1125]\tvalidation-rmse:0.02842\n",
      "[1126]\tvalidation-rmse:0.02842\n",
      "[1127]\tvalidation-rmse:0.02842\n",
      "[1128]\tvalidation-rmse:0.02842\n",
      "[1129]\tvalidation-rmse:0.02842\n",
      "[1130]\tvalidation-rmse:0.02842\n",
      "[1131]\tvalidation-rmse:0.02841\n",
      "[1132]\tvalidation-rmse:0.02841\n",
      "[1133]\tvalidation-rmse:0.02841\n",
      "[1134]\tvalidation-rmse:0.02841\n",
      "[1135]\tvalidation-rmse:0.02841\n",
      "[1136]\tvalidation-rmse:0.02841\n",
      "[1137]\tvalidation-rmse:0.02840\n",
      "[1138]\tvalidation-rmse:0.02840\n",
      "[1139]\tvalidation-rmse:0.02840\n",
      "[1140]\tvalidation-rmse:0.02840\n",
      "[1141]\tvalidation-rmse:0.02840\n",
      "[1142]\tvalidation-rmse:0.02840\n",
      "[1143]\tvalidation-rmse:0.02840\n",
      "[1144]\tvalidation-rmse:0.02839\n",
      "[1145]\tvalidation-rmse:0.02839\n",
      "[1146]\tvalidation-rmse:0.02839\n",
      "[1147]\tvalidation-rmse:0.02839\n",
      "[1148]\tvalidation-rmse:0.02839\n",
      "[1149]\tvalidation-rmse:0.02838\n",
      "[1150]\tvalidation-rmse:0.02838\n",
      "[1151]\tvalidation-rmse:0.02838\n",
      "[1152]\tvalidation-rmse:0.02838\n",
      "[1153]\tvalidation-rmse:0.02838\n",
      "[1154]\tvalidation-rmse:0.02838\n",
      "[1155]\tvalidation-rmse:0.02838\n",
      "[1156]\tvalidation-rmse:0.02838\n",
      "[1157]\tvalidation-rmse:0.02837\n",
      "[1158]\tvalidation-rmse:0.02837\n",
      "[1159]\tvalidation-rmse:0.02837\n",
      "[1160]\tvalidation-rmse:0.02837\n",
      "[1161]\tvalidation-rmse:0.02837\n",
      "[1162]\tvalidation-rmse:0.02837\n",
      "[1163]\tvalidation-rmse:0.02837\n",
      "[1164]\tvalidation-rmse:0.02836\n",
      "[1165]\tvalidation-rmse:0.02836\n",
      "[1166]\tvalidation-rmse:0.02836\n",
      "[1167]\tvalidation-rmse:0.02836\n",
      "[1168]\tvalidation-rmse:0.02836\n",
      "[1169]\tvalidation-rmse:0.02835\n",
      "[1170]\tvalidation-rmse:0.02836\n",
      "[1171]\tvalidation-rmse:0.02835\n",
      "[1172]\tvalidation-rmse:0.02835\n",
      "[1173]\tvalidation-rmse:0.02835\n",
      "[1174]\tvalidation-rmse:0.02835\n",
      "[1175]\tvalidation-rmse:0.02835\n",
      "[1176]\tvalidation-rmse:0.02834\n",
      "[1177]\tvalidation-rmse:0.02834\n",
      "[1178]\tvalidation-rmse:0.02834\n",
      "[1179]\tvalidation-rmse:0.02834\n",
      "[1180]\tvalidation-rmse:0.02834\n",
      "[1181]\tvalidation-rmse:0.02834\n",
      "[1182]\tvalidation-rmse:0.02834\n",
      "[1183]\tvalidation-rmse:0.02834\n",
      "[1184]\tvalidation-rmse:0.02834\n",
      "[1185]\tvalidation-rmse:0.02833\n",
      "[1186]\tvalidation-rmse:0.02834\n",
      "[1187]\tvalidation-rmse:0.02833\n",
      "[1188]\tvalidation-rmse:0.02833\n",
      "[1189]\tvalidation-rmse:0.02833\n",
      "[1190]\tvalidation-rmse:0.02833\n",
      "[1191]\tvalidation-rmse:0.02833\n",
      "[1192]\tvalidation-rmse:0.02832\n",
      "[1193]\tvalidation-rmse:0.02832\n",
      "[1194]\tvalidation-rmse:0.02832\n",
      "[1195]\tvalidation-rmse:0.02832\n",
      "[1196]\tvalidation-rmse:0.02831\n",
      "[1197]\tvalidation-rmse:0.02831\n",
      "[1198]\tvalidation-rmse:0.02831\n",
      "[1199]\tvalidation-rmse:0.02831\n",
      "[1200]\tvalidation-rmse:0.02831\n",
      "[1201]\tvalidation-rmse:0.02831\n",
      "[1202]\tvalidation-rmse:0.02831\n",
      "[1203]\tvalidation-rmse:0.02831\n",
      "[1204]\tvalidation-rmse:0.02830\n",
      "[1205]\tvalidation-rmse:0.02830\n",
      "[1206]\tvalidation-rmse:0.02830\n",
      "[1207]\tvalidation-rmse:0.02830\n",
      "[1208]\tvalidation-rmse:0.02830\n",
      "[1209]\tvalidation-rmse:0.02830\n",
      "[1210]\tvalidation-rmse:0.02830\n",
      "[1211]\tvalidation-rmse:0.02830\n",
      "[1212]\tvalidation-rmse:0.02830\n",
      "[1213]\tvalidation-rmse:0.02829\n",
      "[1214]\tvalidation-rmse:0.02830\n",
      "[1215]\tvalidation-rmse:0.02830\n",
      "[1216]\tvalidation-rmse:0.02829\n",
      "[1217]\tvalidation-rmse:0.02829\n",
      "[1218]\tvalidation-rmse:0.02829\n",
      "[1219]\tvalidation-rmse:0.02829\n",
      "[1220]\tvalidation-rmse:0.02829\n",
      "[1221]\tvalidation-rmse:0.02829\n",
      "[1222]\tvalidation-rmse:0.02829\n",
      "[1223]\tvalidation-rmse:0.02829\n",
      "[1224]\tvalidation-rmse:0.02829\n",
      "[1225]\tvalidation-rmse:0.02829\n",
      "[1226]\tvalidation-rmse:0.02828\n",
      "[1227]\tvalidation-rmse:0.02828\n",
      "[1228]\tvalidation-rmse:0.02828\n",
      "[1229]\tvalidation-rmse:0.02828\n",
      "[1230]\tvalidation-rmse:0.02828\n",
      "[1231]\tvalidation-rmse:0.02828\n",
      "[1232]\tvalidation-rmse:0.02828\n",
      "[1233]\tvalidation-rmse:0.02828\n",
      "[1234]\tvalidation-rmse:0.02828\n",
      "[1235]\tvalidation-rmse:0.02827\n",
      "[1236]\tvalidation-rmse:0.02827\n",
      "[1237]\tvalidation-rmse:0.02827\n",
      "[1238]\tvalidation-rmse:0.02827\n",
      "[1239]\tvalidation-rmse:0.02827\n",
      "[1240]\tvalidation-rmse:0.02827\n",
      "[1241]\tvalidation-rmse:0.02827\n",
      "[1242]\tvalidation-rmse:0.02827\n",
      "[1243]\tvalidation-rmse:0.02827\n",
      "[1244]\tvalidation-rmse:0.02826\n",
      "[1245]\tvalidation-rmse:0.02826\n",
      "[1246]\tvalidation-rmse:0.02826\n",
      "[1247]\tvalidation-rmse:0.02826\n",
      "[1248]\tvalidation-rmse:0.02826\n",
      "[1249]\tvalidation-rmse:0.02826\n",
      "[1250]\tvalidation-rmse:0.02826\n",
      "[1251]\tvalidation-rmse:0.02826\n",
      "[1252]\tvalidation-rmse:0.02826\n",
      "[1253]\tvalidation-rmse:0.02825\n",
      "[1254]\tvalidation-rmse:0.02825\n",
      "[1255]\tvalidation-rmse:0.02825\n",
      "[1256]\tvalidation-rmse:0.02825\n",
      "[1257]\tvalidation-rmse:0.02825\n",
      "[1258]\tvalidation-rmse:0.02825\n",
      "[1259]\tvalidation-rmse:0.02825\n",
      "[1260]\tvalidation-rmse:0.02825\n",
      "[1261]\tvalidation-rmse:0.02825\n",
      "[1262]\tvalidation-rmse:0.02825\n",
      "[1263]\tvalidation-rmse:0.02825\n",
      "[1264]\tvalidation-rmse:0.02824\n",
      "[1265]\tvalidation-rmse:0.02824\n",
      "[1266]\tvalidation-rmse:0.02824\n",
      "[1267]\tvalidation-rmse:0.02824\n",
      "[1268]\tvalidation-rmse:0.02824\n",
      "[1269]\tvalidation-rmse:0.02824\n",
      "[1270]\tvalidation-rmse:0.02824\n",
      "[1271]\tvalidation-rmse:0.02824\n",
      "[1272]\tvalidation-rmse:0.02824\n",
      "[1273]\tvalidation-rmse:0.02824\n",
      "[1274]\tvalidation-rmse:0.02824\n",
      "[1275]\tvalidation-rmse:0.02824\n",
      "[1276]\tvalidation-rmse:0.02824\n",
      "[1277]\tvalidation-rmse:0.02824\n",
      "[1278]\tvalidation-rmse:0.02824\n",
      "[1279]\tvalidation-rmse:0.02824\n",
      "[1280]\tvalidation-rmse:0.02824\n",
      "[1281]\tvalidation-rmse:0.02823\n",
      "[1282]\tvalidation-rmse:0.02823\n",
      "[1283]\tvalidation-rmse:0.02823\n",
      "[1284]\tvalidation-rmse:0.02822\n",
      "[1285]\tvalidation-rmse:0.02822\n",
      "[1286]\tvalidation-rmse:0.02822\n",
      "[1287]\tvalidation-rmse:0.02822\n",
      "[1288]\tvalidation-rmse:0.02822\n",
      "[1289]\tvalidation-rmse:0.02822\n",
      "[1290]\tvalidation-rmse:0.02822\n",
      "[1291]\tvalidation-rmse:0.02822\n",
      "[1292]\tvalidation-rmse:0.02822\n",
      "[1293]\tvalidation-rmse:0.02822\n",
      "[1294]\tvalidation-rmse:0.02822\n",
      "[1295]\tvalidation-rmse:0.02822\n",
      "[1296]\tvalidation-rmse:0.02822\n",
      "[1297]\tvalidation-rmse:0.02822\n",
      "[1298]\tvalidation-rmse:0.02822\n",
      "[1299]\tvalidation-rmse:0.02822\n",
      "[1300]\tvalidation-rmse:0.02822\n",
      "[1301]\tvalidation-rmse:0.02822\n",
      "[1302]\tvalidation-rmse:0.02822\n",
      "[1303]\tvalidation-rmse:0.02821\n",
      "[1304]\tvalidation-rmse:0.02821\n",
      "[1305]\tvalidation-rmse:0.02821\n",
      "[1306]\tvalidation-rmse:0.02821\n",
      "[1307]\tvalidation-rmse:0.02821\n",
      "[1308]\tvalidation-rmse:0.02821\n",
      "[1309]\tvalidation-rmse:0.02821\n",
      "[1310]\tvalidation-rmse:0.02821\n",
      "[1311]\tvalidation-rmse:0.02820\n",
      "[1312]\tvalidation-rmse:0.02820\n",
      "[1313]\tvalidation-rmse:0.02819\n",
      "[1314]\tvalidation-rmse:0.02819\n",
      "[1315]\tvalidation-rmse:0.02819\n",
      "[1316]\tvalidation-rmse:0.02819\n",
      "[1317]\tvalidation-rmse:0.02819\n",
      "[1318]\tvalidation-rmse:0.02819\n",
      "[1319]\tvalidation-rmse:0.02819\n",
      "[1320]\tvalidation-rmse:0.02819\n",
      "[1321]\tvalidation-rmse:0.02818\n",
      "[1322]\tvalidation-rmse:0.02818\n",
      "[1323]\tvalidation-rmse:0.02818\n",
      "[1324]\tvalidation-rmse:0.02818\n",
      "[1325]\tvalidation-rmse:0.02818\n",
      "[1326]\tvalidation-rmse:0.02819\n",
      "[1327]\tvalidation-rmse:0.02819\n",
      "[1328]\tvalidation-rmse:0.02819\n",
      "[1329]\tvalidation-rmse:0.02819\n",
      "[1330]\tvalidation-rmse:0.02819\n",
      "[1331]\tvalidation-rmse:0.02819\n",
      "[1332]\tvalidation-rmse:0.02818\n",
      "[1333]\tvalidation-rmse:0.02818\n",
      "[1334]\tvalidation-rmse:0.02818\n",
      "[1335]\tvalidation-rmse:0.02818\n",
      "[1336]\tvalidation-rmse:0.02818\n",
      "[1337]\tvalidation-rmse:0.02818\n",
      "[1338]\tvalidation-rmse:0.02818\n",
      "[1339]\tvalidation-rmse:0.02818\n",
      "[1340]\tvalidation-rmse:0.02817\n",
      "[1341]\tvalidation-rmse:0.02817\n",
      "[1342]\tvalidation-rmse:0.02817\n",
      "[1343]\tvalidation-rmse:0.02817\n",
      "[1344]\tvalidation-rmse:0.02817\n",
      "[1345]\tvalidation-rmse:0.02817\n",
      "[1346]\tvalidation-rmse:0.02817\n",
      "[1347]\tvalidation-rmse:0.02817\n",
      "[1348]\tvalidation-rmse:0.02817\n",
      "[1349]\tvalidation-rmse:0.02816\n",
      "[1350]\tvalidation-rmse:0.02816\n",
      "[1351]\tvalidation-rmse:0.02816\n",
      "[1352]\tvalidation-rmse:0.02816\n",
      "[1353]\tvalidation-rmse:0.02816\n",
      "[1354]\tvalidation-rmse:0.02816\n",
      "[1355]\tvalidation-rmse:0.02816\n",
      "[1356]\tvalidation-rmse:0.02816\n",
      "[1357]\tvalidation-rmse:0.02816\n",
      "[1358]\tvalidation-rmse:0.02816\n",
      "[1359]\tvalidation-rmse:0.02816\n",
      "[1360]\tvalidation-rmse:0.02816\n",
      "[1361]\tvalidation-rmse:0.02815\n",
      "[1362]\tvalidation-rmse:0.02815\n",
      "[1363]\tvalidation-rmse:0.02815\n",
      "[1364]\tvalidation-rmse:0.02815\n",
      "[1365]\tvalidation-rmse:0.02815\n",
      "[1366]\tvalidation-rmse:0.02815\n",
      "[1367]\tvalidation-rmse:0.02815\n",
      "[1368]\tvalidation-rmse:0.02814\n",
      "[1369]\tvalidation-rmse:0.02815\n",
      "[1370]\tvalidation-rmse:0.02815\n",
      "[1371]\tvalidation-rmse:0.02814\n",
      "[1372]\tvalidation-rmse:0.02814\n",
      "[1373]\tvalidation-rmse:0.02813\n",
      "[1374]\tvalidation-rmse:0.02813\n",
      "[1375]\tvalidation-rmse:0.02813\n",
      "[1376]\tvalidation-rmse:0.02813\n",
      "[1377]\tvalidation-rmse:0.02813\n",
      "[1378]\tvalidation-rmse:0.02813\n",
      "[1379]\tvalidation-rmse:0.02813\n",
      "[1380]\tvalidation-rmse:0.02813\n",
      "[1381]\tvalidation-rmse:0.02813\n",
      "[1382]\tvalidation-rmse:0.02813\n",
      "[1383]\tvalidation-rmse:0.02813\n",
      "[1384]\tvalidation-rmse:0.02813\n",
      "[1385]\tvalidation-rmse:0.02813\n",
      "[1386]\tvalidation-rmse:0.02813\n",
      "[1387]\tvalidation-rmse:0.02812\n",
      "[1388]\tvalidation-rmse:0.02812\n",
      "[1389]\tvalidation-rmse:0.02812\n",
      "[1390]\tvalidation-rmse:0.02812\n",
      "[1391]\tvalidation-rmse:0.02812\n",
      "[1392]\tvalidation-rmse:0.02812\n",
      "[1393]\tvalidation-rmse:0.02812\n",
      "[1394]\tvalidation-rmse:0.02812\n",
      "[1395]\tvalidation-rmse:0.02812\n",
      "[1396]\tvalidation-rmse:0.02812\n",
      "[1397]\tvalidation-rmse:0.02812\n",
      "[1398]\tvalidation-rmse:0.02812\n",
      "[1399]\tvalidation-rmse:0.02812\n",
      "[1400]\tvalidation-rmse:0.02812\n",
      "[1401]\tvalidation-rmse:0.02812\n",
      "[1402]\tvalidation-rmse:0.02812\n",
      "[1403]\tvalidation-rmse:0.02812\n",
      "[1404]\tvalidation-rmse:0.02812\n",
      "[1405]\tvalidation-rmse:0.02812\n",
      "[1406]\tvalidation-rmse:0.02812\n",
      "[1407]\tvalidation-rmse:0.02812\n",
      "[1408]\tvalidation-rmse:0.02812\n",
      "[1409]\tvalidation-rmse:0.02812\n",
      "[1410]\tvalidation-rmse:0.02812\n",
      "[1411]\tvalidation-rmse:0.02811\n",
      "[1412]\tvalidation-rmse:0.02811\n",
      "[1413]\tvalidation-rmse:0.02811\n",
      "[1414]\tvalidation-rmse:0.02811\n",
      "[1415]\tvalidation-rmse:0.02811\n",
      "[1416]\tvalidation-rmse:0.02811\n",
      "[1417]\tvalidation-rmse:0.02810\n",
      "[1418]\tvalidation-rmse:0.02810\n",
      "[1419]\tvalidation-rmse:0.02810\n",
      "[1420]\tvalidation-rmse:0.02810\n",
      "[1421]\tvalidation-rmse:0.02810\n",
      "[1422]\tvalidation-rmse:0.02810\n",
      "[1423]\tvalidation-rmse:0.02810\n",
      "[1424]\tvalidation-rmse:0.02810\n",
      "[1425]\tvalidation-rmse:0.02810\n",
      "[1426]\tvalidation-rmse:0.02810\n",
      "[1427]\tvalidation-rmse:0.02809\n",
      "[1428]\tvalidation-rmse:0.02809\n",
      "[1429]\tvalidation-rmse:0.02809\n",
      "[1430]\tvalidation-rmse:0.02809\n",
      "[1431]\tvalidation-rmse:0.02809\n",
      "[1432]\tvalidation-rmse:0.02809\n",
      "[1433]\tvalidation-rmse:0.02809\n",
      "[1434]\tvalidation-rmse:0.02809\n",
      "[1435]\tvalidation-rmse:0.02808\n",
      "[1436]\tvalidation-rmse:0.02808\n",
      "[1437]\tvalidation-rmse:0.02808\n",
      "[1438]\tvalidation-rmse:0.02808\n",
      "[1439]\tvalidation-rmse:0.02808\n",
      "[1440]\tvalidation-rmse:0.02808\n",
      "[1441]\tvalidation-rmse:0.02808\n",
      "[1442]\tvalidation-rmse:0.02807\n",
      "[1443]\tvalidation-rmse:0.02807\n",
      "[1444]\tvalidation-rmse:0.02807\n",
      "[1445]\tvalidation-rmse:0.02807\n",
      "[1446]\tvalidation-rmse:0.02807\n",
      "[1447]\tvalidation-rmse:0.02807\n",
      "[1448]\tvalidation-rmse:0.02807\n",
      "[1449]\tvalidation-rmse:0.02807\n",
      "[1450]\tvalidation-rmse:0.02806\n",
      "[1451]\tvalidation-rmse:0.02806\n",
      "[1452]\tvalidation-rmse:0.02806\n",
      "[1453]\tvalidation-rmse:0.02806\n",
      "[1454]\tvalidation-rmse:0.02806\n",
      "[1455]\tvalidation-rmse:0.02806\n",
      "[1456]\tvalidation-rmse:0.02806\n",
      "[1457]\tvalidation-rmse:0.02806\n",
      "[1458]\tvalidation-rmse:0.02806\n",
      "[1459]\tvalidation-rmse:0.02805\n",
      "[1460]\tvalidation-rmse:0.02806\n",
      "[1461]\tvalidation-rmse:0.02805\n",
      "[1462]\tvalidation-rmse:0.02805\n",
      "[1463]\tvalidation-rmse:0.02805\n",
      "[1464]\tvalidation-rmse:0.02805\n",
      "[1465]\tvalidation-rmse:0.02805\n",
      "[1466]\tvalidation-rmse:0.02805\n",
      "[1467]\tvalidation-rmse:0.02805\n",
      "[1468]\tvalidation-rmse:0.02805\n",
      "[1469]\tvalidation-rmse:0.02805\n",
      "[1470]\tvalidation-rmse:0.02804\n",
      "[1471]\tvalidation-rmse:0.02804\n",
      "[1472]\tvalidation-rmse:0.02804\n",
      "[1473]\tvalidation-rmse:0.02804\n",
      "[1474]\tvalidation-rmse:0.02804\n",
      "[1475]\tvalidation-rmse:0.02804\n",
      "[1476]\tvalidation-rmse:0.02804\n",
      "[1477]\tvalidation-rmse:0.02804\n",
      "[1478]\tvalidation-rmse:0.02804\n",
      "[1479]\tvalidation-rmse:0.02804\n",
      "[1480]\tvalidation-rmse:0.02804\n",
      "[1481]\tvalidation-rmse:0.02804\n",
      "[1482]\tvalidation-rmse:0.02805\n",
      "[1483]\tvalidation-rmse:0.02804\n",
      "[1484]\tvalidation-rmse:0.02804\n",
      "[1485]\tvalidation-rmse:0.02804\n",
      "[1486]\tvalidation-rmse:0.02804\n",
      "[1487]\tvalidation-rmse:0.02804\n",
      "[1488]\tvalidation-rmse:0.02803\n",
      "[1489]\tvalidation-rmse:0.02803\n",
      "[1490]\tvalidation-rmse:0.02803\n",
      "[1491]\tvalidation-rmse:0.02803\n",
      "[1492]\tvalidation-rmse:0.02803\n",
      "[1493]\tvalidation-rmse:0.02803\n",
      "[1494]\tvalidation-rmse:0.02803\n",
      "[1495]\tvalidation-rmse:0.02803\n",
      "[1496]\tvalidation-rmse:0.02803\n",
      "[1497]\tvalidation-rmse:0.02802\n",
      "[1498]\tvalidation-rmse:0.02802\n",
      "[1499]\tvalidation-rmse:0.02802\n",
      "[1500]\tvalidation-rmse:0.02802\n",
      "[1501]\tvalidation-rmse:0.02802\n",
      "[1502]\tvalidation-rmse:0.02802\n",
      "[1503]\tvalidation-rmse:0.02802\n",
      "[1504]\tvalidation-rmse:0.02802\n",
      "[1505]\tvalidation-rmse:0.02802\n",
      "[1506]\tvalidation-rmse:0.02801\n",
      "[1507]\tvalidation-rmse:0.02801\n",
      "[1508]\tvalidation-rmse:0.02801\n",
      "[1509]\tvalidation-rmse:0.02801\n",
      "[1510]\tvalidation-rmse:0.02801\n",
      "[1511]\tvalidation-rmse:0.02801\n",
      "[1512]\tvalidation-rmse:0.02801\n",
      "[1513]\tvalidation-rmse:0.02801\n",
      "[1514]\tvalidation-rmse:0.02801\n",
      "[1515]\tvalidation-rmse:0.02801\n",
      "[1516]\tvalidation-rmse:0.02801\n",
      "[1517]\tvalidation-rmse:0.02801\n",
      "[1518]\tvalidation-rmse:0.02801\n",
      "[1519]\tvalidation-rmse:0.02801\n",
      "[1520]\tvalidation-rmse:0.02800\n",
      "[1521]\tvalidation-rmse:0.02800\n",
      "[1522]\tvalidation-rmse:0.02800\n",
      "[1523]\tvalidation-rmse:0.02800\n",
      "[1524]\tvalidation-rmse:0.02800\n",
      "[1525]\tvalidation-rmse:0.02800\n",
      "[1526]\tvalidation-rmse:0.02800\n",
      "[1527]\tvalidation-rmse:0.02800\n",
      "[1528]\tvalidation-rmse:0.02800\n",
      "[1529]\tvalidation-rmse:0.02800\n",
      "[1530]\tvalidation-rmse:0.02800\n",
      "[1531]\tvalidation-rmse:0.02800\n",
      "[1532]\tvalidation-rmse:0.02800\n",
      "[1533]\tvalidation-rmse:0.02800\n",
      "[1534]\tvalidation-rmse:0.02800\n",
      "[1535]\tvalidation-rmse:0.02799\n",
      "[1536]\tvalidation-rmse:0.02799\n",
      "[1537]\tvalidation-rmse:0.02799\n",
      "[1538]\tvalidation-rmse:0.02799\n",
      "[1539]\tvalidation-rmse:0.02799\n",
      "[1540]\tvalidation-rmse:0.02799\n",
      "[1541]\tvalidation-rmse:0.02799\n",
      "[1542]\tvalidation-rmse:0.02799\n",
      "[1543]\tvalidation-rmse:0.02799\n",
      "[1544]\tvalidation-rmse:0.02799\n",
      "[1545]\tvalidation-rmse:0.02799\n",
      "[1546]\tvalidation-rmse:0.02798\n",
      "[1547]\tvalidation-rmse:0.02798\n",
      "[1548]\tvalidation-rmse:0.02798\n",
      "[1549]\tvalidation-rmse:0.02798\n",
      "[1550]\tvalidation-rmse:0.02798\n",
      "[1551]\tvalidation-rmse:0.02798\n",
      "[1552]\tvalidation-rmse:0.02798\n",
      "[1553]\tvalidation-rmse:0.02798\n",
      "[1554]\tvalidation-rmse:0.02798\n",
      "[1555]\tvalidation-rmse:0.02798\n",
      "[1556]\tvalidation-rmse:0.02798\n",
      "[1557]\tvalidation-rmse:0.02798\n",
      "[1558]\tvalidation-rmse:0.02798\n",
      "[1559]\tvalidation-rmse:0.02798\n",
      "[1560]\tvalidation-rmse:0.02797\n",
      "[1561]\tvalidation-rmse:0.02797\n",
      "[1562]\tvalidation-rmse:0.02797\n",
      "[1563]\tvalidation-rmse:0.02797\n",
      "[1564]\tvalidation-rmse:0.02797\n",
      "[1565]\tvalidation-rmse:0.02797\n",
      "[1566]\tvalidation-rmse:0.02797\n",
      "[1567]\tvalidation-rmse:0.02797\n",
      "[1568]\tvalidation-rmse:0.02797\n",
      "[1569]\tvalidation-rmse:0.02796\n",
      "[1570]\tvalidation-rmse:0.02796\n",
      "[1571]\tvalidation-rmse:0.02796\n",
      "[1572]\tvalidation-rmse:0.02796\n",
      "[1573]\tvalidation-rmse:0.02796\n",
      "[1574]\tvalidation-rmse:0.02796\n",
      "[1575]\tvalidation-rmse:0.02796\n",
      "[1576]\tvalidation-rmse:0.02796\n",
      "[1577]\tvalidation-rmse:0.02796\n",
      "[1578]\tvalidation-rmse:0.02796\n",
      "[1579]\tvalidation-rmse:0.02796\n",
      "[1580]\tvalidation-rmse:0.02796\n",
      "[1581]\tvalidation-rmse:0.02796\n",
      "[1582]\tvalidation-rmse:0.02796\n",
      "[1583]\tvalidation-rmse:0.02796\n",
      "[1584]\tvalidation-rmse:0.02796\n",
      "[1585]\tvalidation-rmse:0.02796\n",
      "[1586]\tvalidation-rmse:0.02796\n",
      "[1587]\tvalidation-rmse:0.02796\n",
      "[1588]\tvalidation-rmse:0.02796\n",
      "[1589]\tvalidation-rmse:0.02796\n",
      "[1590]\tvalidation-rmse:0.02796\n",
      "[1591]\tvalidation-rmse:0.02796\n",
      "[1592]\tvalidation-rmse:0.02796\n",
      "[1593]\tvalidation-rmse:0.02796\n",
      "[1594]\tvalidation-rmse:0.02795\n",
      "[1595]\tvalidation-rmse:0.02795\n",
      "[1596]\tvalidation-rmse:0.02795\n",
      "[1597]\tvalidation-rmse:0.02795\n",
      "[1598]\tvalidation-rmse:0.02795\n",
      "[1599]\tvalidation-rmse:0.02795\n",
      "[1600]\tvalidation-rmse:0.02795\n",
      "[1601]\tvalidation-rmse:0.02795\n",
      "[1602]\tvalidation-rmse:0.02795\n",
      "[1603]\tvalidation-rmse:0.02795\n",
      "[1604]\tvalidation-rmse:0.02795\n",
      "[1605]\tvalidation-rmse:0.02795\n",
      "[1606]\tvalidation-rmse:0.02795\n",
      "[1607]\tvalidation-rmse:0.02794\n",
      "[1608]\tvalidation-rmse:0.02794\n",
      "[1609]\tvalidation-rmse:0.02794\n",
      "[1610]\tvalidation-rmse:0.02794\n",
      "[1611]\tvalidation-rmse:0.02794\n",
      "[1612]\tvalidation-rmse:0.02794\n",
      "[1613]\tvalidation-rmse:0.02794\n",
      "[1614]\tvalidation-rmse:0.02794\n",
      "[1615]\tvalidation-rmse:0.02794\n",
      "[1616]\tvalidation-rmse:0.02794\n",
      "[1617]\tvalidation-rmse:0.02794\n",
      "[1618]\tvalidation-rmse:0.02794\n",
      "[1619]\tvalidation-rmse:0.02794\n",
      "[1620]\tvalidation-rmse:0.02794\n",
      "[1621]\tvalidation-rmse:0.02794\n",
      "[1622]\tvalidation-rmse:0.02794\n",
      "[1623]\tvalidation-rmse:0.02793\n",
      "[1624]\tvalidation-rmse:0.02793\n",
      "[1625]\tvalidation-rmse:0.02793\n",
      "[1626]\tvalidation-rmse:0.02793\n",
      "[1627]\tvalidation-rmse:0.02793\n",
      "[1628]\tvalidation-rmse:0.02793\n",
      "[1629]\tvalidation-rmse:0.02792\n",
      "[1630]\tvalidation-rmse:0.02792\n",
      "[1631]\tvalidation-rmse:0.02792\n",
      "[1632]\tvalidation-rmse:0.02792\n",
      "[1633]\tvalidation-rmse:0.02792\n",
      "[1634]\tvalidation-rmse:0.02792\n",
      "[1635]\tvalidation-rmse:0.02792\n",
      "[1636]\tvalidation-rmse:0.02791\n",
      "[1637]\tvalidation-rmse:0.02791\n",
      "[1638]\tvalidation-rmse:0.02791\n",
      "[1639]\tvalidation-rmse:0.02791\n",
      "[1640]\tvalidation-rmse:0.02791\n",
      "[1641]\tvalidation-rmse:0.02791\n",
      "[1642]\tvalidation-rmse:0.02791\n",
      "[1643]\tvalidation-rmse:0.02791\n",
      "[1644]\tvalidation-rmse:0.02791\n",
      "[1645]\tvalidation-rmse:0.02791\n",
      "[1646]\tvalidation-rmse:0.02791\n",
      "[1647]\tvalidation-rmse:0.02791\n",
      "[1648]\tvalidation-rmse:0.02791\n",
      "[1649]\tvalidation-rmse:0.02790\n",
      "[1650]\tvalidation-rmse:0.02790\n",
      "[1651]\tvalidation-rmse:0.02791\n",
      "[1652]\tvalidation-rmse:0.02790\n",
      "[1653]\tvalidation-rmse:0.02790\n",
      "[1654]\tvalidation-rmse:0.02790\n",
      "[1655]\tvalidation-rmse:0.02790\n",
      "[1656]\tvalidation-rmse:0.02790\n",
      "[1657]\tvalidation-rmse:0.02790\n",
      "[1658]\tvalidation-rmse:0.02790\n",
      "[1659]\tvalidation-rmse:0.02790\n",
      "[1660]\tvalidation-rmse:0.02790\n",
      "[1661]\tvalidation-rmse:0.02790\n",
      "[1662]\tvalidation-rmse:0.02790\n",
      "[1663]\tvalidation-rmse:0.02790\n",
      "[1664]\tvalidation-rmse:0.02790\n",
      "[1665]\tvalidation-rmse:0.02790\n",
      "[1666]\tvalidation-rmse:0.02790\n",
      "[1667]\tvalidation-rmse:0.02789\n",
      "[1668]\tvalidation-rmse:0.02789\n",
      "[1669]\tvalidation-rmse:0.02789\n",
      "[1670]\tvalidation-rmse:0.02790\n",
      "[1671]\tvalidation-rmse:0.02790\n",
      "[1672]\tvalidation-rmse:0.02789\n",
      "[1673]\tvalidation-rmse:0.02789\n",
      "[1674]\tvalidation-rmse:0.02789\n",
      "[1675]\tvalidation-rmse:0.02789\n",
      "[1676]\tvalidation-rmse:0.02789\n",
      "[1677]\tvalidation-rmse:0.02789\n",
      "[1678]\tvalidation-rmse:0.02788\n",
      "[1679]\tvalidation-rmse:0.02788\n",
      "[1680]\tvalidation-rmse:0.02788\n",
      "[1681]\tvalidation-rmse:0.02789\n",
      "[1682]\tvalidation-rmse:0.02788\n",
      "[1683]\tvalidation-rmse:0.02788\n",
      "[1684]\tvalidation-rmse:0.02789\n",
      "[1685]\tvalidation-rmse:0.02788\n",
      "[1686]\tvalidation-rmse:0.02788\n",
      "[1687]\tvalidation-rmse:0.02788\n",
      "[1688]\tvalidation-rmse:0.02788\n",
      "[1689]\tvalidation-rmse:0.02788\n",
      "[1690]\tvalidation-rmse:0.02788\n",
      "[1691]\tvalidation-rmse:0.02788\n",
      "[1692]\tvalidation-rmse:0.02788\n",
      "[1693]\tvalidation-rmse:0.02787\n",
      "[1694]\tvalidation-rmse:0.02787\n",
      "[1695]\tvalidation-rmse:0.02787\n",
      "[1696]\tvalidation-rmse:0.02787\n",
      "[1697]\tvalidation-rmse:0.02787\n",
      "[1698]\tvalidation-rmse:0.02787\n",
      "[1699]\tvalidation-rmse:0.02787\n",
      "[1700]\tvalidation-rmse:0.02787\n",
      "[1701]\tvalidation-rmse:0.02787\n",
      "[1702]\tvalidation-rmse:0.02787\n",
      "[1703]\tvalidation-rmse:0.02787\n",
      "[1704]\tvalidation-rmse:0.02787\n",
      "[1705]\tvalidation-rmse:0.02786\n",
      "[1706]\tvalidation-rmse:0.02786\n",
      "[1707]\tvalidation-rmse:0.02786\n",
      "[1708]\tvalidation-rmse:0.02786\n",
      "[1709]\tvalidation-rmse:0.02786\n",
      "[1710]\tvalidation-rmse:0.02786\n",
      "[1711]\tvalidation-rmse:0.02786\n",
      "[1712]\tvalidation-rmse:0.02786\n",
      "[1713]\tvalidation-rmse:0.02786\n",
      "[1714]\tvalidation-rmse:0.02786\n",
      "[1715]\tvalidation-rmse:0.02786\n",
      "[1716]\tvalidation-rmse:0.02786\n",
      "[1717]\tvalidation-rmse:0.02786\n",
      "[1718]\tvalidation-rmse:0.02786\n",
      "[1719]\tvalidation-rmse:0.02786\n",
      "[1720]\tvalidation-rmse:0.02786\n",
      "[1721]\tvalidation-rmse:0.02786\n",
      "[1722]\tvalidation-rmse:0.02786\n",
      "[1723]\tvalidation-rmse:0.02786\n",
      "[1724]\tvalidation-rmse:0.02786\n",
      "[1725]\tvalidation-rmse:0.02786\n",
      "[1726]\tvalidation-rmse:0.02786\n",
      "[1727]\tvalidation-rmse:0.02786\n",
      "[1728]\tvalidation-rmse:0.02786\n",
      "[1729]\tvalidation-rmse:0.02786\n",
      "[1730]\tvalidation-rmse:0.02786\n",
      "[1731]\tvalidation-rmse:0.02786\n",
      "[1732]\tvalidation-rmse:0.02786\n",
      "[1733]\tvalidation-rmse:0.02785\n",
      "[1734]\tvalidation-rmse:0.02785\n",
      "[1735]\tvalidation-rmse:0.02785\n",
      "[1736]\tvalidation-rmse:0.02785\n",
      "[1737]\tvalidation-rmse:0.02785\n",
      "[1738]\tvalidation-rmse:0.02785\n",
      "[1739]\tvalidation-rmse:0.02785\n",
      "[1740]\tvalidation-rmse:0.02785\n",
      "[1741]\tvalidation-rmse:0.02785\n",
      "[1742]\tvalidation-rmse:0.02785\n",
      "[1743]\tvalidation-rmse:0.02785\n",
      "[1744]\tvalidation-rmse:0.02785\n",
      "[1745]\tvalidation-rmse:0.02785\n",
      "[1746]\tvalidation-rmse:0.02785\n",
      "[1747]\tvalidation-rmse:0.02785\n",
      "[1748]\tvalidation-rmse:0.02785\n",
      "[1749]\tvalidation-rmse:0.02785\n",
      "[1750]\tvalidation-rmse:0.02785\n",
      "[1751]\tvalidation-rmse:0.02785\n",
      "[1752]\tvalidation-rmse:0.02785\n",
      "[1753]\tvalidation-rmse:0.02784\n",
      "[1754]\tvalidation-rmse:0.02785\n",
      "[1755]\tvalidation-rmse:0.02784\n",
      "[1756]\tvalidation-rmse:0.02784\n",
      "[1757]\tvalidation-rmse:0.02784\n",
      "[1758]\tvalidation-rmse:0.02784\n",
      "[1759]\tvalidation-rmse:0.02784\n",
      "[1760]\tvalidation-rmse:0.02784\n",
      "[1761]\tvalidation-rmse:0.02784\n",
      "[1762]\tvalidation-rmse:0.02784\n",
      "[1763]\tvalidation-rmse:0.02784\n",
      "[1764]\tvalidation-rmse:0.02784\n",
      "[1765]\tvalidation-rmse:0.02784\n",
      "[1766]\tvalidation-rmse:0.02784\n",
      "[1767]\tvalidation-rmse:0.02784\n",
      "[1768]\tvalidation-rmse:0.02784\n",
      "[1769]\tvalidation-rmse:0.02784\n",
      "[1770]\tvalidation-rmse:0.02784\n",
      "[1771]\tvalidation-rmse:0.02784\n",
      "[1772]\tvalidation-rmse:0.02784\n",
      "[1773]\tvalidation-rmse:0.02784\n",
      "[1774]\tvalidation-rmse:0.02784\n",
      "[1775]\tvalidation-rmse:0.02784\n",
      "[1776]\tvalidation-rmse:0.02784\n",
      "[1777]\tvalidation-rmse:0.02784\n",
      "[1778]\tvalidation-rmse:0.02784\n",
      "[1779]\tvalidation-rmse:0.02784\n",
      "[1780]\tvalidation-rmse:0.02784\n",
      "[1781]\tvalidation-rmse:0.02784\n",
      "[1782]\tvalidation-rmse:0.02784\n",
      "[1783]\tvalidation-rmse:0.02784\n",
      "[1784]\tvalidation-rmse:0.02784\n",
      "[1785]\tvalidation-rmse:0.02784\n",
      "[1786]\tvalidation-rmse:0.02784\n",
      "[1787]\tvalidation-rmse:0.02784\n",
      "[1788]\tvalidation-rmse:0.02784\n",
      "[1789]\tvalidation-rmse:0.02784\n",
      "[1790]\tvalidation-rmse:0.02784\n",
      "[1791]\tvalidation-rmse:0.02783\n",
      "[1792]\tvalidation-rmse:0.02783\n",
      "[1793]\tvalidation-rmse:0.02783\n",
      "[1794]\tvalidation-rmse:0.02783\n",
      "[1795]\tvalidation-rmse:0.02783\n",
      "[1796]\tvalidation-rmse:0.02783\n",
      "[1797]\tvalidation-rmse:0.02783\n",
      "[1798]\tvalidation-rmse:0.02783\n",
      "[1799]\tvalidation-rmse:0.02783\n",
      "[1800]\tvalidation-rmse:0.02783\n",
      "[1801]\tvalidation-rmse:0.02782\n",
      "[1802]\tvalidation-rmse:0.02782\n",
      "[1803]\tvalidation-rmse:0.02782\n",
      "[1804]\tvalidation-rmse:0.02782\n",
      "[1805]\tvalidation-rmse:0.02782\n",
      "[1806]\tvalidation-rmse:0.02782\n",
      "[1807]\tvalidation-rmse:0.02782\n",
      "[1808]\tvalidation-rmse:0.02782\n",
      "[1809]\tvalidation-rmse:0.02782\n",
      "[1810]\tvalidation-rmse:0.02782\n",
      "[1811]\tvalidation-rmse:0.02782\n",
      "[1812]\tvalidation-rmse:0.02782\n",
      "[1813]\tvalidation-rmse:0.02782\n",
      "[1814]\tvalidation-rmse:0.02781\n",
      "[1815]\tvalidation-rmse:0.02781\n",
      "[1816]\tvalidation-rmse:0.02781\n",
      "[1817]\tvalidation-rmse:0.02781\n",
      "[1818]\tvalidation-rmse:0.02781\n",
      "[1819]\tvalidation-rmse:0.02781\n",
      "[1820]\tvalidation-rmse:0.02781\n",
      "[1821]\tvalidation-rmse:0.02781\n",
      "[1822]\tvalidation-rmse:0.02781\n",
      "[1823]\tvalidation-rmse:0.02781\n",
      "[1824]\tvalidation-rmse:0.02781\n",
      "[1825]\tvalidation-rmse:0.02781\n",
      "[1826]\tvalidation-rmse:0.02781\n",
      "[1827]\tvalidation-rmse:0.02781\n",
      "[1828]\tvalidation-rmse:0.02781\n",
      "[1829]\tvalidation-rmse:0.02781\n",
      "[1830]\tvalidation-rmse:0.02781\n",
      "[1831]\tvalidation-rmse:0.02781\n",
      "[1832]\tvalidation-rmse:0.02781\n",
      "[1833]\tvalidation-rmse:0.02781\n",
      "[1834]\tvalidation-rmse:0.02781\n",
      "[1835]\tvalidation-rmse:0.02781\n",
      "[1836]\tvalidation-rmse:0.02781\n",
      "[1837]\tvalidation-rmse:0.02781\n",
      "[1838]\tvalidation-rmse:0.02781\n",
      "[1839]\tvalidation-rmse:0.02781\n",
      "[1840]\tvalidation-rmse:0.02781\n",
      "[1841]\tvalidation-rmse:0.02781\n",
      "[1842]\tvalidation-rmse:0.02781\n",
      "[1843]\tvalidation-rmse:0.02781\n",
      "[1844]\tvalidation-rmse:0.02780\n",
      "[1845]\tvalidation-rmse:0.02780\n",
      "[1846]\tvalidation-rmse:0.02780\n",
      "[1847]\tvalidation-rmse:0.02780\n",
      "[1848]\tvalidation-rmse:0.02780\n",
      "[1849]\tvalidation-rmse:0.02780\n",
      "[1850]\tvalidation-rmse:0.02780\n",
      "[1851]\tvalidation-rmse:0.02780\n",
      "[1852]\tvalidation-rmse:0.02780\n",
      "[1853]\tvalidation-rmse:0.02780\n",
      "[1854]\tvalidation-rmse:0.02780\n",
      "[1855]\tvalidation-rmse:0.02780\n",
      "[1856]\tvalidation-rmse:0.02780\n",
      "[1857]\tvalidation-rmse:0.02780\n",
      "[1858]\tvalidation-rmse:0.02780\n",
      "[1859]\tvalidation-rmse:0.02780\n",
      "[1860]\tvalidation-rmse:0.02780\n",
      "[1861]\tvalidation-rmse:0.02779\n",
      "[1862]\tvalidation-rmse:0.02779\n",
      "[1863]\tvalidation-rmse:0.02779\n",
      "[1864]\tvalidation-rmse:0.02778\n",
      "[1865]\tvalidation-rmse:0.02778\n",
      "[1866]\tvalidation-rmse:0.02778\n",
      "[1867]\tvalidation-rmse:0.02778\n",
      "[1868]\tvalidation-rmse:0.02778\n",
      "[1869]\tvalidation-rmse:0.02778\n",
      "[1870]\tvalidation-rmse:0.02778\n",
      "[1871]\tvalidation-rmse:0.02778\n",
      "[1872]\tvalidation-rmse:0.02778\n",
      "[1873]\tvalidation-rmse:0.02778\n",
      "[1874]\tvalidation-rmse:0.02778\n",
      "[1875]\tvalidation-rmse:0.02778\n",
      "[1876]\tvalidation-rmse:0.02778\n",
      "[1877]\tvalidation-rmse:0.02778\n",
      "[1878]\tvalidation-rmse:0.02778\n",
      "[1879]\tvalidation-rmse:0.02778\n",
      "[1880]\tvalidation-rmse:0.02778\n",
      "[1881]\tvalidation-rmse:0.02778\n",
      "[1882]\tvalidation-rmse:0.02778\n",
      "[1883]\tvalidation-rmse:0.02778\n",
      "[1884]\tvalidation-rmse:0.02778\n",
      "[1885]\tvalidation-rmse:0.02778\n",
      "[1886]\tvalidation-rmse:0.02777\n",
      "[1887]\tvalidation-rmse:0.02777\n",
      "[1888]\tvalidation-rmse:0.02777\n",
      "[1889]\tvalidation-rmse:0.02777\n",
      "[1890]\tvalidation-rmse:0.02777\n",
      "[1891]\tvalidation-rmse:0.02777\n",
      "[1892]\tvalidation-rmse:0.02777\n",
      "[1893]\tvalidation-rmse:0.02777\n",
      "[1894]\tvalidation-rmse:0.02777\n",
      "[1895]\tvalidation-rmse:0.02777\n",
      "[1896]\tvalidation-rmse:0.02777\n",
      "[1897]\tvalidation-rmse:0.02776\n",
      "[1898]\tvalidation-rmse:0.02776\n",
      "[1899]\tvalidation-rmse:0.02776\n",
      "[1900]\tvalidation-rmse:0.02776\n",
      "[1901]\tvalidation-rmse:0.02776\n",
      "[1902]\tvalidation-rmse:0.02776\n",
      "[1903]\tvalidation-rmse:0.02776\n",
      "[1904]\tvalidation-rmse:0.02776\n",
      "[1905]\tvalidation-rmse:0.02776\n",
      "[1906]\tvalidation-rmse:0.02776\n",
      "[1907]\tvalidation-rmse:0.02776\n",
      "[1908]\tvalidation-rmse:0.02777\n",
      "[1909]\tvalidation-rmse:0.02777\n",
      "[1910]\tvalidation-rmse:0.02777\n",
      "[1911]\tvalidation-rmse:0.02777\n",
      "[1912]\tvalidation-rmse:0.02777\n",
      "[1913]\tvalidation-rmse:0.02777\n",
      "[1914]\tvalidation-rmse:0.02777\n",
      "[1915]\tvalidation-rmse:0.02777\n",
      "[1916]\tvalidation-rmse:0.02776\n",
      "[1917]\tvalidation-rmse:0.02776\n",
      "[1918]\tvalidation-rmse:0.02776\n",
      "[1919]\tvalidation-rmse:0.02776\n",
      "[1920]\tvalidation-rmse:0.02776\n",
      "[1921]\tvalidation-rmse:0.02776\n",
      "[1922]\tvalidation-rmse:0.02776\n",
      "[1923]\tvalidation-rmse:0.02776\n",
      "[1924]\tvalidation-rmse:0.02776\n",
      "[1925]\tvalidation-rmse:0.02776\n",
      "[1926]\tvalidation-rmse:0.02776\n",
      "[1927]\tvalidation-rmse:0.02776\n",
      "[1928]\tvalidation-rmse:0.02776\n",
      "[1929]\tvalidation-rmse:0.02776\n",
      "[1930]\tvalidation-rmse:0.02776\n",
      "[1931]\tvalidation-rmse:0.02776\n",
      "[1932]\tvalidation-rmse:0.02776\n",
      "[1933]\tvalidation-rmse:0.02776\n",
      "[1934]\tvalidation-rmse:0.02775\n",
      "[1935]\tvalidation-rmse:0.02776\n",
      "[1936]\tvalidation-rmse:0.02775\n",
      "[1937]\tvalidation-rmse:0.02775\n",
      "[1938]\tvalidation-rmse:0.02775\n",
      "[1939]\tvalidation-rmse:0.02775\n",
      "[1940]\tvalidation-rmse:0.02775\n",
      "[1941]\tvalidation-rmse:0.02775\n",
      "[1942]\tvalidation-rmse:0.02775\n",
      "[1943]\tvalidation-rmse:0.02775\n",
      "[1944]\tvalidation-rmse:0.02775\n",
      "[1945]\tvalidation-rmse:0.02775\n",
      "[1946]\tvalidation-rmse:0.02775\n",
      "[1947]\tvalidation-rmse:0.02775\n",
      "[1948]\tvalidation-rmse:0.02775\n",
      "[1949]\tvalidation-rmse:0.02775\n",
      "[1950]\tvalidation-rmse:0.02775\n",
      "[1951]\tvalidation-rmse:0.02775\n",
      "[1952]\tvalidation-rmse:0.02775\n",
      "[1953]\tvalidation-rmse:0.02775\n",
      "[1954]\tvalidation-rmse:0.02775\n",
      "[1955]\tvalidation-rmse:0.02775\n",
      "[1956]\tvalidation-rmse:0.02774\n",
      "[1957]\tvalidation-rmse:0.02774\n",
      "[1958]\tvalidation-rmse:0.02774\n",
      "[1959]\tvalidation-rmse:0.02774\n",
      "[1960]\tvalidation-rmse:0.02774\n",
      "[1961]\tvalidation-rmse:0.02774\n",
      "[1962]\tvalidation-rmse:0.02774\n",
      "[1963]\tvalidation-rmse:0.02774\n",
      "[1964]\tvalidation-rmse:0.02774\n",
      "[1965]\tvalidation-rmse:0.02774\n",
      "[1966]\tvalidation-rmse:0.02774\n",
      "[1967]\tvalidation-rmse:0.02774\n",
      "[1968]\tvalidation-rmse:0.02774\n",
      "[1969]\tvalidation-rmse:0.02774\n",
      "[1970]\tvalidation-rmse:0.02774\n",
      "[1971]\tvalidation-rmse:0.02774\n",
      "[1972]\tvalidation-rmse:0.02774\n",
      "[1973]\tvalidation-rmse:0.02774\n",
      "[1974]\tvalidation-rmse:0.02774\n",
      "[1975]\tvalidation-rmse:0.02774\n",
      "[1976]\tvalidation-rmse:0.02774\n",
      "[1977]\tvalidation-rmse:0.02774\n",
      "[1978]\tvalidation-rmse:0.02774\n",
      "[1979]\tvalidation-rmse:0.02774\n",
      "[1980]\tvalidation-rmse:0.02774\n",
      "[1981]\tvalidation-rmse:0.02774\n",
      "[1982]\tvalidation-rmse:0.02774\n",
      "[1983]\tvalidation-rmse:0.02774\n",
      "[1984]\tvalidation-rmse:0.02773\n",
      "[1985]\tvalidation-rmse:0.02773\n",
      "[1986]\tvalidation-rmse:0.02773\n",
      "[1987]\tvalidation-rmse:0.02773\n",
      "[1988]\tvalidation-rmse:0.02773\n",
      "[1989]\tvalidation-rmse:0.02773\n",
      "[1990]\tvalidation-rmse:0.02773\n",
      "[1991]\tvalidation-rmse:0.02773\n",
      "[1992]\tvalidation-rmse:0.02773\n",
      "[1993]\tvalidation-rmse:0.02773\n",
      "[1994]\tvalidation-rmse:0.02773\n",
      "[1995]\tvalidation-rmse:0.02773\n",
      "[1996]\tvalidation-rmse:0.02773\n",
      "[1997]\tvalidation-rmse:0.02773\n",
      "[1998]\tvalidation-rmse:0.02773\n",
      "[1999]\tvalidation-rmse:0.02773\n",
      "[2000]\tvalidation-rmse:0.02773\n",
      "[2001]\tvalidation-rmse:0.02773\n",
      "[2002]\tvalidation-rmse:0.02773\n",
      "[2003]\tvalidation-rmse:0.02773\n",
      "[2004]\tvalidation-rmse:0.02773\n",
      "[2005]\tvalidation-rmse:0.02773\n",
      "[2006]\tvalidation-rmse:0.02773\n",
      "[2007]\tvalidation-rmse:0.02773\n",
      "[2008]\tvalidation-rmse:0.02773\n",
      "[2009]\tvalidation-rmse:0.02773\n",
      "[2010]\tvalidation-rmse:0.02773\n",
      "[2011]\tvalidation-rmse:0.02772\n",
      "[2012]\tvalidation-rmse:0.02772\n",
      "[2013]\tvalidation-rmse:0.02772\n",
      "[2014]\tvalidation-rmse:0.02772\n",
      "[2015]\tvalidation-rmse:0.02772\n",
      "[2016]\tvalidation-rmse:0.02772\n",
      "[2017]\tvalidation-rmse:0.02772\n",
      "[2018]\tvalidation-rmse:0.02772\n",
      "[2019]\tvalidation-rmse:0.02772\n",
      "[2020]\tvalidation-rmse:0.02772\n",
      "[2021]\tvalidation-rmse:0.02772\n",
      "[2022]\tvalidation-rmse:0.02772\n",
      "[2023]\tvalidation-rmse:0.02772\n",
      "[2024]\tvalidation-rmse:0.02772\n",
      "[2025]\tvalidation-rmse:0.02772\n",
      "[2026]\tvalidation-rmse:0.02772\n",
      "[2027]\tvalidation-rmse:0.02772\n",
      "[2028]\tvalidation-rmse:0.02772\n",
      "[2029]\tvalidation-rmse:0.02772\n",
      "[2030]\tvalidation-rmse:0.02772\n",
      "[2031]\tvalidation-rmse:0.02772\n",
      "[2032]\tvalidation-rmse:0.02772\n",
      "[2033]\tvalidation-rmse:0.02772\n",
      "[2034]\tvalidation-rmse:0.02772\n",
      "[2035]\tvalidation-rmse:0.02771\n",
      "[2036]\tvalidation-rmse:0.02771\n",
      "[2037]\tvalidation-rmse:0.02771\n",
      "[2038]\tvalidation-rmse:0.02771\n",
      "[2039]\tvalidation-rmse:0.02771\n",
      "[2040]\tvalidation-rmse:0.02771\n",
      "[2041]\tvalidation-rmse:0.02771\n",
      "[2042]\tvalidation-rmse:0.02771\n",
      "[2043]\tvalidation-rmse:0.02771\n",
      "[2044]\tvalidation-rmse:0.02771\n",
      "[2045]\tvalidation-rmse:0.02771\n",
      "[2046]\tvalidation-rmse:0.02771\n",
      "[2047]\tvalidation-rmse:0.02771\n",
      "[2048]\tvalidation-rmse:0.02771\n",
      "[2049]\tvalidation-rmse:0.02771\n",
      "[2050]\tvalidation-rmse:0.02771\n",
      "[2051]\tvalidation-rmse:0.02771\n",
      "[2052]\tvalidation-rmse:0.02771\n",
      "[2053]\tvalidation-rmse:0.02771\n",
      "[2054]\tvalidation-rmse:0.02771\n",
      "[2055]\tvalidation-rmse:0.02771\n",
      "[2056]\tvalidation-rmse:0.02771\n",
      "[2057]\tvalidation-rmse:0.02771\n",
      "[2058]\tvalidation-rmse:0.02771\n",
      "[2059]\tvalidation-rmse:0.02771\n",
      "[2060]\tvalidation-rmse:0.02771\n",
      "[2061]\tvalidation-rmse:0.02771\n",
      "[2062]\tvalidation-rmse:0.02771\n",
      "[2063]\tvalidation-rmse:0.02771\n",
      "[2064]\tvalidation-rmse:0.02770\n",
      "[2065]\tvalidation-rmse:0.02770\n",
      "[2066]\tvalidation-rmse:0.02770\n",
      "[2067]\tvalidation-rmse:0.02770\n",
      "[2068]\tvalidation-rmse:0.02770\n",
      "[2069]\tvalidation-rmse:0.02770\n",
      "[2070]\tvalidation-rmse:0.02770\n",
      "[2071]\tvalidation-rmse:0.02770\n",
      "[2072]\tvalidation-rmse:0.02770\n",
      "[2073]\tvalidation-rmse:0.02770\n",
      "[2074]\tvalidation-rmse:0.02770\n",
      "[2075]\tvalidation-rmse:0.02770\n",
      "[2076]\tvalidation-rmse:0.02770\n",
      "[2077]\tvalidation-rmse:0.02770\n",
      "[2078]\tvalidation-rmse:0.02770\n",
      "[2079]\tvalidation-rmse:0.02770\n",
      "[2080]\tvalidation-rmse:0.02769\n",
      "[2081]\tvalidation-rmse:0.02769\n",
      "[2082]\tvalidation-rmse:0.02769\n",
      "[2083]\tvalidation-rmse:0.02769\n",
      "[2084]\tvalidation-rmse:0.02769\n",
      "[2085]\tvalidation-rmse:0.02769\n",
      "[2086]\tvalidation-rmse:0.02769\n",
      "[2087]\tvalidation-rmse:0.02769\n",
      "[2088]\tvalidation-rmse:0.02769\n",
      "[2089]\tvalidation-rmse:0.02769\n",
      "[2090]\tvalidation-rmse:0.02769\n",
      "[2091]\tvalidation-rmse:0.02769\n",
      "[2092]\tvalidation-rmse:0.02769\n",
      "[2093]\tvalidation-rmse:0.02769\n",
      "[2094]\tvalidation-rmse:0.02769\n",
      "[2095]\tvalidation-rmse:0.02769\n",
      "[2096]\tvalidation-rmse:0.02769\n",
      "[2097]\tvalidation-rmse:0.02769\n",
      "[2098]\tvalidation-rmse:0.02769\n",
      "[2099]\tvalidation-rmse:0.02769\n",
      "[2100]\tvalidation-rmse:0.02769\n",
      "[2101]\tvalidation-rmse:0.02769\n",
      "[2102]\tvalidation-rmse:0.02769\n",
      "[2103]\tvalidation-rmse:0.02769\n",
      "[2104]\tvalidation-rmse:0.02769\n",
      "[2105]\tvalidation-rmse:0.02769\n",
      "[2106]\tvalidation-rmse:0.02769\n",
      "[2107]\tvalidation-rmse:0.02769\n",
      "[2108]\tvalidation-rmse:0.02768\n",
      "[2109]\tvalidation-rmse:0.02768\n",
      "[2110]\tvalidation-rmse:0.02768\n",
      "[2111]\tvalidation-rmse:0.02768\n",
      "[2112]\tvalidation-rmse:0.02768\n",
      "[2113]\tvalidation-rmse:0.02768\n",
      "[2114]\tvalidation-rmse:0.02769\n",
      "[2115]\tvalidation-rmse:0.02769\n",
      "[2116]\tvalidation-rmse:0.02768\n",
      "[2117]\tvalidation-rmse:0.02768\n",
      "[2118]\tvalidation-rmse:0.02768\n",
      "[2119]\tvalidation-rmse:0.02768\n",
      "[2120]\tvalidation-rmse:0.02768\n",
      "[2121]\tvalidation-rmse:0.02768\n",
      "[2122]\tvalidation-rmse:0.02768\n",
      "[2123]\tvalidation-rmse:0.02768\n",
      "[2124]\tvalidation-rmse:0.02768\n",
      "[2125]\tvalidation-rmse:0.02768\n",
      "[2126]\tvalidation-rmse:0.02768\n",
      "[2127]\tvalidation-rmse:0.02768\n",
      "[2128]\tvalidation-rmse:0.02768\n",
      "[2129]\tvalidation-rmse:0.02768\n",
      "[2130]\tvalidation-rmse:0.02768\n",
      "[2131]\tvalidation-rmse:0.02768\n",
      "[2132]\tvalidation-rmse:0.02768\n",
      "[2133]\tvalidation-rmse:0.02768\n",
      "[2134]\tvalidation-rmse:0.02768\n",
      "[2135]\tvalidation-rmse:0.02768\n",
      "[2136]\tvalidation-rmse:0.02768\n",
      "[2137]\tvalidation-rmse:0.02767\n",
      "[2138]\tvalidation-rmse:0.02767\n",
      "[2139]\tvalidation-rmse:0.02767\n",
      "[2140]\tvalidation-rmse:0.02767\n",
      "[2141]\tvalidation-rmse:0.02767\n",
      "[2142]\tvalidation-rmse:0.02767\n",
      "[2143]\tvalidation-rmse:0.02767\n",
      "[2144]\tvalidation-rmse:0.02767\n",
      "[2145]\tvalidation-rmse:0.02767\n",
      "[2146]\tvalidation-rmse:0.02767\n",
      "[2147]\tvalidation-rmse:0.02767\n",
      "[2148]\tvalidation-rmse:0.02767\n",
      "[2149]\tvalidation-rmse:0.02767\n",
      "[2150]\tvalidation-rmse:0.02767\n",
      "[2151]\tvalidation-rmse:0.02767\n",
      "[2152]\tvalidation-rmse:0.02767\n",
      "[2153]\tvalidation-rmse:0.02767\n",
      "[2154]\tvalidation-rmse:0.02767\n",
      "[2155]\tvalidation-rmse:0.02767\n",
      "[2156]\tvalidation-rmse:0.02767\n",
      "[2157]\tvalidation-rmse:0.02767\n",
      "[2158]\tvalidation-rmse:0.02767\n",
      "[2159]\tvalidation-rmse:0.02767\n",
      "[2160]\tvalidation-rmse:0.02767\n",
      "[2161]\tvalidation-rmse:0.02767\n",
      "[2162]\tvalidation-rmse:0.02767\n",
      "[2163]\tvalidation-rmse:0.02767\n",
      "[2164]\tvalidation-rmse:0.02767\n",
      "[2165]\tvalidation-rmse:0.02767\n",
      "[2166]\tvalidation-rmse:0.02767\n",
      "[2167]\tvalidation-rmse:0.02767\n",
      "[2168]\tvalidation-rmse:0.02767\n",
      "[2169]\tvalidation-rmse:0.02767\n",
      "[2170]\tvalidation-rmse:0.02767\n",
      "[2171]\tvalidation-rmse:0.02767\n",
      "[2172]\tvalidation-rmse:0.02767\n",
      "[2173]\tvalidation-rmse:0.02767\n",
      "[2174]\tvalidation-rmse:0.02767\n",
      "[2175]\tvalidation-rmse:0.02767\n",
      "[2176]\tvalidation-rmse:0.02767\n",
      "[2177]\tvalidation-rmse:0.02767\n",
      "[2178]\tvalidation-rmse:0.02767\n",
      "[2179]\tvalidation-rmse:0.02767\n",
      "[2180]\tvalidation-rmse:0.02767\n",
      "[2181]\tvalidation-rmse:0.02767\n",
      "[2182]\tvalidation-rmse:0.02767\n",
      "[2183]\tvalidation-rmse:0.02767\n",
      "[2184]\tvalidation-rmse:0.02767\n",
      "[2185]\tvalidation-rmse:0.02767\n",
      "[2186]\tvalidation-rmse:0.02767\n",
      "[2187]\tvalidation-rmse:0.02767\n",
      "[2188]\tvalidation-rmse:0.02766\n",
      "[2189]\tvalidation-rmse:0.02766\n",
      "[2190]\tvalidation-rmse:0.02766\n",
      "[2191]\tvalidation-rmse:0.02766\n",
      "[2192]\tvalidation-rmse:0.02766\n",
      "[2193]\tvalidation-rmse:0.02766\n",
      "[2194]\tvalidation-rmse:0.02766\n",
      "[2195]\tvalidation-rmse:0.02766\n",
      "[2196]\tvalidation-rmse:0.02766\n",
      "[2197]\tvalidation-rmse:0.02766\n",
      "[2198]\tvalidation-rmse:0.02766\n",
      "[2199]\tvalidation-rmse:0.02767\n",
      "[2200]\tvalidation-rmse:0.02767\n",
      "[2201]\tvalidation-rmse:0.02767\n",
      "[2202]\tvalidation-rmse:0.02766\n",
      "[2203]\tvalidation-rmse:0.02766\n",
      "[2204]\tvalidation-rmse:0.02766\n",
      "[2205]\tvalidation-rmse:0.02766\n",
      "[2206]\tvalidation-rmse:0.02766\n",
      "[2207]\tvalidation-rmse:0.02766\n",
      "[2208]\tvalidation-rmse:0.02766\n",
      "[2209]\tvalidation-rmse:0.02766\n",
      "[2210]\tvalidation-rmse:0.02766\n",
      "[2211]\tvalidation-rmse:0.02766\n",
      "[2212]\tvalidation-rmse:0.02766\n",
      "[2213]\tvalidation-rmse:0.02766\n",
      "[2214]\tvalidation-rmse:0.02766\n",
      "[2215]\tvalidation-rmse:0.02766\n",
      "[2216]\tvalidation-rmse:0.02766\n",
      "[2217]\tvalidation-rmse:0.02766\n",
      "[2218]\tvalidation-rmse:0.02766\n",
      "[2219]\tvalidation-rmse:0.02766\n",
      "[2220]\tvalidation-rmse:0.02766\n",
      "[2221]\tvalidation-rmse:0.02766\n",
      "[2222]\tvalidation-rmse:0.02766\n",
      "[2223]\tvalidation-rmse:0.02766\n",
      "[2224]\tvalidation-rmse:0.02766\n",
      "[2225]\tvalidation-rmse:0.02766\n",
      "[2226]\tvalidation-rmse:0.02766\n",
      "[2227]\tvalidation-rmse:0.02766\n",
      "[2228]\tvalidation-rmse:0.02766\n",
      "[2229]\tvalidation-rmse:0.02766\n",
      "[2230]\tvalidation-rmse:0.02766\n",
      "[2231]\tvalidation-rmse:0.02766\n",
      "[2232]\tvalidation-rmse:0.02766\n",
      "[2233]\tvalidation-rmse:0.02766\n",
      "[2234]\tvalidation-rmse:0.02766\n",
      "[2235]\tvalidation-rmse:0.02766\n",
      "[2236]\tvalidation-rmse:0.02766\n",
      "[2237]\tvalidation-rmse:0.02766\n",
      "[2238]\tvalidation-rmse:0.02766\n",
      "[2239]\tvalidation-rmse:0.02766\n",
      "[2240]\tvalidation-rmse:0.02766\n",
      "[2241]\tvalidation-rmse:0.02766\n",
      "[2242]\tvalidation-rmse:0.02766\n",
      "[2243]\tvalidation-rmse:0.02766\n",
      "[2244]\tvalidation-rmse:0.02765\n",
      "[2245]\tvalidation-rmse:0.02765\n",
      "[2246]\tvalidation-rmse:0.02765\n",
      "[2247]\tvalidation-rmse:0.02765\n",
      "[2248]\tvalidation-rmse:0.02765\n",
      "[2249]\tvalidation-rmse:0.02765\n",
      "[2250]\tvalidation-rmse:0.02765\n",
      "[2251]\tvalidation-rmse:0.02765\n",
      "[2252]\tvalidation-rmse:0.02765\n",
      "[2253]\tvalidation-rmse:0.02764\n",
      "[2254]\tvalidation-rmse:0.02764\n",
      "[2255]\tvalidation-rmse:0.02764\n",
      "[2256]\tvalidation-rmse:0.02764\n",
      "[2257]\tvalidation-rmse:0.02764\n",
      "[2258]\tvalidation-rmse:0.02764\n",
      "[2259]\tvalidation-rmse:0.02764\n",
      "[2260]\tvalidation-rmse:0.02764\n",
      "[2261]\tvalidation-rmse:0.02764\n",
      "[2262]\tvalidation-rmse:0.02764\n",
      "[2263]\tvalidation-rmse:0.02764\n",
      "[2264]\tvalidation-rmse:0.02764\n",
      "[2265]\tvalidation-rmse:0.02764\n",
      "[2266]\tvalidation-rmse:0.02764\n",
      "[2267]\tvalidation-rmse:0.02764\n",
      "[2268]\tvalidation-rmse:0.02764\n",
      "[2269]\tvalidation-rmse:0.02764\n",
      "[2270]\tvalidation-rmse:0.02764\n",
      "[2271]\tvalidation-rmse:0.02764\n",
      "[2272]\tvalidation-rmse:0.02764\n",
      "[2273]\tvalidation-rmse:0.02764\n",
      "[2274]\tvalidation-rmse:0.02764\n",
      "[2275]\tvalidation-rmse:0.02764\n",
      "[2276]\tvalidation-rmse:0.02764\n",
      "[2277]\tvalidation-rmse:0.02764\n",
      "[2278]\tvalidation-rmse:0.02764\n",
      "[2279]\tvalidation-rmse:0.02764\n",
      "[2280]\tvalidation-rmse:0.02763\n",
      "[2281]\tvalidation-rmse:0.02763\n",
      "[2282]\tvalidation-rmse:0.02763\n",
      "[2283]\tvalidation-rmse:0.02763\n",
      "[2284]\tvalidation-rmse:0.02764\n",
      "[2285]\tvalidation-rmse:0.02764\n",
      "[2286]\tvalidation-rmse:0.02764\n",
      "[2287]\tvalidation-rmse:0.02764\n",
      "[2288]\tvalidation-rmse:0.02764\n",
      "[2289]\tvalidation-rmse:0.02764\n",
      "[2290]\tvalidation-rmse:0.02764\n",
      "[2291]\tvalidation-rmse:0.02764\n",
      "[2292]\tvalidation-rmse:0.02764\n",
      "[2293]\tvalidation-rmse:0.02764\n",
      "[2294]\tvalidation-rmse:0.02764\n",
      "[2295]\tvalidation-rmse:0.02764\n",
      "[2296]\tvalidation-rmse:0.02764\n",
      "[2297]\tvalidation-rmse:0.02764\n",
      "[2298]\tvalidation-rmse:0.02764\n",
      "[2299]\tvalidation-rmse:0.02764\n",
      "[2300]\tvalidation-rmse:0.02764\n",
      "[2301]\tvalidation-rmse:0.02763\n",
      "[2302]\tvalidation-rmse:0.02763\n",
      "[2303]\tvalidation-rmse:0.02763\n",
      "[2304]\tvalidation-rmse:0.02763\n",
      "[2305]\tvalidation-rmse:0.02763\n",
      "[2306]\tvalidation-rmse:0.02763\n",
      "[2307]\tvalidation-rmse:0.02763\n",
      "[2308]\tvalidation-rmse:0.02763\n",
      "[2309]\tvalidation-rmse:0.02763\n",
      "[2310]\tvalidation-rmse:0.02763\n",
      "[2311]\tvalidation-rmse:0.02763\n",
      "[2312]\tvalidation-rmse:0.02763\n",
      "[2313]\tvalidation-rmse:0.02763\n",
      "[2314]\tvalidation-rmse:0.02763\n",
      "[2315]\tvalidation-rmse:0.02763\n",
      "[2316]\tvalidation-rmse:0.02763\n",
      "[2317]\tvalidation-rmse:0.02763\n",
      "[2318]\tvalidation-rmse:0.02763\n",
      "[2319]\tvalidation-rmse:0.02763\n",
      "[2320]\tvalidation-rmse:0.02763\n",
      "[2321]\tvalidation-rmse:0.02763\n",
      "[2322]\tvalidation-rmse:0.02763\n",
      "[2323]\tvalidation-rmse:0.02762\n",
      "[2324]\tvalidation-rmse:0.02762\n",
      "[2325]\tvalidation-rmse:0.02762\n",
      "[2326]\tvalidation-rmse:0.02762\n",
      "[2327]\tvalidation-rmse:0.02762\n",
      "[2328]\tvalidation-rmse:0.02762\n",
      "[2329]\tvalidation-rmse:0.02762\n",
      "[2330]\tvalidation-rmse:0.02762\n",
      "[2331]\tvalidation-rmse:0.02762\n",
      "[2332]\tvalidation-rmse:0.02762\n",
      "[2333]\tvalidation-rmse:0.02762\n",
      "[2334]\tvalidation-rmse:0.02762\n",
      "[2335]\tvalidation-rmse:0.02762\n",
      "[2336]\tvalidation-rmse:0.02762\n",
      "[2337]\tvalidation-rmse:0.02762\n",
      "[2338]\tvalidation-rmse:0.02762\n",
      "[2339]\tvalidation-rmse:0.02762\n",
      "[2340]\tvalidation-rmse:0.02762\n",
      "[2341]\tvalidation-rmse:0.02762\n",
      "[2342]\tvalidation-rmse:0.02762\n",
      "[2343]\tvalidation-rmse:0.02762\n",
      "[2344]\tvalidation-rmse:0.02762\n",
      "[2345]\tvalidation-rmse:0.02762\n",
      "[2346]\tvalidation-rmse:0.02762\n",
      "[2347]\tvalidation-rmse:0.02762\n",
      "[2348]\tvalidation-rmse:0.02761\n",
      "[2349]\tvalidation-rmse:0.02761\n",
      "[2350]\tvalidation-rmse:0.02761\n",
      "[2351]\tvalidation-rmse:0.02761\n",
      "[2352]\tvalidation-rmse:0.02761\n",
      "[2353]\tvalidation-rmse:0.02761\n",
      "[2354]\tvalidation-rmse:0.02761\n",
      "[2355]\tvalidation-rmse:0.02761\n",
      "[2356]\tvalidation-rmse:0.02761\n",
      "[2357]\tvalidation-rmse:0.02761\n",
      "[2358]\tvalidation-rmse:0.02761\n",
      "[2359]\tvalidation-rmse:0.02761\n",
      "[2360]\tvalidation-rmse:0.02761\n",
      "[2361]\tvalidation-rmse:0.02761\n",
      "[2362]\tvalidation-rmse:0.02761\n",
      "[2363]\tvalidation-rmse:0.02761\n",
      "[2364]\tvalidation-rmse:0.02761\n",
      "[2365]\tvalidation-rmse:0.02761\n",
      "[2366]\tvalidation-rmse:0.02761\n",
      "[2367]\tvalidation-rmse:0.02761\n",
      "[2368]\tvalidation-rmse:0.02761\n",
      "[2369]\tvalidation-rmse:0.02761\n",
      "[2370]\tvalidation-rmse:0.02761\n",
      "[2371]\tvalidation-rmse:0.02761\n",
      "[2372]\tvalidation-rmse:0.02761\n",
      "[2373]\tvalidation-rmse:0.02761\n",
      "[2374]\tvalidation-rmse:0.02761\n",
      "[2375]\tvalidation-rmse:0.02761\n",
      "[2376]\tvalidation-rmse:0.02761\n",
      "[2377]\tvalidation-rmse:0.02761\n",
      "[2378]\tvalidation-rmse:0.02760\n",
      "[2379]\tvalidation-rmse:0.02760\n",
      "[2380]\tvalidation-rmse:0.02760\n",
      "[2381]\tvalidation-rmse:0.02760\n",
      "[2382]\tvalidation-rmse:0.02760\n",
      "[2383]\tvalidation-rmse:0.02760\n",
      "[2384]\tvalidation-rmse:0.02760\n",
      "[2385]\tvalidation-rmse:0.02760\n",
      "[2386]\tvalidation-rmse:0.02760\n",
      "[2387]\tvalidation-rmse:0.02760\n",
      "[2388]\tvalidation-rmse:0.02760\n",
      "[2389]\tvalidation-rmse:0.02760\n",
      "[2390]\tvalidation-rmse:0.02760\n",
      "[2391]\tvalidation-rmse:0.02760\n",
      "[2392]\tvalidation-rmse:0.02760\n",
      "[2393]\tvalidation-rmse:0.02760\n",
      "[2394]\tvalidation-rmse:0.02760\n",
      "[2395]\tvalidation-rmse:0.02760\n",
      "[2396]\tvalidation-rmse:0.02760\n",
      "[2397]\tvalidation-rmse:0.02760\n",
      "[2398]\tvalidation-rmse:0.02760\n",
      "[2399]\tvalidation-rmse:0.02760\n",
      "[2400]\tvalidation-rmse:0.02760\n",
      "[2401]\tvalidation-rmse:0.02760\n",
      "[2402]\tvalidation-rmse:0.02760\n",
      "[2403]\tvalidation-rmse:0.02760\n",
      "[2404]\tvalidation-rmse:0.02760\n",
      "[2405]\tvalidation-rmse:0.02760\n",
      "[2406]\tvalidation-rmse:0.02760\n",
      "[2407]\tvalidation-rmse:0.02760\n",
      "[2408]\tvalidation-rmse:0.02760\n",
      "[2409]\tvalidation-rmse:0.02760\n",
      "[2410]\tvalidation-rmse:0.02760\n",
      "[2411]\tvalidation-rmse:0.02760\n",
      "[2412]\tvalidation-rmse:0.02760\n",
      "[2413]\tvalidation-rmse:0.02760\n",
      "[2414]\tvalidation-rmse:0.02760\n",
      "[2415]\tvalidation-rmse:0.02760\n",
      "[2416]\tvalidation-rmse:0.02760\n",
      "[2417]\tvalidation-rmse:0.02760\n",
      "[2418]\tvalidation-rmse:0.02760\n",
      "[2419]\tvalidation-rmse:0.02760\n",
      "[2420]\tvalidation-rmse:0.02760\n",
      "[2421]\tvalidation-rmse:0.02760\n",
      "[2422]\tvalidation-rmse:0.02760\n",
      "[2423]\tvalidation-rmse:0.02760\n",
      "[2424]\tvalidation-rmse:0.02760\n",
      "[2425]\tvalidation-rmse:0.02760\n",
      "[2426]\tvalidation-rmse:0.02760\n",
      "[2427]\tvalidation-rmse:0.02760\n",
      "[2428]\tvalidation-rmse:0.02760\n",
      "[2429]\tvalidation-rmse:0.02760\n",
      "[2430]\tvalidation-rmse:0.02760\n",
      "[2431]\tvalidation-rmse:0.02760\n",
      "[2432]\tvalidation-rmse:0.02760\n",
      "[2433]\tvalidation-rmse:0.02760\n",
      "[2434]\tvalidation-rmse:0.02760\n",
      "[2435]\tvalidation-rmse:0.02760\n",
      "[2436]\tvalidation-rmse:0.02760\n",
      "[2437]\tvalidation-rmse:0.02760\n",
      "[2438]\tvalidation-rmse:0.02760\n",
      "[2439]\tvalidation-rmse:0.02760\n",
      "[2440]\tvalidation-rmse:0.02760\n",
      "[2441]\tvalidation-rmse:0.02760\n",
      "[2442]\tvalidation-rmse:0.02760\n",
      "[2443]\tvalidation-rmse:0.02760\n",
      "[2444]\tvalidation-rmse:0.02760\n",
      "[2445]\tvalidation-rmse:0.02760\n",
      "[2446]\tvalidation-rmse:0.02760\n",
      "[2447]\tvalidation-rmse:0.02759\n",
      "[2448]\tvalidation-rmse:0.02759\n",
      "[2449]\tvalidation-rmse:0.02759\n",
      "[2450]\tvalidation-rmse:0.02759\n",
      "[2451]\tvalidation-rmse:0.02759\n",
      "[2452]\tvalidation-rmse:0.02759\n",
      "[2453]\tvalidation-rmse:0.02759\n",
      "[2454]\tvalidation-rmse:0.02759\n",
      "[2455]\tvalidation-rmse:0.02759\n",
      "[2456]\tvalidation-rmse:0.02759\n",
      "[2457]\tvalidation-rmse:0.02759\n",
      "[2458]\tvalidation-rmse:0.02759\n",
      "[2459]\tvalidation-rmse:0.02759\n",
      "[2460]\tvalidation-rmse:0.02759\n",
      "[2461]\tvalidation-rmse:0.02759\n",
      "[2462]\tvalidation-rmse:0.02760\n",
      "[2463]\tvalidation-rmse:0.02759\n",
      "[2464]\tvalidation-rmse:0.02759\n",
      "[2465]\tvalidation-rmse:0.02759\n",
      "[2466]\tvalidation-rmse:0.02759\n",
      "[2467]\tvalidation-rmse:0.02759\n",
      "[2468]\tvalidation-rmse:0.02759\n",
      "[2469]\tvalidation-rmse:0.02759\n",
      "[2470]\tvalidation-rmse:0.02759\n",
      "[2471]\tvalidation-rmse:0.02759\n",
      "[2472]\tvalidation-rmse:0.02760\n",
      "[2473]\tvalidation-rmse:0.02759\n",
      "[2474]\tvalidation-rmse:0.02759\n",
      "[2475]\tvalidation-rmse:0.02759\n",
      "[2476]\tvalidation-rmse:0.02759\n",
      "[2477]\tvalidation-rmse:0.02759\n",
      "[2478]\tvalidation-rmse:0.02759\n",
      "[2479]\tvalidation-rmse:0.02759\n",
      "[2480]\tvalidation-rmse:0.02759\n",
      "[2481]\tvalidation-rmse:0.02759\n",
      "[2482]\tvalidation-rmse:0.02759\n",
      "[2483]\tvalidation-rmse:0.02759\n",
      "[2484]\tvalidation-rmse:0.02759\n",
      "[2485]\tvalidation-rmse:0.02759\n",
      "[2486]\tvalidation-rmse:0.02759\n",
      "[2487]\tvalidation-rmse:0.02759\n",
      "[2488]\tvalidation-rmse:0.02759\n",
      "[2489]\tvalidation-rmse:0.02759\n",
      "[2490]\tvalidation-rmse:0.02759\n",
      "[2491]\tvalidation-rmse:0.02759\n",
      "[2492]\tvalidation-rmse:0.02759\n",
      "[2493]\tvalidation-rmse:0.02759\n",
      "[2494]\tvalidation-rmse:0.02759\n",
      "[2495]\tvalidation-rmse:0.02759\n",
      "[2496]\tvalidation-rmse:0.02759\n",
      "[2497]\tvalidation-rmse:0.02759\n",
      "[2498]\tvalidation-rmse:0.02759\n",
      "[2499]\tvalidation-rmse:0.02759\n",
      "[2500]\tvalidation-rmse:0.02759\n",
      "[2501]\tvalidation-rmse:0.02759\n",
      "[2502]\tvalidation-rmse:0.02759\n",
      "[2503]\tvalidation-rmse:0.02759\n",
      "[2504]\tvalidation-rmse:0.02758\n",
      "[2505]\tvalidation-rmse:0.02758\n",
      "[2506]\tvalidation-rmse:0.02758\n",
      "[2507]\tvalidation-rmse:0.02758\n",
      "[2508]\tvalidation-rmse:0.02758\n",
      "[2509]\tvalidation-rmse:0.02758\n",
      "[2510]\tvalidation-rmse:0.02758\n",
      "[2511]\tvalidation-rmse:0.02758\n",
      "[2512]\tvalidation-rmse:0.02758\n",
      "[2513]\tvalidation-rmse:0.02758\n",
      "[2514]\tvalidation-rmse:0.02758\n",
      "[2515]\tvalidation-rmse:0.02758\n",
      "[2516]\tvalidation-rmse:0.02758\n",
      "[2517]\tvalidation-rmse:0.02758\n",
      "[2518]\tvalidation-rmse:0.02758\n",
      "[2519]\tvalidation-rmse:0.02758\n",
      "[2520]\tvalidation-rmse:0.02758\n",
      "[2521]\tvalidation-rmse:0.02758\n",
      "[2522]\tvalidation-rmse:0.02758\n",
      "[2523]\tvalidation-rmse:0.02758\n",
      "[2524]\tvalidation-rmse:0.02758\n",
      "[2525]\tvalidation-rmse:0.02758\n",
      "[2526]\tvalidation-rmse:0.02758\n",
      "[2527]\tvalidation-rmse:0.02758\n",
      "[2528]\tvalidation-rmse:0.02758\n",
      "[2529]\tvalidation-rmse:0.02758\n",
      "[2530]\tvalidation-rmse:0.02758\n",
      "[2531]\tvalidation-rmse:0.02757\n",
      "[2532]\tvalidation-rmse:0.02757\n",
      "[2533]\tvalidation-rmse:0.02757\n",
      "[2534]\tvalidation-rmse:0.02757\n",
      "[2535]\tvalidation-rmse:0.02757\n",
      "[2536]\tvalidation-rmse:0.02757\n",
      "[2537]\tvalidation-rmse:0.02757\n",
      "[2538]\tvalidation-rmse:0.02757\n",
      "[2539]\tvalidation-rmse:0.02757\n",
      "[2540]\tvalidation-rmse:0.02757\n",
      "[2541]\tvalidation-rmse:0.02757\n",
      "[2542]\tvalidation-rmse:0.02757\n",
      "[2543]\tvalidation-rmse:0.02757\n",
      "[2544]\tvalidation-rmse:0.02757\n",
      "[2545]\tvalidation-rmse:0.02757\n",
      "[2546]\tvalidation-rmse:0.02757\n",
      "[2547]\tvalidation-rmse:0.02757\n",
      "[2548]\tvalidation-rmse:0.02757\n",
      "[2549]\tvalidation-rmse:0.02757\n",
      "[2550]\tvalidation-rmse:0.02757\n",
      "[2551]\tvalidation-rmse:0.02757\n",
      "[2552]\tvalidation-rmse:0.02757\n",
      "[2553]\tvalidation-rmse:0.02757\n",
      "[2554]\tvalidation-rmse:0.02757\n",
      "[2555]\tvalidation-rmse:0.02757\n",
      "[2556]\tvalidation-rmse:0.02757\n",
      "[2557]\tvalidation-rmse:0.02757\n",
      "[2558]\tvalidation-rmse:0.02757\n",
      "[2559]\tvalidation-rmse:0.02757\n",
      "[2560]\tvalidation-rmse:0.02757\n",
      "[2561]\tvalidation-rmse:0.02757\n",
      "[2562]\tvalidation-rmse:0.02757\n",
      "[2563]\tvalidation-rmse:0.02757\n",
      "[2564]\tvalidation-rmse:0.02757\n",
      "[2565]\tvalidation-rmse:0.02757\n",
      "[2566]\tvalidation-rmse:0.02757\n",
      "[2567]\tvalidation-rmse:0.02757\n",
      "[2568]\tvalidation-rmse:0.02757\n",
      "[2569]\tvalidation-rmse:0.02757\n",
      "[2570]\tvalidation-rmse:0.02757\n",
      "[2571]\tvalidation-rmse:0.02757\n",
      "[2572]\tvalidation-rmse:0.02757\n",
      "[2573]\tvalidation-rmse:0.02757\n",
      "[2574]\tvalidation-rmse:0.02757\n",
      "[2575]\tvalidation-rmse:0.02757\n",
      "[2576]\tvalidation-rmse:0.02757\n",
      "[2577]\tvalidation-rmse:0.02757\n",
      "[2578]\tvalidation-rmse:0.02756\n",
      "[2579]\tvalidation-rmse:0.02756\n",
      "[2580]\tvalidation-rmse:0.02756\n",
      "[2581]\tvalidation-rmse:0.02756\n",
      "[2582]\tvalidation-rmse:0.02756\n",
      "[2583]\tvalidation-rmse:0.02756\n",
      "[2584]\tvalidation-rmse:0.02756\n",
      "[2585]\tvalidation-rmse:0.02756\n",
      "[2586]\tvalidation-rmse:0.02756\n",
      "[2587]\tvalidation-rmse:0.02756\n",
      "[2588]\tvalidation-rmse:0.02756\n",
      "[2589]\tvalidation-rmse:0.02756\n",
      "[2590]\tvalidation-rmse:0.02756\n",
      "[2591]\tvalidation-rmse:0.02756\n",
      "[2592]\tvalidation-rmse:0.02756\n",
      "[2593]\tvalidation-rmse:0.02756\n",
      "[2594]\tvalidation-rmse:0.02756\n",
      "[2595]\tvalidation-rmse:0.02756\n",
      "[2596]\tvalidation-rmse:0.02756\n",
      "[2597]\tvalidation-rmse:0.02756\n",
      "[2598]\tvalidation-rmse:0.02756\n",
      "[2599]\tvalidation-rmse:0.02756\n",
      "[2600]\tvalidation-rmse:0.02756\n",
      "[2601]\tvalidation-rmse:0.02756\n",
      "[2602]\tvalidation-rmse:0.02756\n",
      "[2603]\tvalidation-rmse:0.02756\n",
      "[2604]\tvalidation-rmse:0.02756\n",
      "[2605]\tvalidation-rmse:0.02756\n",
      "[2606]\tvalidation-rmse:0.02756\n",
      "[2607]\tvalidation-rmse:0.02756\n",
      "[2608]\tvalidation-rmse:0.02756\n",
      "[2609]\tvalidation-rmse:0.02756\n",
      "[2610]\tvalidation-rmse:0.02756\n",
      "[2611]\tvalidation-rmse:0.02756\n",
      "[2612]\tvalidation-rmse:0.02756\n",
      "[2613]\tvalidation-rmse:0.02756\n",
      "[2614]\tvalidation-rmse:0.02756\n",
      "[2615]\tvalidation-rmse:0.02756\n",
      "[2616]\tvalidation-rmse:0.02756\n",
      "[2617]\tvalidation-rmse:0.02756\n",
      "[2618]\tvalidation-rmse:0.02756\n",
      "[2619]\tvalidation-rmse:0.02756\n",
      "[2620]\tvalidation-rmse:0.02756\n",
      "[2621]\tvalidation-rmse:0.02756\n",
      "[2622]\tvalidation-rmse:0.02756\n",
      "[2623]\tvalidation-rmse:0.02755\n",
      "[2624]\tvalidation-rmse:0.02755\n",
      "[2625]\tvalidation-rmse:0.02755\n",
      "[2626]\tvalidation-rmse:0.02755\n",
      "[2627]\tvalidation-rmse:0.02755\n",
      "[2628]\tvalidation-rmse:0.02755\n",
      "[2629]\tvalidation-rmse:0.02755\n",
      "[2630]\tvalidation-rmse:0.02755\n",
      "[2631]\tvalidation-rmse:0.02755\n",
      "[2632]\tvalidation-rmse:0.02755\n",
      "[2633]\tvalidation-rmse:0.02755\n",
      "[2634]\tvalidation-rmse:0.02755\n",
      "[2635]\tvalidation-rmse:0.02755\n",
      "[2636]\tvalidation-rmse:0.02755\n",
      "[2637]\tvalidation-rmse:0.02755\n",
      "[2638]\tvalidation-rmse:0.02755\n",
      "[2639]\tvalidation-rmse:0.02755\n",
      "[2640]\tvalidation-rmse:0.02755\n",
      "[2641]\tvalidation-rmse:0.02755\n",
      "[2642]\tvalidation-rmse:0.02755\n",
      "[2643]\tvalidation-rmse:0.02755\n",
      "[2644]\tvalidation-rmse:0.02755\n",
      "[2645]\tvalidation-rmse:0.02755\n",
      "[2646]\tvalidation-rmse:0.02755\n",
      "[2647]\tvalidation-rmse:0.02755\n",
      "[2648]\tvalidation-rmse:0.02755\n",
      "[2649]\tvalidation-rmse:0.02755\n",
      "[2650]\tvalidation-rmse:0.02755\n",
      "[2651]\tvalidation-rmse:0.02755\n",
      "[2652]\tvalidation-rmse:0.02755\n",
      "[2653]\tvalidation-rmse:0.02754\n",
      "[2654]\tvalidation-rmse:0.02754\n",
      "[2655]\tvalidation-rmse:0.02754\n",
      "[2656]\tvalidation-rmse:0.02754\n",
      "[2657]\tvalidation-rmse:0.02754\n",
      "[2658]\tvalidation-rmse:0.02754\n",
      "[2659]\tvalidation-rmse:0.02754\n",
      "[2660]\tvalidation-rmse:0.02754\n",
      "[2661]\tvalidation-rmse:0.02754\n",
      "[2662]\tvalidation-rmse:0.02754\n",
      "[2663]\tvalidation-rmse:0.02754\n",
      "[2664]\tvalidation-rmse:0.02754\n",
      "[2665]\tvalidation-rmse:0.02754\n",
      "[2666]\tvalidation-rmse:0.02754\n",
      "[2667]\tvalidation-rmse:0.02754\n",
      "[2668]\tvalidation-rmse:0.02754\n",
      "[2669]\tvalidation-rmse:0.02754\n",
      "[2670]\tvalidation-rmse:0.02754\n",
      "[2671]\tvalidation-rmse:0.02754\n",
      "[2672]\tvalidation-rmse:0.02754\n",
      "[2673]\tvalidation-rmse:0.02754\n",
      "[2674]\tvalidation-rmse:0.02754\n",
      "[2675]\tvalidation-rmse:0.02754\n",
      "[2676]\tvalidation-rmse:0.02754\n",
      "[2677]\tvalidation-rmse:0.02754\n",
      "[2678]\tvalidation-rmse:0.02754\n",
      "[2679]\tvalidation-rmse:0.02754\n",
      "[2680]\tvalidation-rmse:0.02754\n",
      "[2681]\tvalidation-rmse:0.02754\n",
      "[2682]\tvalidation-rmse:0.02754\n",
      "[2683]\tvalidation-rmse:0.02754\n",
      "[2684]\tvalidation-rmse:0.02754\n",
      "[2685]\tvalidation-rmse:0.02754\n",
      "[2686]\tvalidation-rmse:0.02754\n",
      "[2687]\tvalidation-rmse:0.02754\n",
      "[2688]\tvalidation-rmse:0.02754\n",
      "[2689]\tvalidation-rmse:0.02754\n",
      "[2690]\tvalidation-rmse:0.02754\n",
      "[2691]\tvalidation-rmse:0.02754\n",
      "[2692]\tvalidation-rmse:0.02754\n",
      "[2693]\tvalidation-rmse:0.02754\n",
      "[2694]\tvalidation-rmse:0.02754\n",
      "[2695]\tvalidation-rmse:0.02754\n",
      "[2696]\tvalidation-rmse:0.02754\n",
      "[2697]\tvalidation-rmse:0.02754\n",
      "[2698]\tvalidation-rmse:0.02754\n",
      "[2699]\tvalidation-rmse:0.02754\n",
      "[2700]\tvalidation-rmse:0.02754\n",
      "[2701]\tvalidation-rmse:0.02754\n",
      "[2702]\tvalidation-rmse:0.02754\n",
      "[2703]\tvalidation-rmse:0.02754\n",
      "[2704]\tvalidation-rmse:0.02754\n",
      "[2705]\tvalidation-rmse:0.02754\n",
      "[2706]\tvalidation-rmse:0.02754\n",
      "[2707]\tvalidation-rmse:0.02754\n",
      "[2708]\tvalidation-rmse:0.02754\n",
      "[2709]\tvalidation-rmse:0.02754\n",
      "[2710]\tvalidation-rmse:0.02753\n",
      "[2711]\tvalidation-rmse:0.02753\n",
      "[2712]\tvalidation-rmse:0.02753\n",
      "[2713]\tvalidation-rmse:0.02753\n",
      "[2714]\tvalidation-rmse:0.02753\n",
      "[2715]\tvalidation-rmse:0.02753\n",
      "[2716]\tvalidation-rmse:0.02753\n",
      "[2717]\tvalidation-rmse:0.02753\n",
      "[2718]\tvalidation-rmse:0.02753\n",
      "[2719]\tvalidation-rmse:0.02753\n",
      "[2720]\tvalidation-rmse:0.02753\n",
      "[2721]\tvalidation-rmse:0.02753\n",
      "[2722]\tvalidation-rmse:0.02753\n",
      "[2723]\tvalidation-rmse:0.02753\n",
      "[2724]\tvalidation-rmse:0.02753\n",
      "[2725]\tvalidation-rmse:0.02753\n",
      "[2726]\tvalidation-rmse:0.02753\n",
      "[2727]\tvalidation-rmse:0.02753\n",
      "[2728]\tvalidation-rmse:0.02753\n",
      "[2729]\tvalidation-rmse:0.02753\n",
      "[2730]\tvalidation-rmse:0.02753\n",
      "[2731]\tvalidation-rmse:0.02753\n",
      "[2732]\tvalidation-rmse:0.02753\n",
      "[2733]\tvalidation-rmse:0.02753\n",
      "[2734]\tvalidation-rmse:0.02753\n",
      "[2735]\tvalidation-rmse:0.02753\n",
      "[2736]\tvalidation-rmse:0.02753\n",
      "[2737]\tvalidation-rmse:0.02753\n",
      "[2738]\tvalidation-rmse:0.02753\n",
      "[2739]\tvalidation-rmse:0.02753\n",
      "[2740]\tvalidation-rmse:0.02753\n",
      "[2741]\tvalidation-rmse:0.02753\n",
      "[2742]\tvalidation-rmse:0.02753\n",
      "[2743]\tvalidation-rmse:0.02753\n",
      "[2744]\tvalidation-rmse:0.02753\n",
      "[2745]\tvalidation-rmse:0.02753\n",
      "[2746]\tvalidation-rmse:0.02753\n",
      "[2747]\tvalidation-rmse:0.02753\n",
      "[2748]\tvalidation-rmse:0.02753\n",
      "[2749]\tvalidation-rmse:0.02753\n",
      "[2750]\tvalidation-rmse:0.02753\n",
      "[2751]\tvalidation-rmse:0.02753\n",
      "[2752]\tvalidation-rmse:0.02753\n",
      "[2753]\tvalidation-rmse:0.02753\n",
      "[2754]\tvalidation-rmse:0.02753\n",
      "[2755]\tvalidation-rmse:0.02753\n",
      "[2756]\tvalidation-rmse:0.02753\n",
      "[2757]\tvalidation-rmse:0.02753\n",
      "[2758]\tvalidation-rmse:0.02753\n",
      "[2759]\tvalidation-rmse:0.02753\n",
      "[2760]\tvalidation-rmse:0.02753\n",
      "[2761]\tvalidation-rmse:0.02753\n",
      "[2762]\tvalidation-rmse:0.02753\n",
      "[2763]\tvalidation-rmse:0.02753\n",
      "[2764]\tvalidation-rmse:0.02753\n",
      "[2765]\tvalidation-rmse:0.02753\n",
      "[2766]\tvalidation-rmse:0.02753\n",
      "[2767]\tvalidation-rmse:0.02752\n",
      "[2768]\tvalidation-rmse:0.02752\n",
      "[2769]\tvalidation-rmse:0.02752\n",
      "[2770]\tvalidation-rmse:0.02752\n",
      "[2771]\tvalidation-rmse:0.02752\n",
      "[2772]\tvalidation-rmse:0.02752\n",
      "[2773]\tvalidation-rmse:0.02752\n",
      "[2774]\tvalidation-rmse:0.02752\n",
      "[2775]\tvalidation-rmse:0.02752\n",
      "[2776]\tvalidation-rmse:0.02752\n",
      "[2777]\tvalidation-rmse:0.02752\n",
      "[2778]\tvalidation-rmse:0.02752\n",
      "[2779]\tvalidation-rmse:0.02752\n",
      "[2780]\tvalidation-rmse:0.02752\n",
      "[2781]\tvalidation-rmse:0.02752\n",
      "[2782]\tvalidation-rmse:0.02752\n",
      "[2783]\tvalidation-rmse:0.02752\n",
      "[2784]\tvalidation-rmse:0.02752\n",
      "[2785]\tvalidation-rmse:0.02752\n",
      "[2786]\tvalidation-rmse:0.02752\n",
      "[2787]\tvalidation-rmse:0.02752\n",
      "[2788]\tvalidation-rmse:0.02752\n",
      "[2789]\tvalidation-rmse:0.02752\n",
      "[2790]\tvalidation-rmse:0.02752\n",
      "[2791]\tvalidation-rmse:0.02752\n",
      "[2792]\tvalidation-rmse:0.02752\n",
      "[2793]\tvalidation-rmse:0.02752\n",
      "[2794]\tvalidation-rmse:0.02752\n",
      "[2795]\tvalidation-rmse:0.02752\n",
      "[2796]\tvalidation-rmse:0.02752\n",
      "[2797]\tvalidation-rmse:0.02752\n",
      "[2798]\tvalidation-rmse:0.02752\n",
      "[2799]\tvalidation-rmse:0.02752\n",
      "[2800]\tvalidation-rmse:0.02752\n",
      "[2801]\tvalidation-rmse:0.02752\n",
      "[2802]\tvalidation-rmse:0.02752\n",
      "[2803]\tvalidation-rmse:0.02752\n",
      "[2804]\tvalidation-rmse:0.02752\n",
      "[2805]\tvalidation-rmse:0.02752\n",
      "[2806]\tvalidation-rmse:0.02752\n",
      "[2807]\tvalidation-rmse:0.02752\n",
      "[2808]\tvalidation-rmse:0.02751\n",
      "[2809]\tvalidation-rmse:0.02751\n",
      "[2810]\tvalidation-rmse:0.02751\n",
      "[2811]\tvalidation-rmse:0.02751\n",
      "[2812]\tvalidation-rmse:0.02751\n",
      "[2813]\tvalidation-rmse:0.02751\n",
      "[2814]\tvalidation-rmse:0.02751\n",
      "[2815]\tvalidation-rmse:0.02751\n",
      "[2816]\tvalidation-rmse:0.02751\n",
      "[2817]\tvalidation-rmse:0.02751\n",
      "[2818]\tvalidation-rmse:0.02751\n",
      "[2819]\tvalidation-rmse:0.02751\n",
      "[2820]\tvalidation-rmse:0.02751\n",
      "[2821]\tvalidation-rmse:0.02751\n",
      "[2822]\tvalidation-rmse:0.02751\n",
      "[2823]\tvalidation-rmse:0.02751\n",
      "[2824]\tvalidation-rmse:0.02751\n",
      "[2825]\tvalidation-rmse:0.02751\n",
      "[2826]\tvalidation-rmse:0.02751\n",
      "[2827]\tvalidation-rmse:0.02751\n",
      "[2828]\tvalidation-rmse:0.02751\n",
      "[2829]\tvalidation-rmse:0.02751\n",
      "[2830]\tvalidation-rmse:0.02751\n",
      "[2831]\tvalidation-rmse:0.02751\n",
      "[2832]\tvalidation-rmse:0.02751\n",
      "[2833]\tvalidation-rmse:0.02751\n",
      "[2834]\tvalidation-rmse:0.02751\n",
      "[2835]\tvalidation-rmse:0.02751\n",
      "[2836]\tvalidation-rmse:0.02751\n",
      "[2837]\tvalidation-rmse:0.02751\n",
      "[2838]\tvalidation-rmse:0.02751\n",
      "[2839]\tvalidation-rmse:0.02750\n",
      "[2840]\tvalidation-rmse:0.02750\n",
      "[2841]\tvalidation-rmse:0.02750\n",
      "[2842]\tvalidation-rmse:0.02750\n",
      "[2843]\tvalidation-rmse:0.02750\n",
      "[2844]\tvalidation-rmse:0.02750\n",
      "[2845]\tvalidation-rmse:0.02750\n",
      "[2846]\tvalidation-rmse:0.02750\n",
      "[2847]\tvalidation-rmse:0.02750\n",
      "[2848]\tvalidation-rmse:0.02750\n",
      "[2849]\tvalidation-rmse:0.02750\n",
      "[2850]\tvalidation-rmse:0.02750\n",
      "[2851]\tvalidation-rmse:0.02750\n",
      "[2852]\tvalidation-rmse:0.02750\n",
      "[2853]\tvalidation-rmse:0.02750\n",
      "[2854]\tvalidation-rmse:0.02750\n",
      "[2855]\tvalidation-rmse:0.02750\n",
      "[2856]\tvalidation-rmse:0.02750\n",
      "[2857]\tvalidation-rmse:0.02750\n",
      "[2858]\tvalidation-rmse:0.02750\n",
      "[2859]\tvalidation-rmse:0.02750\n",
      "[2860]\tvalidation-rmse:0.02750\n",
      "[2861]\tvalidation-rmse:0.02750\n",
      "[2862]\tvalidation-rmse:0.02750\n",
      "[2863]\tvalidation-rmse:0.02750\n",
      "[2864]\tvalidation-rmse:0.02750\n",
      "[2865]\tvalidation-rmse:0.02750\n",
      "[2866]\tvalidation-rmse:0.02750\n",
      "[2867]\tvalidation-rmse:0.02749\n",
      "[2868]\tvalidation-rmse:0.02749\n",
      "[2869]\tvalidation-rmse:0.02749\n",
      "[2870]\tvalidation-rmse:0.02749\n",
      "[2871]\tvalidation-rmse:0.02749\n",
      "[2872]\tvalidation-rmse:0.02749\n",
      "[2873]\tvalidation-rmse:0.02749\n",
      "[2874]\tvalidation-rmse:0.02749\n",
      "[2875]\tvalidation-rmse:0.02749\n",
      "[2876]\tvalidation-rmse:0.02749\n",
      "[2877]\tvalidation-rmse:0.02749\n",
      "[2878]\tvalidation-rmse:0.02749\n",
      "[2879]\tvalidation-rmse:0.02749\n",
      "[2880]\tvalidation-rmse:0.02749\n",
      "[2881]\tvalidation-rmse:0.02749\n",
      "[2882]\tvalidation-rmse:0.02749\n",
      "[2883]\tvalidation-rmse:0.02749\n",
      "[2884]\tvalidation-rmse:0.02749\n",
      "[2885]\tvalidation-rmse:0.02749\n",
      "[2886]\tvalidation-rmse:0.02749\n",
      "[2887]\tvalidation-rmse:0.02749\n",
      "[2888]\tvalidation-rmse:0.02749\n",
      "[2889]\tvalidation-rmse:0.02748\n",
      "[2890]\tvalidation-rmse:0.02748\n",
      "[2891]\tvalidation-rmse:0.02748\n",
      "[2892]\tvalidation-rmse:0.02748\n",
      "[2893]\tvalidation-rmse:0.02748\n",
      "[2894]\tvalidation-rmse:0.02748\n",
      "[2895]\tvalidation-rmse:0.02748\n",
      "[2896]\tvalidation-rmse:0.02748\n",
      "[2897]\tvalidation-rmse:0.02748\n",
      "[2898]\tvalidation-rmse:0.02748\n",
      "[2899]\tvalidation-rmse:0.02748\n",
      "[2900]\tvalidation-rmse:0.02748\n",
      "[2901]\tvalidation-rmse:0.02748\n",
      "[2902]\tvalidation-rmse:0.02748\n",
      "[2903]\tvalidation-rmse:0.02748\n",
      "[2904]\tvalidation-rmse:0.02748\n",
      "[2905]\tvalidation-rmse:0.02748\n",
      "[2906]\tvalidation-rmse:0.02748\n",
      "[2907]\tvalidation-rmse:0.02748\n",
      "[2908]\tvalidation-rmse:0.02748\n",
      "[2909]\tvalidation-rmse:0.02748\n",
      "[2910]\tvalidation-rmse:0.02748\n",
      "[2911]\tvalidation-rmse:0.02748\n",
      "[2912]\tvalidation-rmse:0.02748\n",
      "[2913]\tvalidation-rmse:0.02748\n",
      "[2914]\tvalidation-rmse:0.02747\n",
      "[2915]\tvalidation-rmse:0.02747\n",
      "[2916]\tvalidation-rmse:0.02747\n",
      "[2917]\tvalidation-rmse:0.02747\n",
      "[2918]\tvalidation-rmse:0.02747\n",
      "[2919]\tvalidation-rmse:0.02747\n",
      "[2920]\tvalidation-rmse:0.02747\n",
      "[2921]\tvalidation-rmse:0.02747\n",
      "[2922]\tvalidation-rmse:0.02747\n",
      "[2923]\tvalidation-rmse:0.02747\n",
      "[2924]\tvalidation-rmse:0.02747\n",
      "[2925]\tvalidation-rmse:0.02747\n",
      "[2926]\tvalidation-rmse:0.02747\n",
      "[2927]\tvalidation-rmse:0.02747\n",
      "[2928]\tvalidation-rmse:0.02747\n",
      "[2929]\tvalidation-rmse:0.02747\n",
      "[2930]\tvalidation-rmse:0.02747\n",
      "[2931]\tvalidation-rmse:0.02747\n",
      "[2932]\tvalidation-rmse:0.02747\n",
      "[2933]\tvalidation-rmse:0.02748\n",
      "[2934]\tvalidation-rmse:0.02748\n",
      "[2935]\tvalidation-rmse:0.02748\n",
      "[2936]\tvalidation-rmse:0.02748\n",
      "[2937]\tvalidation-rmse:0.02748\n",
      "[2938]\tvalidation-rmse:0.02748\n",
      "[2939]\tvalidation-rmse:0.02748\n",
      "[2940]\tvalidation-rmse:0.02747\n",
      "[2941]\tvalidation-rmse:0.02747\n",
      "[2942]\tvalidation-rmse:0.02747\n",
      "[2943]\tvalidation-rmse:0.02747\n",
      "[2944]\tvalidation-rmse:0.02747\n",
      "[2945]\tvalidation-rmse:0.02747\n",
      "[2946]\tvalidation-rmse:0.02747\n",
      "[2947]\tvalidation-rmse:0.02747\n",
      "[2948]\tvalidation-rmse:0.02747\n",
      "[2949]\tvalidation-rmse:0.02747\n",
      "[2950]\tvalidation-rmse:0.02747\n",
      "[2951]\tvalidation-rmse:0.02747\n",
      "[2952]\tvalidation-rmse:0.02747\n",
      "[2953]\tvalidation-rmse:0.02747\n",
      "[2954]\tvalidation-rmse:0.02747\n",
      "[2955]\tvalidation-rmse:0.02747\n",
      "[2956]\tvalidation-rmse:0.02747\n",
      "[2957]\tvalidation-rmse:0.02747\n",
      "[2958]\tvalidation-rmse:0.02747\n",
      "[2959]\tvalidation-rmse:0.02747\n",
      "[2960]\tvalidation-rmse:0.02747\n",
      "[2961]\tvalidation-rmse:0.02747\n",
      "[2962]\tvalidation-rmse:0.02747\n",
      "[2963]\tvalidation-rmse:0.02747\n",
      "[2964]\tvalidation-rmse:0.02747\n",
      "[2965]\tvalidation-rmse:0.02747\n",
      "[2966]\tvalidation-rmse:0.02747\n",
      "[2967]\tvalidation-rmse:0.02747\n",
      "[2968]\tvalidation-rmse:0.02747\n",
      "[2969]\tvalidation-rmse:0.02747\n",
      "[2970]\tvalidation-rmse:0.02747\n",
      "[2971]\tvalidation-rmse:0.02747\n",
      "[2972]\tvalidation-rmse:0.02747\n",
      "[2973]\tvalidation-rmse:0.02747\n",
      "[2974]\tvalidation-rmse:0.02747\n",
      "[2975]\tvalidation-rmse:0.02747\n",
      "[2976]\tvalidation-rmse:0.02747\n",
      "[2977]\tvalidation-rmse:0.02747\n",
      "[2978]\tvalidation-rmse:0.02747\n",
      "[2979]\tvalidation-rmse:0.02747\n",
      "[2980]\tvalidation-rmse:0.02747\n",
      "[2981]\tvalidation-rmse:0.02747\n",
      "[2982]\tvalidation-rmse:0.02747\n",
      "[2983]\tvalidation-rmse:0.02747\n",
      "[2984]\tvalidation-rmse:0.02747\n",
      "[2985]\tvalidation-rmse:0.02747\n",
      "[2986]\tvalidation-rmse:0.02747\n",
      "[2987]\tvalidation-rmse:0.02747\n",
      "[2988]\tvalidation-rmse:0.02747\n",
      "[2989]\tvalidation-rmse:0.02747\n",
      "[2990]\tvalidation-rmse:0.02747\n",
      "[2991]\tvalidation-rmse:0.02747\n",
      "[2992]\tvalidation-rmse:0.02747\n",
      "[2993]\tvalidation-rmse:0.02747\n",
      "[2994]\tvalidation-rmse:0.02747\n",
      "[2995]\tvalidation-rmse:0.02747\n",
      "[2996]\tvalidation-rmse:0.02747\n",
      "[2997]\tvalidation-rmse:0.02747\n",
      "[2998]\tvalidation-rmse:0.02747\n",
      "[2999]\tvalidation-rmse:0.02747\n",
      "[3000]\tvalidation-rmse:0.02747\n",
      "[3001]\tvalidation-rmse:0.02747\n",
      "[3002]\tvalidation-rmse:0.02747\n",
      "[3003]\tvalidation-rmse:0.02747\n",
      "[3004]\tvalidation-rmse:0.02747\n",
      "[3005]\tvalidation-rmse:0.02747\n",
      "[3006]\tvalidation-rmse:0.02747\n",
      "[3007]\tvalidation-rmse:0.02747\n",
      "[3008]\tvalidation-rmse:0.02747\n",
      "[3009]\tvalidation-rmse:0.02747\n",
      "[3010]\tvalidation-rmse:0.02747\n",
      "[3011]\tvalidation-rmse:0.02747\n",
      "[3012]\tvalidation-rmse:0.02747\n",
      "[3013]\tvalidation-rmse:0.02747\n",
      "[3014]\tvalidation-rmse:0.02747\n",
      "[3015]\tvalidation-rmse:0.02747\n",
      "[3016]\tvalidation-rmse:0.02747\n",
      "[3017]\tvalidation-rmse:0.02747\n",
      "[3018]\tvalidation-rmse:0.02747\n",
      "[3019]\tvalidation-rmse:0.02747\n",
      "[3020]\tvalidation-rmse:0.02747\n",
      "[3021]\tvalidation-rmse:0.02747\n",
      "[3022]\tvalidation-rmse:0.02747\n",
      "[3023]\tvalidation-rmse:0.02747\n",
      "[3024]\tvalidation-rmse:0.02747\n",
      "[3025]\tvalidation-rmse:0.02747\n",
      "[3026]\tvalidation-rmse:0.02747\n",
      "[3027]\tvalidation-rmse:0.02747\n",
      "[3028]\tvalidation-rmse:0.02747\n",
      "[3029]\tvalidation-rmse:0.02747\n",
      "[3030]\tvalidation-rmse:0.02747\n",
      "[3031]\tvalidation-rmse:0.02747\n",
      "[3032]\tvalidation-rmse:0.02747\n",
      "[3033]\tvalidation-rmse:0.02747\n",
      "[3034]\tvalidation-rmse:0.02747\n",
      "[3035]\tvalidation-rmse:0.02747\n",
      "[3036]\tvalidation-rmse:0.02747\n",
      "[3037]\tvalidation-rmse:0.02747\n",
      "[3038]\tvalidation-rmse:0.02747\n",
      "[3039]\tvalidation-rmse:0.02747\n",
      "[3040]\tvalidation-rmse:0.02747\n",
      "[3041]\tvalidation-rmse:0.02747\n",
      "[3042]\tvalidation-rmse:0.02747\n",
      "[3043]\tvalidation-rmse:0.02747\n",
      "[3044]\tvalidation-rmse:0.02747\n",
      "[3045]\tvalidation-rmse:0.02746\n",
      "[3046]\tvalidation-rmse:0.02746\n",
      "[3047]\tvalidation-rmse:0.02746\n",
      "[3048]\tvalidation-rmse:0.02747\n",
      "[3049]\tvalidation-rmse:0.02747\n",
      "[3050]\tvalidation-rmse:0.02747\n",
      "[3051]\tvalidation-rmse:0.02747\n",
      "[3052]\tvalidation-rmse:0.02747\n",
      "[3053]\tvalidation-rmse:0.02746\n",
      "[3054]\tvalidation-rmse:0.02746\n",
      "[3055]\tvalidation-rmse:0.02746\n",
      "[3056]\tvalidation-rmse:0.02746\n",
      "[3057]\tvalidation-rmse:0.02746\n",
      "[3058]\tvalidation-rmse:0.02746\n",
      "[3059]\tvalidation-rmse:0.02746\n",
      "[3060]\tvalidation-rmse:0.02746\n",
      "[3061]\tvalidation-rmse:0.02746\n",
      "[3062]\tvalidation-rmse:0.02746\n",
      "[3063]\tvalidation-rmse:0.02746\n",
      "[3064]\tvalidation-rmse:0.02746\n",
      "[3065]\tvalidation-rmse:0.02746\n",
      "[3066]\tvalidation-rmse:0.02746\n",
      "[3067]\tvalidation-rmse:0.02746\n",
      "[3068]\tvalidation-rmse:0.02746\n",
      "[3069]\tvalidation-rmse:0.02746\n",
      "[3070]\tvalidation-rmse:0.02746\n",
      "[3071]\tvalidation-rmse:0.02746\n",
      "[3072]\tvalidation-rmse:0.02746\n",
      "[3073]\tvalidation-rmse:0.02746\n",
      "[3074]\tvalidation-rmse:0.02746\n",
      "[3075]\tvalidation-rmse:0.02746\n",
      "[3076]\tvalidation-rmse:0.02746\n",
      "[3077]\tvalidation-rmse:0.02746\n",
      "[3078]\tvalidation-rmse:0.02746\n",
      "[3079]\tvalidation-rmse:0.02746\n",
      "[3080]\tvalidation-rmse:0.02746\n",
      "[3081]\tvalidation-rmse:0.02746\n",
      "[3082]\tvalidation-rmse:0.02746\n",
      "[3083]\tvalidation-rmse:0.02746\n",
      "[3084]\tvalidation-rmse:0.02746\n",
      "[3085]\tvalidation-rmse:0.02746\n",
      "[3086]\tvalidation-rmse:0.02746\n",
      "[3087]\tvalidation-rmse:0.02746\n",
      "[3088]\tvalidation-rmse:0.02746\n",
      "[3089]\tvalidation-rmse:0.02745\n",
      "[3090]\tvalidation-rmse:0.02745\n",
      "[3091]\tvalidation-rmse:0.02745\n",
      "[3092]\tvalidation-rmse:0.02745\n",
      "[3093]\tvalidation-rmse:0.02745\n",
      "[3094]\tvalidation-rmse:0.02745\n",
      "[3095]\tvalidation-rmse:0.02745\n",
      "[3096]\tvalidation-rmse:0.02745\n",
      "[3097]\tvalidation-rmse:0.02745\n",
      "[3098]\tvalidation-rmse:0.02745\n",
      "[3099]\tvalidation-rmse:0.02745\n",
      "[3100]\tvalidation-rmse:0.02745\n",
      "[3101]\tvalidation-rmse:0.02745\n",
      "[3102]\tvalidation-rmse:0.02745\n",
      "[3103]\tvalidation-rmse:0.02745\n",
      "[3104]\tvalidation-rmse:0.02745\n",
      "[3105]\tvalidation-rmse:0.02745\n",
      "[3106]\tvalidation-rmse:0.02745\n",
      "[3107]\tvalidation-rmse:0.02745\n",
      "[3108]\tvalidation-rmse:0.02745\n",
      "[3109]\tvalidation-rmse:0.02745\n",
      "[3110]\tvalidation-rmse:0.02745\n",
      "[3111]\tvalidation-rmse:0.02745\n",
      "[3112]\tvalidation-rmse:0.02745\n",
      "[3113]\tvalidation-rmse:0.02745\n",
      "[3114]\tvalidation-rmse:0.02745\n",
      "[3115]\tvalidation-rmse:0.02745\n",
      "[3116]\tvalidation-rmse:0.02745\n",
      "[3117]\tvalidation-rmse:0.02745\n",
      "[3118]\tvalidation-rmse:0.02745\n",
      "[3119]\tvalidation-rmse:0.02745\n",
      "[3120]\tvalidation-rmse:0.02745\n",
      "[3121]\tvalidation-rmse:0.02745\n",
      "[3122]\tvalidation-rmse:0.02745\n",
      "[3123]\tvalidation-rmse:0.02745\n",
      "[3124]\tvalidation-rmse:0.02745\n",
      "[3125]\tvalidation-rmse:0.02745\n",
      "[3126]\tvalidation-rmse:0.02745\n",
      "[3127]\tvalidation-rmse:0.02745\n",
      "[3128]\tvalidation-rmse:0.02745\n",
      "[3129]\tvalidation-rmse:0.02745\n",
      "[3130]\tvalidation-rmse:0.02745\n",
      "[3131]\tvalidation-rmse:0.02745\n",
      "[3132]\tvalidation-rmse:0.02745\n",
      "[3133]\tvalidation-rmse:0.02745\n",
      "[3134]\tvalidation-rmse:0.02745\n",
      "[3135]\tvalidation-rmse:0.02745\n",
      "[3136]\tvalidation-rmse:0.02745\n",
      "[3137]\tvalidation-rmse:0.02745\n",
      "[3138]\tvalidation-rmse:0.02745\n",
      "[3139]\tvalidation-rmse:0.02745\n",
      "[3140]\tvalidation-rmse:0.02745\n",
      "[3141]\tvalidation-rmse:0.02745\n",
      "[3142]\tvalidation-rmse:0.02744\n",
      "[3143]\tvalidation-rmse:0.02744\n",
      "[3144]\tvalidation-rmse:0.02744\n",
      "[3145]\tvalidation-rmse:0.02744\n",
      "[3146]\tvalidation-rmse:0.02744\n",
      "[3147]\tvalidation-rmse:0.02744\n",
      "[3148]\tvalidation-rmse:0.02744\n",
      "[3149]\tvalidation-rmse:0.02744\n",
      "[3150]\tvalidation-rmse:0.02744\n",
      "[3151]\tvalidation-rmse:0.02744\n",
      "[3152]\tvalidation-rmse:0.02744\n",
      "[3153]\tvalidation-rmse:0.02744\n",
      "[3154]\tvalidation-rmse:0.02744\n",
      "[3155]\tvalidation-rmse:0.02744\n",
      "[3156]\tvalidation-rmse:0.02744\n",
      "[3157]\tvalidation-rmse:0.02744\n",
      "[3158]\tvalidation-rmse:0.02744\n",
      "[3159]\tvalidation-rmse:0.02744\n",
      "[3160]\tvalidation-rmse:0.02744\n",
      "[3161]\tvalidation-rmse:0.02744\n",
      "[3162]\tvalidation-rmse:0.02744\n",
      "[3163]\tvalidation-rmse:0.02744\n",
      "[3164]\tvalidation-rmse:0.02744\n",
      "[3165]\tvalidation-rmse:0.02744\n",
      "[3166]\tvalidation-rmse:0.02744\n",
      "[3167]\tvalidation-rmse:0.02744\n",
      "[3168]\tvalidation-rmse:0.02744\n",
      "[3169]\tvalidation-rmse:0.02744\n",
      "[3170]\tvalidation-rmse:0.02743\n",
      "[3171]\tvalidation-rmse:0.02743\n",
      "[3172]\tvalidation-rmse:0.02743\n",
      "[3173]\tvalidation-rmse:0.02743\n",
      "[3174]\tvalidation-rmse:0.02743\n",
      "[3175]\tvalidation-rmse:0.02743\n",
      "[3176]\tvalidation-rmse:0.02743\n",
      "[3177]\tvalidation-rmse:0.02743\n",
      "[3178]\tvalidation-rmse:0.02743\n",
      "[3179]\tvalidation-rmse:0.02743\n",
      "[3180]\tvalidation-rmse:0.02743\n",
      "[3181]\tvalidation-rmse:0.02743\n",
      "[3182]\tvalidation-rmse:0.02743\n",
      "[3183]\tvalidation-rmse:0.02743\n",
      "[3184]\tvalidation-rmse:0.02743\n",
      "[3185]\tvalidation-rmse:0.02743\n",
      "[3186]\tvalidation-rmse:0.02743\n",
      "[3187]\tvalidation-rmse:0.02743\n",
      "[3188]\tvalidation-rmse:0.02743\n",
      "[3189]\tvalidation-rmse:0.02743\n",
      "[3190]\tvalidation-rmse:0.02743\n",
      "[3191]\tvalidation-rmse:0.02743\n",
      "[3192]\tvalidation-rmse:0.02743\n",
      "[3193]\tvalidation-rmse:0.02743\n",
      "[3194]\tvalidation-rmse:0.02743\n",
      "[3195]\tvalidation-rmse:0.02743\n",
      "[3196]\tvalidation-rmse:0.02744\n",
      "[3197]\tvalidation-rmse:0.02744\n",
      "[3198]\tvalidation-rmse:0.02744\n",
      "[3199]\tvalidation-rmse:0.02744\n",
      "[3200]\tvalidation-rmse:0.02744\n",
      "[3201]\tvalidation-rmse:0.02744\n",
      "[3202]\tvalidation-rmse:0.02744\n",
      "[3203]\tvalidation-rmse:0.02744\n",
      "[3204]\tvalidation-rmse:0.02744\n",
      "[3205]\tvalidation-rmse:0.02744\n",
      "[3206]\tvalidation-rmse:0.02744\n",
      "[3207]\tvalidation-rmse:0.02744\n",
      "[3208]\tvalidation-rmse:0.02744\n",
      "[3209]\tvalidation-rmse:0.02744\n",
      "[3210]\tvalidation-rmse:0.02744\n",
      "[3211]\tvalidation-rmse:0.02744\n",
      "[3212]\tvalidation-rmse:0.02744\n",
      "[3213]\tvalidation-rmse:0.02744\n",
      "[3214]\tvalidation-rmse:0.02743\n",
      "[3215]\tvalidation-rmse:0.02743\n",
      "[3216]\tvalidation-rmse:0.02743\n",
      "[3217]\tvalidation-rmse:0.02744\n",
      "[3218]\tvalidation-rmse:0.02743\n",
      "[3219]\tvalidation-rmse:0.02743\n",
      "[3220]\tvalidation-rmse:0.02743\n",
      "[3221]\tvalidation-rmse:0.02743\n",
      "[3222]\tvalidation-rmse:0.02743\n",
      "[3223]\tvalidation-rmse:0.02743\n",
      "[3224]\tvalidation-rmse:0.02743\n",
      "[3225]\tvalidation-rmse:0.02743\n",
      "[3226]\tvalidation-rmse:0.02743\n",
      "[3227]\tvalidation-rmse:0.02743\n",
      "[3228]\tvalidation-rmse:0.02743\n",
      "[3229]\tvalidation-rmse:0.02743\n",
      "[3230]\tvalidation-rmse:0.02743\n",
      "[3231]\tvalidation-rmse:0.02743\n",
      "[3232]\tvalidation-rmse:0.02743\n",
      "[3233]\tvalidation-rmse:0.02743\n",
      "[3234]\tvalidation-rmse:0.02743\n",
      "[3235]\tvalidation-rmse:0.02743\n",
      "[3236]\tvalidation-rmse:0.02743\n",
      "[3237]\tvalidation-rmse:0.02743\n",
      "[3238]\tvalidation-rmse:0.02743\n",
      "[3239]\tvalidation-rmse:0.02743\n",
      "[3240]\tvalidation-rmse:0.02743\n",
      "[3241]\tvalidation-rmse:0.02743\n",
      "[3242]\tvalidation-rmse:0.02743\n",
      "[3243]\tvalidation-rmse:0.02743\n",
      "[3244]\tvalidation-rmse:0.02743\n",
      "[3245]\tvalidation-rmse:0.02743\n",
      "[3246]\tvalidation-rmse:0.02743\n",
      "[3247]\tvalidation-rmse:0.02743\n",
      "[3248]\tvalidation-rmse:0.02743\n",
      "[3249]\tvalidation-rmse:0.02743\n",
      "[3250]\tvalidation-rmse:0.02743\n",
      "[3251]\tvalidation-rmse:0.02743\n",
      "[3252]\tvalidation-rmse:0.02743\n",
      "[3253]\tvalidation-rmse:0.02743\n",
      "[3254]\tvalidation-rmse:0.02743\n",
      "[3255]\tvalidation-rmse:0.02743\n",
      "[3256]\tvalidation-rmse:0.02743\n",
      "[3257]\tvalidation-rmse:0.02743\n",
      "[3258]\tvalidation-rmse:0.02743\n",
      "[3259]\tvalidation-rmse:0.02743\n",
      "[3260]\tvalidation-rmse:0.02743\n",
      "[3261]\tvalidation-rmse:0.02743\n",
      "[3262]\tvalidation-rmse:0.02743\n",
      "[3263]\tvalidation-rmse:0.02743\n",
      "[3264]\tvalidation-rmse:0.02743\n",
      "[3265]\tvalidation-rmse:0.02743\n",
      "[3266]\tvalidation-rmse:0.02743\n",
      "[3267]\tvalidation-rmse:0.02743\n",
      "[3268]\tvalidation-rmse:0.02743\n",
      "[3269]\tvalidation-rmse:0.02743\n",
      "[3270]\tvalidation-rmse:0.02743\n",
      "[3271]\tvalidation-rmse:0.02743\n",
      "[3272]\tvalidation-rmse:0.02743\n",
      "[3273]\tvalidation-rmse:0.02743\n",
      "[3274]\tvalidation-rmse:0.02743\n",
      "[3275]\tvalidation-rmse:0.02743\n",
      "[3276]\tvalidation-rmse:0.02743\n",
      "[3277]\tvalidation-rmse:0.02743\n",
      "[3278]\tvalidation-rmse:0.02743\n",
      "[3279]\tvalidation-rmse:0.02743\n",
      "[3280]\tvalidation-rmse:0.02743\n",
      "[3281]\tvalidation-rmse:0.02743\n",
      "[3282]\tvalidation-rmse:0.02743\n",
      "[3283]\tvalidation-rmse:0.02743\n",
      "[3284]\tvalidation-rmse:0.02743\n",
      "[3285]\tvalidation-rmse:0.02743\n",
      "[3286]\tvalidation-rmse:0.02743\n",
      "[3287]\tvalidation-rmse:0.02743\n",
      "[3288]\tvalidation-rmse:0.02743\n",
      "[3289]\tvalidation-rmse:0.02743\n",
      "[3290]\tvalidation-rmse:0.02743\n",
      "[3291]\tvalidation-rmse:0.02743\n",
      "[3292]\tvalidation-rmse:0.02743\n",
      "[3293]\tvalidation-rmse:0.02743\n",
      "[3294]\tvalidation-rmse:0.02743\n",
      "[3295]\tvalidation-rmse:0.02743\n",
      "[3296]\tvalidation-rmse:0.02743\n",
      "[3297]\tvalidation-rmse:0.02743\n",
      "[3298]\tvalidation-rmse:0.02743\n",
      "[3299]\tvalidation-rmse:0.02743\n",
      "[3300]\tvalidation-rmse:0.02743\n",
      "[3301]\tvalidation-rmse:0.02743\n",
      "[3302]\tvalidation-rmse:0.02743\n",
      "[3303]\tvalidation-rmse:0.02743\n",
      "[3304]\tvalidation-rmse:0.02743\n",
      "[3305]\tvalidation-rmse:0.02743\n",
      "[3306]\tvalidation-rmse:0.02742\n",
      "[3307]\tvalidation-rmse:0.02742\n",
      "[3308]\tvalidation-rmse:0.02742\n",
      "[3309]\tvalidation-rmse:0.02742\n",
      "[3310]\tvalidation-rmse:0.02742\n",
      "[3311]\tvalidation-rmse:0.02742\n",
      "[3312]\tvalidation-rmse:0.02742\n",
      "[3313]\tvalidation-rmse:0.02742\n",
      "[3314]\tvalidation-rmse:0.02742\n",
      "[3315]\tvalidation-rmse:0.02742\n",
      "[3316]\tvalidation-rmse:0.02742\n",
      "[3317]\tvalidation-rmse:0.02742\n",
      "[3318]\tvalidation-rmse:0.02742\n",
      "[3319]\tvalidation-rmse:0.02742\n",
      "[3320]\tvalidation-rmse:0.02742\n",
      "[3321]\tvalidation-rmse:0.02742\n",
      "[3322]\tvalidation-rmse:0.02742\n",
      "[3323]\tvalidation-rmse:0.02742\n",
      "[3324]\tvalidation-rmse:0.02742\n",
      "[3325]\tvalidation-rmse:0.02742\n",
      "[3326]\tvalidation-rmse:0.02742\n",
      "[3327]\tvalidation-rmse:0.02742\n",
      "[3328]\tvalidation-rmse:0.02742\n",
      "[3329]\tvalidation-rmse:0.02742\n",
      "[3330]\tvalidation-rmse:0.02742\n",
      "[3331]\tvalidation-rmse:0.02742\n",
      "[3332]\tvalidation-rmse:0.02742\n",
      "[3333]\tvalidation-rmse:0.02742\n",
      "[3334]\tvalidation-rmse:0.02742\n",
      "[3335]\tvalidation-rmse:0.02742\n",
      "[3336]\tvalidation-rmse:0.02742\n",
      "[3337]\tvalidation-rmse:0.02742\n",
      "[3338]\tvalidation-rmse:0.02742\n",
      "[3339]\tvalidation-rmse:0.02742\n",
      "[3340]\tvalidation-rmse:0.02742\n",
      "[3341]\tvalidation-rmse:0.02742\n",
      "[3342]\tvalidation-rmse:0.02742\n",
      "[3343]\tvalidation-rmse:0.02742\n",
      "[3344]\tvalidation-rmse:0.02742\n",
      "[3345]\tvalidation-rmse:0.02742\n",
      "[3346]\tvalidation-rmse:0.02742\n",
      "[3347]\tvalidation-rmse:0.02742\n",
      "[3348]\tvalidation-rmse:0.02742\n",
      "[3349]\tvalidation-rmse:0.02742\n",
      "[3350]\tvalidation-rmse:0.02742\n",
      "[3351]\tvalidation-rmse:0.02742\n",
      "[3352]\tvalidation-rmse:0.02742\n",
      "[3353]\tvalidation-rmse:0.02742\n",
      "[3354]\tvalidation-rmse:0.02742\n",
      "[3355]\tvalidation-rmse:0.02742\n",
      "[3356]\tvalidation-rmse:0.02742\n",
      "[3357]\tvalidation-rmse:0.02741\n",
      "[3358]\tvalidation-rmse:0.02741\n",
      "[3359]\tvalidation-rmse:0.02741\n",
      "[3360]\tvalidation-rmse:0.02741\n",
      "[3361]\tvalidation-rmse:0.02741\n",
      "[3362]\tvalidation-rmse:0.02741\n",
      "[3363]\tvalidation-rmse:0.02741\n",
      "[3364]\tvalidation-rmse:0.02741\n",
      "[3365]\tvalidation-rmse:0.02741\n",
      "[3366]\tvalidation-rmse:0.02741\n",
      "[3367]\tvalidation-rmse:0.02741\n",
      "[3368]\tvalidation-rmse:0.02741\n",
      "[3369]\tvalidation-rmse:0.02741\n",
      "[3370]\tvalidation-rmse:0.02741\n",
      "[3371]\tvalidation-rmse:0.02741\n",
      "[3372]\tvalidation-rmse:0.02741\n",
      "[3373]\tvalidation-rmse:0.02741\n",
      "[3374]\tvalidation-rmse:0.02741\n",
      "[3375]\tvalidation-rmse:0.02741\n",
      "[3376]\tvalidation-rmse:0.02741\n",
      "[3377]\tvalidation-rmse:0.02741\n",
      "[3378]\tvalidation-rmse:0.02741\n",
      "[3379]\tvalidation-rmse:0.02741\n",
      "[3380]\tvalidation-rmse:0.02741\n",
      "[3381]\tvalidation-rmse:0.02741\n",
      "[3382]\tvalidation-rmse:0.02741\n",
      "[3383]\tvalidation-rmse:0.02741\n",
      "[3384]\tvalidation-rmse:0.02741\n",
      "[3385]\tvalidation-rmse:0.02741\n",
      "[3386]\tvalidation-rmse:0.02741\n",
      "[3387]\tvalidation-rmse:0.02741\n",
      "[3388]\tvalidation-rmse:0.02741\n",
      "[3389]\tvalidation-rmse:0.02741\n",
      "[3390]\tvalidation-rmse:0.02741\n",
      "[3391]\tvalidation-rmse:0.02741\n",
      "[3392]\tvalidation-rmse:0.02741\n",
      "[3393]\tvalidation-rmse:0.02741\n",
      "[3394]\tvalidation-rmse:0.02741\n",
      "[3395]\tvalidation-rmse:0.02741\n",
      "[3396]\tvalidation-rmse:0.02741\n",
      "[3397]\tvalidation-rmse:0.02741\n",
      "[3398]\tvalidation-rmse:0.02741\n",
      "[3399]\tvalidation-rmse:0.02741\n",
      "[3400]\tvalidation-rmse:0.02741\n",
      "[3401]\tvalidation-rmse:0.02741\n",
      "[3402]\tvalidation-rmse:0.02741\n",
      "[3403]\tvalidation-rmse:0.02741\n",
      "[3404]\tvalidation-rmse:0.02741\n",
      "[3405]\tvalidation-rmse:0.02741\n",
      "[3406]\tvalidation-rmse:0.02741\n",
      "[3407]\tvalidation-rmse:0.02741\n",
      "[3408]\tvalidation-rmse:0.02741\n",
      "[3409]\tvalidation-rmse:0.02741\n",
      "[3410]\tvalidation-rmse:0.02741\n",
      "[3411]\tvalidation-rmse:0.02741\n",
      "[3412]\tvalidation-rmse:0.02741\n",
      "[3413]\tvalidation-rmse:0.02741\n",
      "[3414]\tvalidation-rmse:0.02741\n",
      "[3415]\tvalidation-rmse:0.02741\n",
      "[3416]\tvalidation-rmse:0.02741\n",
      "[3417]\tvalidation-rmse:0.02741\n",
      "[3418]\tvalidation-rmse:0.02741\n",
      "[3419]\tvalidation-rmse:0.02741\n",
      "[3420]\tvalidation-rmse:0.02741\n",
      "[3421]\tvalidation-rmse:0.02741\n",
      "[3422]\tvalidation-rmse:0.02741\n",
      "[3423]\tvalidation-rmse:0.02741\n",
      "[3424]\tvalidation-rmse:0.02741\n",
      "[3425]\tvalidation-rmse:0.02741\n",
      "[3426]\tvalidation-rmse:0.02741\n",
      "[3427]\tvalidation-rmse:0.02741\n",
      "[3428]\tvalidation-rmse:0.02741\n",
      "[3429]\tvalidation-rmse:0.02741\n",
      "[3430]\tvalidation-rmse:0.02740\n",
      "[3431]\tvalidation-rmse:0.02740\n",
      "[3432]\tvalidation-rmse:0.02740\n",
      "[3433]\tvalidation-rmse:0.02740\n",
      "[3434]\tvalidation-rmse:0.02740\n",
      "[3435]\tvalidation-rmse:0.02740\n",
      "[3436]\tvalidation-rmse:0.02741\n",
      "[3437]\tvalidation-rmse:0.02740\n",
      "[3438]\tvalidation-rmse:0.02740\n",
      "[3439]\tvalidation-rmse:0.02740\n",
      "[3440]\tvalidation-rmse:0.02740\n",
      "[3441]\tvalidation-rmse:0.02740\n",
      "[3442]\tvalidation-rmse:0.02740\n",
      "[3443]\tvalidation-rmse:0.02740\n",
      "[3444]\tvalidation-rmse:0.02740\n",
      "[3445]\tvalidation-rmse:0.02740\n",
      "[3446]\tvalidation-rmse:0.02740\n",
      "[3447]\tvalidation-rmse:0.02740\n",
      "[3448]\tvalidation-rmse:0.02740\n",
      "[3449]\tvalidation-rmse:0.02740\n",
      "[3450]\tvalidation-rmse:0.02740\n",
      "[3451]\tvalidation-rmse:0.02740\n",
      "[3452]\tvalidation-rmse:0.02740\n",
      "[3453]\tvalidation-rmse:0.02740\n",
      "[3454]\tvalidation-rmse:0.02740\n",
      "[3455]\tvalidation-rmse:0.02740\n",
      "[3456]\tvalidation-rmse:0.02740\n",
      "[3457]\tvalidation-rmse:0.02740\n",
      "[3458]\tvalidation-rmse:0.02740\n",
      "[3459]\tvalidation-rmse:0.02740\n",
      "[3460]\tvalidation-rmse:0.02740\n",
      "[3461]\tvalidation-rmse:0.02740\n",
      "[3462]\tvalidation-rmse:0.02740\n",
      "[3463]\tvalidation-rmse:0.02740\n",
      "[3464]\tvalidation-rmse:0.02740\n",
      "[3465]\tvalidation-rmse:0.02740\n",
      "[3466]\tvalidation-rmse:0.02740\n",
      "[3467]\tvalidation-rmse:0.02740\n",
      "[3468]\tvalidation-rmse:0.02740\n",
      "[3469]\tvalidation-rmse:0.02740\n",
      "[3470]\tvalidation-rmse:0.02740\n",
      "[3471]\tvalidation-rmse:0.02740\n",
      "[3472]\tvalidation-rmse:0.02740\n",
      "[3473]\tvalidation-rmse:0.02740\n",
      "[3474]\tvalidation-rmse:0.02740\n",
      "[3475]\tvalidation-rmse:0.02740\n",
      "[3476]\tvalidation-rmse:0.02740\n",
      "[3477]\tvalidation-rmse:0.02740\n",
      "[3478]\tvalidation-rmse:0.02740\n",
      "[3479]\tvalidation-rmse:0.02740\n",
      "[3480]\tvalidation-rmse:0.02740\n",
      "[3481]\tvalidation-rmse:0.02740\n",
      "[3482]\tvalidation-rmse:0.02740\n",
      "[3483]\tvalidation-rmse:0.02740\n",
      "[3484]\tvalidation-rmse:0.02740\n",
      "[3485]\tvalidation-rmse:0.02740\n",
      "[3486]\tvalidation-rmse:0.02740\n",
      "[3487]\tvalidation-rmse:0.02740\n",
      "[3488]\tvalidation-rmse:0.02740\n",
      "[3489]\tvalidation-rmse:0.02740\n",
      "[3490]\tvalidation-rmse:0.02740\n",
      "[3491]\tvalidation-rmse:0.02740\n",
      "[3492]\tvalidation-rmse:0.02740\n",
      "[3493]\tvalidation-rmse:0.02740\n",
      "[3494]\tvalidation-rmse:0.02740\n",
      "[3495]\tvalidation-rmse:0.02740\n",
      "[3496]\tvalidation-rmse:0.02740\n",
      "[3497]\tvalidation-rmse:0.02740\n",
      "[3498]\tvalidation-rmse:0.02740\n",
      "[3499]\tvalidation-rmse:0.02740\n",
      "[3500]\tvalidation-rmse:0.02740\n",
      "[3501]\tvalidation-rmse:0.02740\n",
      "[3502]\tvalidation-rmse:0.02739\n",
      "[3503]\tvalidation-rmse:0.02739\n",
      "[3504]\tvalidation-rmse:0.02739\n",
      "[3505]\tvalidation-rmse:0.02739\n",
      "[3506]\tvalidation-rmse:0.02739\n",
      "[3507]\tvalidation-rmse:0.02739\n",
      "[3508]\tvalidation-rmse:0.02739\n",
      "[3509]\tvalidation-rmse:0.02739\n",
      "[3510]\tvalidation-rmse:0.02739\n",
      "[3511]\tvalidation-rmse:0.02739\n",
      "[3512]\tvalidation-rmse:0.02739\n",
      "[3513]\tvalidation-rmse:0.02739\n",
      "[3514]\tvalidation-rmse:0.02739\n",
      "[3515]\tvalidation-rmse:0.02739\n",
      "[3516]\tvalidation-rmse:0.02739\n",
      "[3517]\tvalidation-rmse:0.02739\n",
      "[3518]\tvalidation-rmse:0.02739\n",
      "[3519]\tvalidation-rmse:0.02739\n",
      "[3520]\tvalidation-rmse:0.02739\n",
      "[3521]\tvalidation-rmse:0.02739\n",
      "[3522]\tvalidation-rmse:0.02739\n",
      "[3523]\tvalidation-rmse:0.02739\n",
      "[3524]\tvalidation-rmse:0.02739\n",
      "[3525]\tvalidation-rmse:0.02739\n",
      "[3526]\tvalidation-rmse:0.02739\n",
      "[3527]\tvalidation-rmse:0.02739\n",
      "[3528]\tvalidation-rmse:0.02739\n",
      "[3529]\tvalidation-rmse:0.02739\n",
      "[3530]\tvalidation-rmse:0.02739\n",
      "[3531]\tvalidation-rmse:0.02739\n",
      "[3532]\tvalidation-rmse:0.02739\n",
      "[3533]\tvalidation-rmse:0.02739\n",
      "[3534]\tvalidation-rmse:0.02739\n",
      "[3535]\tvalidation-rmse:0.02739\n",
      "[3536]\tvalidation-rmse:0.02739\n",
      "[3537]\tvalidation-rmse:0.02739\n",
      "[3538]\tvalidation-rmse:0.02739\n",
      "[3539]\tvalidation-rmse:0.02739\n",
      "[3540]\tvalidation-rmse:0.02739\n",
      "[3541]\tvalidation-rmse:0.02739\n",
      "[3542]\tvalidation-rmse:0.02739\n",
      "[3543]\tvalidation-rmse:0.02739\n",
      "[3544]\tvalidation-rmse:0.02739\n",
      "[3545]\tvalidation-rmse:0.02739\n",
      "[3546]\tvalidation-rmse:0.02739\n",
      "[3547]\tvalidation-rmse:0.02739\n",
      "[3548]\tvalidation-rmse:0.02738\n",
      "[3549]\tvalidation-rmse:0.02739\n",
      "[3550]\tvalidation-rmse:0.02738\n",
      "[3551]\tvalidation-rmse:0.02738\n",
      "[3552]\tvalidation-rmse:0.02738\n",
      "[3553]\tvalidation-rmse:0.02738\n",
      "[3554]\tvalidation-rmse:0.02738\n",
      "[3555]\tvalidation-rmse:0.02738\n",
      "[3556]\tvalidation-rmse:0.02738\n",
      "[3557]\tvalidation-rmse:0.02738\n",
      "[3558]\tvalidation-rmse:0.02738\n",
      "[3559]\tvalidation-rmse:0.02738\n",
      "[3560]\tvalidation-rmse:0.02738\n",
      "[3561]\tvalidation-rmse:0.02738\n",
      "[3562]\tvalidation-rmse:0.02738\n",
      "[3563]\tvalidation-rmse:0.02738\n",
      "[3564]\tvalidation-rmse:0.02738\n",
      "[3565]\tvalidation-rmse:0.02738\n",
      "[3566]\tvalidation-rmse:0.02738\n",
      "[3567]\tvalidation-rmse:0.02738\n",
      "[3568]\tvalidation-rmse:0.02738\n",
      "[3569]\tvalidation-rmse:0.02738\n",
      "[3570]\tvalidation-rmse:0.02738\n",
      "[3571]\tvalidation-rmse:0.02738\n",
      "[3572]\tvalidation-rmse:0.02738\n",
      "[3573]\tvalidation-rmse:0.02738\n",
      "[3574]\tvalidation-rmse:0.02738\n",
      "[3575]\tvalidation-rmse:0.02738\n",
      "[3576]\tvalidation-rmse:0.02738\n",
      "[3577]\tvalidation-rmse:0.02738\n",
      "[3578]\tvalidation-rmse:0.02738\n",
      "[3579]\tvalidation-rmse:0.02738\n",
      "[3580]\tvalidation-rmse:0.02738\n",
      "[3581]\tvalidation-rmse:0.02738\n",
      "[3582]\tvalidation-rmse:0.02738\n",
      "[3583]\tvalidation-rmse:0.02738\n",
      "[3584]\tvalidation-rmse:0.02738\n",
      "[3585]\tvalidation-rmse:0.02738\n",
      "[3586]\tvalidation-rmse:0.02738\n",
      "[3587]\tvalidation-rmse:0.02738\n",
      "[3588]\tvalidation-rmse:0.02738\n",
      "[3589]\tvalidation-rmse:0.02738\n",
      "[3590]\tvalidation-rmse:0.02738\n",
      "[3591]\tvalidation-rmse:0.02738\n",
      "[3592]\tvalidation-rmse:0.02738\n",
      "[3593]\tvalidation-rmse:0.02738\n",
      "[3594]\tvalidation-rmse:0.02738\n",
      "[3595]\tvalidation-rmse:0.02738\n",
      "[3596]\tvalidation-rmse:0.02738\n",
      "[3597]\tvalidation-rmse:0.02738\n",
      "[3598]\tvalidation-rmse:0.02738\n",
      "[3599]\tvalidation-rmse:0.02738\n",
      "[3600]\tvalidation-rmse:0.02738\n",
      "[3601]\tvalidation-rmse:0.02738\n",
      "[3602]\tvalidation-rmse:0.02738\n",
      "[3603]\tvalidation-rmse:0.02738\n",
      "[3604]\tvalidation-rmse:0.02738\n",
      "[3605]\tvalidation-rmse:0.02738\n",
      "[3606]\tvalidation-rmse:0.02738\n",
      "[3607]\tvalidation-rmse:0.02738\n",
      "[3608]\tvalidation-rmse:0.02738\n",
      "[3609]\tvalidation-rmse:0.02738\n",
      "[3610]\tvalidation-rmse:0.02738\n",
      "[3611]\tvalidation-rmse:0.02738\n",
      "[3612]\tvalidation-rmse:0.02738\n",
      "[3613]\tvalidation-rmse:0.02738\n",
      "[3614]\tvalidation-rmse:0.02738\n",
      "[3615]\tvalidation-rmse:0.02738\n",
      "[3616]\tvalidation-rmse:0.02738\n",
      "[3617]\tvalidation-rmse:0.02738\n",
      "[3618]\tvalidation-rmse:0.02738\n",
      "[3619]\tvalidation-rmse:0.02738\n",
      "[3620]\tvalidation-rmse:0.02738\n",
      "[3621]\tvalidation-rmse:0.02738\n",
      "[3622]\tvalidation-rmse:0.02738\n",
      "[3623]\tvalidation-rmse:0.02738\n",
      "[3624]\tvalidation-rmse:0.02738\n",
      "[3625]\tvalidation-rmse:0.02738\n",
      "[3626]\tvalidation-rmse:0.02738\n",
      "[3627]\tvalidation-rmse:0.02738\n",
      "[3628]\tvalidation-rmse:0.02738\n",
      "[3629]\tvalidation-rmse:0.02738\n",
      "[3630]\tvalidation-rmse:0.02738\n",
      "[3631]\tvalidation-rmse:0.02738\n",
      "[3632]\tvalidation-rmse:0.02738\n",
      "[3633]\tvalidation-rmse:0.02738\n",
      "[3634]\tvalidation-rmse:0.02738\n",
      "[3635]\tvalidation-rmse:0.02738\n",
      "[3636]\tvalidation-rmse:0.02738\n",
      "[3637]\tvalidation-rmse:0.02738\n",
      "[3638]\tvalidation-rmse:0.02738\n",
      "[3639]\tvalidation-rmse:0.02738\n",
      "[3640]\tvalidation-rmse:0.02738\n",
      "[3641]\tvalidation-rmse:0.02738\n",
      "[3642]\tvalidation-rmse:0.02738\n",
      "[3643]\tvalidation-rmse:0.02738\n",
      "[3644]\tvalidation-rmse:0.02738\n",
      "[3645]\tvalidation-rmse:0.02738\n",
      "[3646]\tvalidation-rmse:0.02738\n",
      "[3647]\tvalidation-rmse:0.02738\n",
      "[3648]\tvalidation-rmse:0.02738\n",
      "[3649]\tvalidation-rmse:0.02738\n",
      "[3650]\tvalidation-rmse:0.02738\n",
      "[3651]\tvalidation-rmse:0.02738\n",
      "[3652]\tvalidation-rmse:0.02738\n",
      "[3653]\tvalidation-rmse:0.02738\n",
      "[3654]\tvalidation-rmse:0.02738\n",
      "[3655]\tvalidation-rmse:0.02738\n",
      "[3656]\tvalidation-rmse:0.02738\n",
      "[3657]\tvalidation-rmse:0.02738\n",
      "[3658]\tvalidation-rmse:0.02738\n",
      "[3659]\tvalidation-rmse:0.02738\n",
      "[3660]\tvalidation-rmse:0.02738\n",
      "[3661]\tvalidation-rmse:0.02738\n",
      "[3662]\tvalidation-rmse:0.02738\n",
      "[3663]\tvalidation-rmse:0.02737\n",
      "[3664]\tvalidation-rmse:0.02738\n",
      "[3665]\tvalidation-rmse:0.02738\n",
      "[3666]\tvalidation-rmse:0.02737\n",
      "[3667]\tvalidation-rmse:0.02737\n",
      "[3668]\tvalidation-rmse:0.02737\n",
      "[3669]\tvalidation-rmse:0.02737\n",
      "[3670]\tvalidation-rmse:0.02737\n",
      "[3671]\tvalidation-rmse:0.02737\n",
      "[3672]\tvalidation-rmse:0.02737\n",
      "[3673]\tvalidation-rmse:0.02737\n",
      "[3674]\tvalidation-rmse:0.02737\n",
      "[3675]\tvalidation-rmse:0.02737\n",
      "[3676]\tvalidation-rmse:0.02737\n",
      "[3677]\tvalidation-rmse:0.02737\n",
      "[3678]\tvalidation-rmse:0.02737\n",
      "[3679]\tvalidation-rmse:0.02737\n",
      "[3680]\tvalidation-rmse:0.02737\n",
      "[3681]\tvalidation-rmse:0.02737\n",
      "[3682]\tvalidation-rmse:0.02737\n",
      "[3683]\tvalidation-rmse:0.02737\n",
      "[3684]\tvalidation-rmse:0.02737\n",
      "[3685]\tvalidation-rmse:0.02737\n",
      "[3686]\tvalidation-rmse:0.02737\n",
      "[3687]\tvalidation-rmse:0.02737\n",
      "[3688]\tvalidation-rmse:0.02737\n",
      "[3689]\tvalidation-rmse:0.02737\n",
      "[3690]\tvalidation-rmse:0.02737\n",
      "[3691]\tvalidation-rmse:0.02737\n",
      "[3692]\tvalidation-rmse:0.02737\n",
      "[3693]\tvalidation-rmse:0.02737\n",
      "[3694]\tvalidation-rmse:0.02737\n",
      "[3695]\tvalidation-rmse:0.02737\n",
      "[3696]\tvalidation-rmse:0.02737\n",
      "[3697]\tvalidation-rmse:0.02737\n",
      "[3698]\tvalidation-rmse:0.02737\n",
      "[3699]\tvalidation-rmse:0.02737\n",
      "[3700]\tvalidation-rmse:0.02737\n",
      "[3701]\tvalidation-rmse:0.02737\n",
      "[3702]\tvalidation-rmse:0.02737\n",
      "[3703]\tvalidation-rmse:0.02737\n",
      "[3704]\tvalidation-rmse:0.02737\n",
      "[3705]\tvalidation-rmse:0.02737\n",
      "[3706]\tvalidation-rmse:0.02737\n",
      "[3707]\tvalidation-rmse:0.02736\n",
      "[3708]\tvalidation-rmse:0.02736\n",
      "[3709]\tvalidation-rmse:0.02736\n",
      "[3710]\tvalidation-rmse:0.02736\n",
      "[3711]\tvalidation-rmse:0.02736\n",
      "[3712]\tvalidation-rmse:0.02736\n",
      "[3713]\tvalidation-rmse:0.02736\n",
      "[3714]\tvalidation-rmse:0.02736\n",
      "[3715]\tvalidation-rmse:0.02736\n",
      "[3716]\tvalidation-rmse:0.02736\n",
      "[3717]\tvalidation-rmse:0.02736\n",
      "[3718]\tvalidation-rmse:0.02736\n",
      "[3719]\tvalidation-rmse:0.02736\n",
      "[3720]\tvalidation-rmse:0.02736\n",
      "[3721]\tvalidation-rmse:0.02736\n",
      "[3722]\tvalidation-rmse:0.02736\n",
      "[3723]\tvalidation-rmse:0.02736\n",
      "[3724]\tvalidation-rmse:0.02736\n",
      "[3725]\tvalidation-rmse:0.02736\n",
      "[3726]\tvalidation-rmse:0.02736\n",
      "[3727]\tvalidation-rmse:0.02737\n",
      "[3728]\tvalidation-rmse:0.02737\n",
      "[3729]\tvalidation-rmse:0.02737\n",
      "[3730]\tvalidation-rmse:0.02737\n",
      "[3731]\tvalidation-rmse:0.02737\n",
      "[3732]\tvalidation-rmse:0.02737\n",
      "[3733]\tvalidation-rmse:0.02737\n",
      "[3734]\tvalidation-rmse:0.02737\n",
      "[3735]\tvalidation-rmse:0.02737\n",
      "[3736]\tvalidation-rmse:0.02737\n",
      "[3737]\tvalidation-rmse:0.02737\n",
      "[3738]\tvalidation-rmse:0.02737\n",
      "[3739]\tvalidation-rmse:0.02737\n",
      "[3740]\tvalidation-rmse:0.02737\n",
      "[3741]\tvalidation-rmse:0.02737\n",
      "[3742]\tvalidation-rmse:0.02737\n",
      "[3743]\tvalidation-rmse:0.02737\n",
      "[3744]\tvalidation-rmse:0.02737\n",
      "[3745]\tvalidation-rmse:0.02737\n",
      "[3746]\tvalidation-rmse:0.02737\n",
      "[3747]\tvalidation-rmse:0.02737\n",
      "[3748]\tvalidation-rmse:0.02737\n",
      "[3749]\tvalidation-rmse:0.02737\n",
      "[3750]\tvalidation-rmse:0.02737\n",
      "[3751]\tvalidation-rmse:0.02736\n",
      "[3752]\tvalidation-rmse:0.02736\n",
      "[3753]\tvalidation-rmse:0.02736\n",
      "[3754]\tvalidation-rmse:0.02736\n",
      "[3755]\tvalidation-rmse:0.02736\n",
      "[3756]\tvalidation-rmse:0.02736\n",
      "[3757]\tvalidation-rmse:0.02736\n",
      "[3758]\tvalidation-rmse:0.02736\n",
      "[3759]\tvalidation-rmse:0.02736\n",
      "[3760]\tvalidation-rmse:0.02736\n",
      "[3761]\tvalidation-rmse:0.02736\n",
      "[3762]\tvalidation-rmse:0.02736\n",
      "[3763]\tvalidation-rmse:0.02736\n",
      "[3764]\tvalidation-rmse:0.02736\n",
      "[3765]\tvalidation-rmse:0.02736\n",
      "[3766]\tvalidation-rmse:0.02736\n",
      "[3767]\tvalidation-rmse:0.02736\n",
      "[3768]\tvalidation-rmse:0.02736\n",
      "[3769]\tvalidation-rmse:0.02736\n",
      "[3770]\tvalidation-rmse:0.02736\n",
      "[3771]\tvalidation-rmse:0.02736\n",
      "[3772]\tvalidation-rmse:0.02736\n",
      "[3773]\tvalidation-rmse:0.02736\n",
      "[3774]\tvalidation-rmse:0.02736\n",
      "[3775]\tvalidation-rmse:0.02736\n",
      "[3776]\tvalidation-rmse:0.02736\n",
      "[3777]\tvalidation-rmse:0.02736\n",
      "[3778]\tvalidation-rmse:0.02736\n",
      "[3779]\tvalidation-rmse:0.02736\n",
      "[3780]\tvalidation-rmse:0.02736\n",
      "[3781]\tvalidation-rmse:0.02736\n",
      "[3782]\tvalidation-rmse:0.02736\n",
      "[3783]\tvalidation-rmse:0.02736\n",
      "[3784]\tvalidation-rmse:0.02736\n",
      "[3785]\tvalidation-rmse:0.02736\n",
      "[3786]\tvalidation-rmse:0.02736\n",
      "[3787]\tvalidation-rmse:0.02736\n",
      "[3788]\tvalidation-rmse:0.02736\n",
      "[3789]\tvalidation-rmse:0.02736\n",
      "[3790]\tvalidation-rmse:0.02736\n",
      "[3791]\tvalidation-rmse:0.02736\n",
      "[3792]\tvalidation-rmse:0.02736\n",
      "[3793]\tvalidation-rmse:0.02736\n",
      "[3794]\tvalidation-rmse:0.02736\n",
      "[3795]\tvalidation-rmse:0.02736\n",
      "[3796]\tvalidation-rmse:0.02736\n",
      "[3797]\tvalidation-rmse:0.02736\n",
      "[3798]\tvalidation-rmse:0.02736\n",
      "[3799]\tvalidation-rmse:0.02736\n",
      "[3800]\tvalidation-rmse:0.02736\n",
      "[3801]\tvalidation-rmse:0.02736\n",
      "[3802]\tvalidation-rmse:0.02736\n",
      "[3803]\tvalidation-rmse:0.02736\n",
      "[3804]\tvalidation-rmse:0.02736\n",
      "[3805]\tvalidation-rmse:0.02736\n",
      "[3806]\tvalidation-rmse:0.02735\n",
      "[3807]\tvalidation-rmse:0.02735\n",
      "[3808]\tvalidation-rmse:0.02735\n",
      "[3809]\tvalidation-rmse:0.02735\n",
      "[3810]\tvalidation-rmse:0.02735\n",
      "[3811]\tvalidation-rmse:0.02735\n",
      "[3812]\tvalidation-rmse:0.02735\n",
      "[3813]\tvalidation-rmse:0.02735\n",
      "[3814]\tvalidation-rmse:0.02735\n",
      "[3815]\tvalidation-rmse:0.02735\n",
      "[3816]\tvalidation-rmse:0.02735\n",
      "[3817]\tvalidation-rmse:0.02735\n",
      "[3818]\tvalidation-rmse:0.02735\n",
      "[3819]\tvalidation-rmse:0.02735\n",
      "[3820]\tvalidation-rmse:0.02735\n",
      "[3821]\tvalidation-rmse:0.02735\n",
      "[3822]\tvalidation-rmse:0.02735\n",
      "[3823]\tvalidation-rmse:0.02735\n",
      "[3824]\tvalidation-rmse:0.02735\n",
      "[3825]\tvalidation-rmse:0.02735\n",
      "[3826]\tvalidation-rmse:0.02735\n",
      "[3827]\tvalidation-rmse:0.02735\n",
      "[3828]\tvalidation-rmse:0.02735\n",
      "[3829]\tvalidation-rmse:0.02735\n",
      "[3830]\tvalidation-rmse:0.02735\n",
      "[3831]\tvalidation-rmse:0.02735\n",
      "[3832]\tvalidation-rmse:0.02735\n",
      "[3833]\tvalidation-rmse:0.02735\n",
      "[3834]\tvalidation-rmse:0.02735\n",
      "[3835]\tvalidation-rmse:0.02735\n",
      "[3836]\tvalidation-rmse:0.02735\n",
      "[3837]\tvalidation-rmse:0.02735\n",
      "[3838]\tvalidation-rmse:0.02735\n",
      "[3839]\tvalidation-rmse:0.02735\n",
      "[3840]\tvalidation-rmse:0.02735\n",
      "[3841]\tvalidation-rmse:0.02735\n",
      "[3842]\tvalidation-rmse:0.02735\n",
      "[3843]\tvalidation-rmse:0.02735\n",
      "[3844]\tvalidation-rmse:0.02735\n",
      "[3845]\tvalidation-rmse:0.02735\n",
      "[3846]\tvalidation-rmse:0.02735\n",
      "[3847]\tvalidation-rmse:0.02735\n",
      "[3848]\tvalidation-rmse:0.02735\n",
      "[3849]\tvalidation-rmse:0.02734\n",
      "[3850]\tvalidation-rmse:0.02734\n",
      "[3851]\tvalidation-rmse:0.02734\n",
      "[3852]\tvalidation-rmse:0.02734\n",
      "[3853]\tvalidation-rmse:0.02734\n",
      "[3854]\tvalidation-rmse:0.02734\n",
      "[3855]\tvalidation-rmse:0.02734\n",
      "[3856]\tvalidation-rmse:0.02734\n",
      "[3857]\tvalidation-rmse:0.02734\n",
      "[3858]\tvalidation-rmse:0.02734\n",
      "[3859]\tvalidation-rmse:0.02734\n",
      "[3860]\tvalidation-rmse:0.02734\n",
      "[3861]\tvalidation-rmse:0.02734\n",
      "[3862]\tvalidation-rmse:0.02734\n",
      "[3863]\tvalidation-rmse:0.02734\n",
      "[3864]\tvalidation-rmse:0.02734\n",
      "[3865]\tvalidation-rmse:0.02734\n",
      "[3866]\tvalidation-rmse:0.02734\n",
      "[3867]\tvalidation-rmse:0.02734\n",
      "[3868]\tvalidation-rmse:0.02734\n",
      "[3869]\tvalidation-rmse:0.02734\n",
      "[3870]\tvalidation-rmse:0.02734\n",
      "[3871]\tvalidation-rmse:0.02734\n",
      "[3872]\tvalidation-rmse:0.02734\n",
      "[3873]\tvalidation-rmse:0.02734\n",
      "[3874]\tvalidation-rmse:0.02734\n",
      "[3875]\tvalidation-rmse:0.02734\n",
      "[3876]\tvalidation-rmse:0.02734\n",
      "[3877]\tvalidation-rmse:0.02734\n",
      "[3878]\tvalidation-rmse:0.02733\n",
      "[3879]\tvalidation-rmse:0.02733\n",
      "[3880]\tvalidation-rmse:0.02733\n",
      "[3881]\tvalidation-rmse:0.02733\n",
      "[3882]\tvalidation-rmse:0.02733\n",
      "[3883]\tvalidation-rmse:0.02733\n",
      "[3884]\tvalidation-rmse:0.02733\n",
      "[3885]\tvalidation-rmse:0.02733\n",
      "[3886]\tvalidation-rmse:0.02733\n",
      "[3887]\tvalidation-rmse:0.02733\n",
      "[3888]\tvalidation-rmse:0.02733\n",
      "[3889]\tvalidation-rmse:0.02733\n",
      "[3890]\tvalidation-rmse:0.02733\n",
      "[3891]\tvalidation-rmse:0.02733\n",
      "[3892]\tvalidation-rmse:0.02733\n",
      "[3893]\tvalidation-rmse:0.02733\n",
      "[3894]\tvalidation-rmse:0.02733\n",
      "[3895]\tvalidation-rmse:0.02733\n",
      "[3896]\tvalidation-rmse:0.02733\n",
      "[3897]\tvalidation-rmse:0.02733\n",
      "[3898]\tvalidation-rmse:0.02733\n",
      "[3899]\tvalidation-rmse:0.02733\n",
      "[3900]\tvalidation-rmse:0.02733\n",
      "[3901]\tvalidation-rmse:0.02733\n",
      "[3902]\tvalidation-rmse:0.02733\n",
      "[3903]\tvalidation-rmse:0.02733\n",
      "[3904]\tvalidation-rmse:0.02733\n",
      "[3905]\tvalidation-rmse:0.02733\n",
      "[3906]\tvalidation-rmse:0.02733\n",
      "[3907]\tvalidation-rmse:0.02733\n",
      "[3908]\tvalidation-rmse:0.02732\n",
      "[3909]\tvalidation-rmse:0.02732\n",
      "[3910]\tvalidation-rmse:0.02732\n",
      "[3911]\tvalidation-rmse:0.02732\n",
      "[3912]\tvalidation-rmse:0.02732\n",
      "[3913]\tvalidation-rmse:0.02732\n",
      "[3914]\tvalidation-rmse:0.02732\n",
      "[3915]\tvalidation-rmse:0.02732\n",
      "[3916]\tvalidation-rmse:0.02732\n",
      "[3917]\tvalidation-rmse:0.02732\n",
      "[3918]\tvalidation-rmse:0.02732\n",
      "[3919]\tvalidation-rmse:0.02732\n",
      "[3920]\tvalidation-rmse:0.02732\n",
      "[3921]\tvalidation-rmse:0.02732\n",
      "[3922]\tvalidation-rmse:0.02732\n",
      "[3923]\tvalidation-rmse:0.02732\n",
      "[3924]\tvalidation-rmse:0.02733\n",
      "[3925]\tvalidation-rmse:0.02733\n",
      "[3926]\tvalidation-rmse:0.02733\n",
      "[3927]\tvalidation-rmse:0.02733\n",
      "[3928]\tvalidation-rmse:0.02733\n",
      "[3929]\tvalidation-rmse:0.02733\n",
      "[3930]\tvalidation-rmse:0.02733\n",
      "[3931]\tvalidation-rmse:0.02733\n",
      "[3932]\tvalidation-rmse:0.02733\n",
      "[3933]\tvalidation-rmse:0.02733\n",
      "[3934]\tvalidation-rmse:0.02733\n",
      "[3935]\tvalidation-rmse:0.02733\n",
      "[3936]\tvalidation-rmse:0.02733\n",
      "[3937]\tvalidation-rmse:0.02733\n",
      "[3938]\tvalidation-rmse:0.02733\n",
      "[3939]\tvalidation-rmse:0.02733\n",
      "[3940]\tvalidation-rmse:0.02733\n",
      "[3941]\tvalidation-rmse:0.02733\n",
      "[3942]\tvalidation-rmse:0.02733\n",
      "[3943]\tvalidation-rmse:0.02733\n",
      "[3944]\tvalidation-rmse:0.02733\n",
      "[3945]\tvalidation-rmse:0.02733\n",
      "[3946]\tvalidation-rmse:0.02733\n",
      "[3947]\tvalidation-rmse:0.02733\n",
      "[3948]\tvalidation-rmse:0.02733\n",
      "[3949]\tvalidation-rmse:0.02733\n",
      "[3950]\tvalidation-rmse:0.02733\n",
      "[3951]\tvalidation-rmse:0.02733\n",
      "[3952]\tvalidation-rmse:0.02733\n",
      "[3953]\tvalidation-rmse:0.02733\n",
      "[3954]\tvalidation-rmse:0.02733\n",
      "[3955]\tvalidation-rmse:0.02733\n",
      "[3956]\tvalidation-rmse:0.02733\n",
      "[3957]\tvalidation-rmse:0.02733\n",
      "[3958]\tvalidation-rmse:0.02733\n",
      "[3959]\tvalidation-rmse:0.02733\n",
      "[3960]\tvalidation-rmse:0.02733\n",
      "[3961]\tvalidation-rmse:0.02733\n",
      "[3962]\tvalidation-rmse:0.02733\n",
      "[3963]\tvalidation-rmse:0.02733\n",
      "[3964]\tvalidation-rmse:0.02732\n",
      "[3965]\tvalidation-rmse:0.02732\n",
      "[3966]\tvalidation-rmse:0.02732\n",
      "[3967]\tvalidation-rmse:0.02732\n",
      "[3968]\tvalidation-rmse:0.02732\n",
      "[3969]\tvalidation-rmse:0.02732\n",
      "[3970]\tvalidation-rmse:0.02732\n",
      "[3971]\tvalidation-rmse:0.02732\n",
      "[3972]\tvalidation-rmse:0.02732\n",
      "[3973]\tvalidation-rmse:0.02732\n",
      "[3974]\tvalidation-rmse:0.02732\n",
      "[3975]\tvalidation-rmse:0.02732\n",
      "[3976]\tvalidation-rmse:0.02732\n",
      "[3977]\tvalidation-rmse:0.02732\n",
      "[3978]\tvalidation-rmse:0.02732\n",
      "[3979]\tvalidation-rmse:0.02732\n",
      "[3980]\tvalidation-rmse:0.02732\n",
      "[3981]\tvalidation-rmse:0.02732\n",
      "[3982]\tvalidation-rmse:0.02732\n",
      "[3983]\tvalidation-rmse:0.02732\n",
      "[3984]\tvalidation-rmse:0.02732\n",
      "[3985]\tvalidation-rmse:0.02732\n",
      "[3986]\tvalidation-rmse:0.02732\n",
      "[3987]\tvalidation-rmse:0.02732\n",
      "[3988]\tvalidation-rmse:0.02732\n",
      "[3989]\tvalidation-rmse:0.02732\n",
      "[3990]\tvalidation-rmse:0.02732\n",
      "[3991]\tvalidation-rmse:0.02732\n",
      "[3992]\tvalidation-rmse:0.02732\n",
      "[3993]\tvalidation-rmse:0.02732\n",
      "[3994]\tvalidation-rmse:0.02732\n",
      "[3995]\tvalidation-rmse:0.02732\n",
      "[3996]\tvalidation-rmse:0.02732\n",
      "[3997]\tvalidation-rmse:0.02732\n",
      "[3998]\tvalidation-rmse:0.02732\n",
      "[3999]\tvalidation-rmse:0.02732\n",
      "[4000]\tvalidation-rmse:0.02732\n",
      "[4001]\tvalidation-rmse:0.02732\n",
      "[4002]\tvalidation-rmse:0.02732\n",
      "[4003]\tvalidation-rmse:0.02732\n",
      "[4004]\tvalidation-rmse:0.02732\n",
      "[4005]\tvalidation-rmse:0.02732\n",
      "[4006]\tvalidation-rmse:0.02732\n",
      "[4007]\tvalidation-rmse:0.02732\n",
      "[4008]\tvalidation-rmse:0.02732\n",
      "[4009]\tvalidation-rmse:0.02732\n",
      "[4010]\tvalidation-rmse:0.02732\n",
      "[4011]\tvalidation-rmse:0.02732\n",
      "[4012]\tvalidation-rmse:0.02732\n",
      "[4013]\tvalidation-rmse:0.02732\n",
      "[4014]\tvalidation-rmse:0.02732\n",
      "[4015]\tvalidation-rmse:0.02732\n",
      "[4016]\tvalidation-rmse:0.02732\n",
      "[4017]\tvalidation-rmse:0.02732\n",
      "[4018]\tvalidation-rmse:0.02732\n",
      "[4019]\tvalidation-rmse:0.02732\n",
      "[4020]\tvalidation-rmse:0.02732\n",
      "[4021]\tvalidation-rmse:0.02732\n",
      "[4022]\tvalidation-rmse:0.02732\n",
      "[4023]\tvalidation-rmse:0.02732\n",
      "[4024]\tvalidation-rmse:0.02732\n",
      "[4025]\tvalidation-rmse:0.02732\n",
      "[4026]\tvalidation-rmse:0.02732\n",
      "[4027]\tvalidation-rmse:0.02732\n",
      "[4028]\tvalidation-rmse:0.02732\n",
      "[4029]\tvalidation-rmse:0.02732\n",
      "[4030]\tvalidation-rmse:0.02732\n",
      "[4031]\tvalidation-rmse:0.02732\n",
      "[4032]\tvalidation-rmse:0.02732\n",
      "[4033]\tvalidation-rmse:0.02732\n",
      "[4034]\tvalidation-rmse:0.02731\n",
      "[4035]\tvalidation-rmse:0.02731\n",
      "[4036]\tvalidation-rmse:0.02731\n",
      "[4037]\tvalidation-rmse:0.02731\n",
      "[4038]\tvalidation-rmse:0.02731\n",
      "[4039]\tvalidation-rmse:0.02731\n",
      "[4040]\tvalidation-rmse:0.02731\n",
      "[4041]\tvalidation-rmse:0.02731\n",
      "[4042]\tvalidation-rmse:0.02731\n",
      "[4043]\tvalidation-rmse:0.02731\n",
      "[4044]\tvalidation-rmse:0.02731\n",
      "[4045]\tvalidation-rmse:0.02731\n",
      "[4046]\tvalidation-rmse:0.02731\n",
      "[4047]\tvalidation-rmse:0.02731\n",
      "[4048]\tvalidation-rmse:0.02731\n",
      "[4049]\tvalidation-rmse:0.02731\n",
      "[4050]\tvalidation-rmse:0.02731\n",
      "[4051]\tvalidation-rmse:0.02731\n",
      "[4052]\tvalidation-rmse:0.02731\n",
      "[4053]\tvalidation-rmse:0.02731\n",
      "[4054]\tvalidation-rmse:0.02731\n",
      "[4055]\tvalidation-rmse:0.02731\n",
      "[4056]\tvalidation-rmse:0.02731\n",
      "[4057]\tvalidation-rmse:0.02731\n",
      "[4058]\tvalidation-rmse:0.02731\n",
      "[4059]\tvalidation-rmse:0.02731\n",
      "[4060]\tvalidation-rmse:0.02731\n",
      "[4061]\tvalidation-rmse:0.02731\n",
      "[4062]\tvalidation-rmse:0.02731\n",
      "[4063]\tvalidation-rmse:0.02731\n",
      "[4064]\tvalidation-rmse:0.02731\n",
      "[4065]\tvalidation-rmse:0.02731\n",
      "[4066]\tvalidation-rmse:0.02731\n",
      "[4067]\tvalidation-rmse:0.02731\n",
      "[4068]\tvalidation-rmse:0.02731\n",
      "[4069]\tvalidation-rmse:0.02731\n",
      "[4070]\tvalidation-rmse:0.02731\n",
      "[4071]\tvalidation-rmse:0.02731\n",
      "[4072]\tvalidation-rmse:0.02731\n",
      "[4073]\tvalidation-rmse:0.02731\n",
      "[4074]\tvalidation-rmse:0.02731\n",
      "[4075]\tvalidation-rmse:0.02731\n",
      "[4076]\tvalidation-rmse:0.02731\n",
      "[4077]\tvalidation-rmse:0.02731\n",
      "[4078]\tvalidation-rmse:0.02731\n",
      "[4079]\tvalidation-rmse:0.02731\n",
      "[4080]\tvalidation-rmse:0.02731\n",
      "[4081]\tvalidation-rmse:0.02731\n",
      "[4082]\tvalidation-rmse:0.02731\n",
      "[4083]\tvalidation-rmse:0.02731\n",
      "[4084]\tvalidation-rmse:0.02731\n",
      "[4085]\tvalidation-rmse:0.02731\n",
      "[4086]\tvalidation-rmse:0.02731\n",
      "[4087]\tvalidation-rmse:0.02730\n",
      "[4088]\tvalidation-rmse:0.02730\n",
      "[4089]\tvalidation-rmse:0.02730\n",
      "[4090]\tvalidation-rmse:0.02730\n",
      "[4091]\tvalidation-rmse:0.02730\n",
      "[4092]\tvalidation-rmse:0.02730\n",
      "[4093]\tvalidation-rmse:0.02730\n",
      "[4094]\tvalidation-rmse:0.02730\n",
      "[4095]\tvalidation-rmse:0.02730\n",
      "[4096]\tvalidation-rmse:0.02730\n",
      "[4097]\tvalidation-rmse:0.02730\n",
      "[4098]\tvalidation-rmse:0.02730\n",
      "[4099]\tvalidation-rmse:0.02730\n",
      "[4100]\tvalidation-rmse:0.02730\n",
      "[4101]\tvalidation-rmse:0.02730\n",
      "[4102]\tvalidation-rmse:0.02730\n",
      "[4103]\tvalidation-rmse:0.02730\n",
      "[4104]\tvalidation-rmse:0.02730\n",
      "[4105]\tvalidation-rmse:0.02730\n",
      "[4106]\tvalidation-rmse:0.02730\n",
      "[4107]\tvalidation-rmse:0.02730\n",
      "[4108]\tvalidation-rmse:0.02730\n",
      "[4109]\tvalidation-rmse:0.02730\n",
      "[4110]\tvalidation-rmse:0.02730\n",
      "[4111]\tvalidation-rmse:0.02730\n",
      "[4112]\tvalidation-rmse:0.02730\n",
      "[4113]\tvalidation-rmse:0.02730\n",
      "[4114]\tvalidation-rmse:0.02730\n",
      "[4115]\tvalidation-rmse:0.02730\n",
      "[4116]\tvalidation-rmse:0.02730\n",
      "[4117]\tvalidation-rmse:0.02730\n",
      "[4118]\tvalidation-rmse:0.02730\n",
      "[4119]\tvalidation-rmse:0.02730\n",
      "[4120]\tvalidation-rmse:0.02730\n",
      "[4121]\tvalidation-rmse:0.02730\n",
      "[4122]\tvalidation-rmse:0.02730\n",
      "[4123]\tvalidation-rmse:0.02730\n",
      "[4124]\tvalidation-rmse:0.02730\n",
      "[4125]\tvalidation-rmse:0.02730\n",
      "[4126]\tvalidation-rmse:0.02730\n",
      "[4127]\tvalidation-rmse:0.02730\n",
      "[4128]\tvalidation-rmse:0.02730\n",
      "[4129]\tvalidation-rmse:0.02730\n",
      "[4130]\tvalidation-rmse:0.02730\n",
      "[4131]\tvalidation-rmse:0.02730\n",
      "[4132]\tvalidation-rmse:0.02730\n",
      "[4133]\tvalidation-rmse:0.02730\n",
      "[4134]\tvalidation-rmse:0.02730\n",
      "[4135]\tvalidation-rmse:0.02730\n",
      "[4136]\tvalidation-rmse:0.02730\n",
      "[4137]\tvalidation-rmse:0.02730\n",
      "[4138]\tvalidation-rmse:0.02730\n",
      "[4139]\tvalidation-rmse:0.02730\n",
      "[4140]\tvalidation-rmse:0.02730\n",
      "[4141]\tvalidation-rmse:0.02730\n",
      "[4142]\tvalidation-rmse:0.02730\n",
      "[4143]\tvalidation-rmse:0.02730\n",
      "[4144]\tvalidation-rmse:0.02730\n",
      "[4145]\tvalidation-rmse:0.02730\n",
      "[4146]\tvalidation-rmse:0.02730\n",
      "[4147]\tvalidation-rmse:0.02730\n",
      "[4148]\tvalidation-rmse:0.02730\n",
      "[4149]\tvalidation-rmse:0.02730\n",
      "[4150]\tvalidation-rmse:0.02730\n",
      "[4151]\tvalidation-rmse:0.02730\n",
      "[4152]\tvalidation-rmse:0.02730\n",
      "[4153]\tvalidation-rmse:0.02730\n",
      "[4154]\tvalidation-rmse:0.02730\n",
      "[4155]\tvalidation-rmse:0.02730\n",
      "[4156]\tvalidation-rmse:0.02730\n",
      "[4157]\tvalidation-rmse:0.02730\n",
      "[4158]\tvalidation-rmse:0.02730\n",
      "[4159]\tvalidation-rmse:0.02730\n",
      "[4160]\tvalidation-rmse:0.02730\n",
      "[4161]\tvalidation-rmse:0.02730\n",
      "[4162]\tvalidation-rmse:0.02730\n",
      "[4163]\tvalidation-rmse:0.02730\n",
      "[4164]\tvalidation-rmse:0.02730\n",
      "[4165]\tvalidation-rmse:0.02730\n",
      "[4166]\tvalidation-rmse:0.02730\n",
      "[4167]\tvalidation-rmse:0.02730\n",
      "[4168]\tvalidation-rmse:0.02731\n",
      "[4169]\tvalidation-rmse:0.02731\n",
      "[4170]\tvalidation-rmse:0.02730\n",
      "[4171]\tvalidation-rmse:0.02730\n",
      "[4172]\tvalidation-rmse:0.02730\n",
      "[4173]\tvalidation-rmse:0.02730\n",
      "[4174]\tvalidation-rmse:0.02730\n",
      "[4175]\tvalidation-rmse:0.02730\n",
      "[4176]\tvalidation-rmse:0.02730\n",
      "[4177]\tvalidation-rmse:0.02730\n",
      "[4178]\tvalidation-rmse:0.02730\n",
      "[4179]\tvalidation-rmse:0.02730\n",
      "[4180]\tvalidation-rmse:0.02730\n",
      "[4181]\tvalidation-rmse:0.02730\n",
      "[4182]\tvalidation-rmse:0.02730\n",
      "[4183]\tvalidation-rmse:0.02730\n",
      "[4184]\tvalidation-rmse:0.02730\n",
      "[4185]\tvalidation-rmse:0.02730\n",
      "[4186]\tvalidation-rmse:0.02730\n",
      "[4187]\tvalidation-rmse:0.02730\n",
      "[4188]\tvalidation-rmse:0.02730\n",
      "[4189]\tvalidation-rmse:0.02730\n",
      "[4190]\tvalidation-rmse:0.02730\n",
      "[4191]\tvalidation-rmse:0.02730\n",
      "[4192]\tvalidation-rmse:0.02730\n",
      "[4193]\tvalidation-rmse:0.02730\n",
      "[4194]\tvalidation-rmse:0.02730\n",
      "[4195]\tvalidation-rmse:0.02730\n",
      "[4196]\tvalidation-rmse:0.02730\n",
      "[4197]\tvalidation-rmse:0.02730\n",
      "[4198]\tvalidation-rmse:0.02730\n",
      "[4199]\tvalidation-rmse:0.02730\n",
      "[4200]\tvalidation-rmse:0.02730\n",
      "[4201]\tvalidation-rmse:0.02730\n",
      "[4202]\tvalidation-rmse:0.02730\n",
      "[4203]\tvalidation-rmse:0.02730\n",
      "[4204]\tvalidation-rmse:0.02730\n",
      "[4205]\tvalidation-rmse:0.02730\n",
      "[4206]\tvalidation-rmse:0.02730\n",
      "[4207]\tvalidation-rmse:0.02730\n",
      "[4208]\tvalidation-rmse:0.02730\n",
      "[4209]\tvalidation-rmse:0.02730\n",
      "[4210]\tvalidation-rmse:0.02730\n",
      "[4211]\tvalidation-rmse:0.02730\n",
      "[4212]\tvalidation-rmse:0.02730\n",
      "[4213]\tvalidation-rmse:0.02730\n",
      "[4214]\tvalidation-rmse:0.02730\n",
      "[4215]\tvalidation-rmse:0.02731\n",
      "[4216]\tvalidation-rmse:0.02731\n",
      "[4217]\tvalidation-rmse:0.02731\n",
      "[4218]\tvalidation-rmse:0.02731\n",
      "[4219]\tvalidation-rmse:0.02731\n",
      "[4220]\tvalidation-rmse:0.02731\n",
      "[4221]\tvalidation-rmse:0.02731\n",
      "[4222]\tvalidation-rmse:0.02731\n",
      "[4223]\tvalidation-rmse:0.02731\n",
      "[4224]\tvalidation-rmse:0.02731\n",
      "[4225]\tvalidation-rmse:0.02730\n",
      "[4226]\tvalidation-rmse:0.02731\n",
      "[4227]\tvalidation-rmse:0.02731\n",
      "[4228]\tvalidation-rmse:0.02731\n",
      "[4229]\tvalidation-rmse:0.02730\n",
      "[4230]\tvalidation-rmse:0.02730\n",
      "[4231]\tvalidation-rmse:0.02730\n",
      "[4232]\tvalidation-rmse:0.02730\n",
      "[4233]\tvalidation-rmse:0.02730\n",
      "[4234]\tvalidation-rmse:0.02731\n",
      "[4235]\tvalidation-rmse:0.02731\n",
      "[4236]\tvalidation-rmse:0.02731\n",
      "[4237]\tvalidation-rmse:0.02731\n",
      "[4238]\tvalidation-rmse:0.02731\n",
      "[4239]\tvalidation-rmse:0.02731\n",
      "[4240]\tvalidation-rmse:0.02731\n",
      "[4241]\tvalidation-rmse:0.02731\n",
      "[4242]\tvalidation-rmse:0.02731\n",
      "[4243]\tvalidation-rmse:0.02731\n",
      "Model for X stopped at best iteration: 4143\n",
      "\n",
      "--- Training Y-coordinate model ---\n",
      "[0]\tvalidation-rmse:1.13979\n",
      "[1]\tvalidation-rmse:1.12868\n",
      "[2]\tvalidation-rmse:1.11772\n",
      "[3]\tvalidation-rmse:1.10700\n",
      "[4]\tvalidation-rmse:1.09638\n",
      "[5]\tvalidation-rmse:1.08579\n",
      "[6]\tvalidation-rmse:1.07525\n",
      "[7]\tvalidation-rmse:1.06492\n",
      "[8]\tvalidation-rmse:1.05577\n",
      "[9]\tvalidation-rmse:1.04546\n",
      "[10]\tvalidation-rmse:1.03540\n",
      "[11]\tvalidation-rmse:1.02541\n",
      "[12]\tvalidation-rmse:1.01549\n",
      "[13]\tvalidation-rmse:1.00569\n",
      "[14]\tvalidation-rmse:0.99598\n",
      "[15]\tvalidation-rmse:0.98658\n",
      "[16]\tvalidation-rmse:0.97706\n",
      "[17]\tvalidation-rmse:0.96948\n",
      "[18]\tvalidation-rmse:0.96020\n",
      "[19]\tvalidation-rmse:0.95099\n",
      "[20]\tvalidation-rmse:0.94236\n",
      "[21]\tvalidation-rmse:0.93464\n",
      "[22]\tvalidation-rmse:0.92635\n",
      "[23]\tvalidation-rmse:0.91760\n",
      "[24]\tvalidation-rmse:0.90884\n",
      "[25]\tvalidation-rmse:0.90022\n",
      "[26]\tvalidation-rmse:0.89159\n",
      "[27]\tvalidation-rmse:0.88320\n",
      "[28]\tvalidation-rmse:0.87491\n",
      "[29]\tvalidation-rmse:0.86633\n",
      "[30]\tvalidation-rmse:0.85813\n",
      "[31]\tvalidation-rmse:0.85014\n",
      "[32]\tvalidation-rmse:0.84207\n",
      "[33]\tvalidation-rmse:0.83402\n",
      "[34]\tvalidation-rmse:0.82602\n",
      "[35]\tvalidation-rmse:0.81816\n",
      "[36]\tvalidation-rmse:0.81059\n",
      "[37]\tvalidation-rmse:0.80386\n",
      "[38]\tvalidation-rmse:0.79612\n",
      "[39]\tvalidation-rmse:0.78856\n",
      "[40]\tvalidation-rmse:0.78109\n",
      "[41]\tvalidation-rmse:0.77359\n",
      "[42]\tvalidation-rmse:0.76625\n",
      "[43]\tvalidation-rmse:0.75891\n",
      "[44]\tvalidation-rmse:0.75158\n",
      "[45]\tvalidation-rmse:0.74450\n",
      "[46]\tvalidation-rmse:0.73745\n",
      "[47]\tvalidation-rmse:0.73037\n",
      "[48]\tvalidation-rmse:0.72347\n",
      "[49]\tvalidation-rmse:0.71654\n",
      "[50]\tvalidation-rmse:0.70964\n",
      "[51]\tvalidation-rmse:0.70278\n",
      "[52]\tvalidation-rmse:0.69616\n",
      "[53]\tvalidation-rmse:0.68942\n",
      "[54]\tvalidation-rmse:0.68278\n",
      "[55]\tvalidation-rmse:0.67640\n",
      "[56]\tvalidation-rmse:0.66994\n",
      "[57]\tvalidation-rmse:0.66345\n",
      "[58]\tvalidation-rmse:0.65712\n",
      "[59]\tvalidation-rmse:0.65083\n",
      "[60]\tvalidation-rmse:0.64467\n",
      "[61]\tvalidation-rmse:0.63854\n",
      "[62]\tvalidation-rmse:0.63239\n",
      "[63]\tvalidation-rmse:0.62657\n",
      "[64]\tvalidation-rmse:0.62073\n",
      "[65]\tvalidation-rmse:0.61483\n",
      "[66]\tvalidation-rmse:0.61048\n",
      "[67]\tvalidation-rmse:0.60461\n",
      "[68]\tvalidation-rmse:0.59881\n",
      "[69]\tvalidation-rmse:0.59307\n",
      "[70]\tvalidation-rmse:0.58744\n",
      "[71]\tvalidation-rmse:0.58197\n",
      "[72]\tvalidation-rmse:0.57637\n",
      "[73]\tvalidation-rmse:0.57108\n",
      "[74]\tvalidation-rmse:0.56580\n",
      "[75]\tvalidation-rmse:0.56050\n",
      "[76]\tvalidation-rmse:0.55580\n",
      "[77]\tvalidation-rmse:0.55047\n",
      "[78]\tvalidation-rmse:0.54525\n",
      "[79]\tvalidation-rmse:0.54005\n",
      "[80]\tvalidation-rmse:0.53506\n",
      "[81]\tvalidation-rmse:0.52994\n",
      "[82]\tvalidation-rmse:0.52488\n",
      "[83]\tvalidation-rmse:0.51981\n",
      "[84]\tvalidation-rmse:0.51484\n",
      "[85]\tvalidation-rmse:0.51005\n",
      "[86]\tvalidation-rmse:0.50517\n",
      "[87]\tvalidation-rmse:0.50053\n",
      "[88]\tvalidation-rmse:0.49587\n",
      "[89]\tvalidation-rmse:0.49110\n",
      "[90]\tvalidation-rmse:0.48642\n",
      "[91]\tvalidation-rmse:0.48214\n",
      "[92]\tvalidation-rmse:0.47754\n",
      "[93]\tvalidation-rmse:0.47295\n",
      "[94]\tvalidation-rmse:0.46843\n",
      "[95]\tvalidation-rmse:0.46400\n",
      "[96]\tvalidation-rmse:0.45965\n",
      "[97]\tvalidation-rmse:0.45525\n",
      "[98]\tvalidation-rmse:0.45239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/callback.py:386: UserWarning: [16:32:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99]\tvalidation-rmse:0.44812\n",
      "[100]\tvalidation-rmse:0.44399\n",
      "[101]\tvalidation-rmse:0.43991\n",
      "[102]\tvalidation-rmse:0.43569\n",
      "[103]\tvalidation-rmse:0.43170\n",
      "[104]\tvalidation-rmse:0.42795\n",
      "[105]\tvalidation-rmse:0.42397\n",
      "[106]\tvalidation-rmse:0.41997\n",
      "[107]\tvalidation-rmse:0.41600\n",
      "[108]\tvalidation-rmse:0.41206\n",
      "[109]\tvalidation-rmse:0.40825\n",
      "[110]\tvalidation-rmse:0.40483\n",
      "[111]\tvalidation-rmse:0.40094\n",
      "[112]\tvalidation-rmse:0.39718\n",
      "[113]\tvalidation-rmse:0.39340\n",
      "[114]\tvalidation-rmse:0.39018\n",
      "[115]\tvalidation-rmse:0.38655\n",
      "[116]\tvalidation-rmse:0.38322\n",
      "[117]\tvalidation-rmse:0.37967\n",
      "[118]\tvalidation-rmse:0.37605\n",
      "[119]\tvalidation-rmse:0.37256\n",
      "[120]\tvalidation-rmse:0.36902\n",
      "[121]\tvalidation-rmse:0.36548\n",
      "[122]\tvalidation-rmse:0.36198\n",
      "[123]\tvalidation-rmse:0.35864\n",
      "[124]\tvalidation-rmse:0.35520\n",
      "[125]\tvalidation-rmse:0.35181\n",
      "[126]\tvalidation-rmse:0.34847\n",
      "[127]\tvalidation-rmse:0.34518\n",
      "[128]\tvalidation-rmse:0.34204\n",
      "[129]\tvalidation-rmse:0.33881\n",
      "[130]\tvalidation-rmse:0.33562\n",
      "[131]\tvalidation-rmse:0.33246\n",
      "[132]\tvalidation-rmse:0.32940\n",
      "[133]\tvalidation-rmse:0.32627\n",
      "[134]\tvalidation-rmse:0.32317\n",
      "[135]\tvalidation-rmse:0.32019\n",
      "[136]\tvalidation-rmse:0.31715\n",
      "[137]\tvalidation-rmse:0.31449\n",
      "[138]\tvalidation-rmse:0.31162\n",
      "[139]\tvalidation-rmse:0.30881\n",
      "[140]\tvalidation-rmse:0.30597\n",
      "[141]\tvalidation-rmse:0.30311\n",
      "[142]\tvalidation-rmse:0.30026\n",
      "[143]\tvalidation-rmse:0.29742\n",
      "[144]\tvalidation-rmse:0.29469\n",
      "[145]\tvalidation-rmse:0.29190\n",
      "[146]\tvalidation-rmse:0.28918\n",
      "[147]\tvalidation-rmse:0.28667\n",
      "[148]\tvalidation-rmse:0.28397\n",
      "[149]\tvalidation-rmse:0.28130\n",
      "[150]\tvalidation-rmse:0.27863\n",
      "[151]\tvalidation-rmse:0.27634\n",
      "[152]\tvalidation-rmse:0.27384\n",
      "[153]\tvalidation-rmse:0.27127\n",
      "[154]\tvalidation-rmse:0.26873\n",
      "[155]\tvalidation-rmse:0.26621\n",
      "[156]\tvalidation-rmse:0.26369\n",
      "[157]\tvalidation-rmse:0.26121\n",
      "[158]\tvalidation-rmse:0.25872\n",
      "[159]\tvalidation-rmse:0.25630\n",
      "[160]\tvalidation-rmse:0.25389\n",
      "[161]\tvalidation-rmse:0.25159\n",
      "[162]\tvalidation-rmse:0.24924\n",
      "[163]\tvalidation-rmse:0.24689\n",
      "[164]\tvalidation-rmse:0.24464\n",
      "[165]\tvalidation-rmse:0.24238\n",
      "[166]\tvalidation-rmse:0.24021\n",
      "[167]\tvalidation-rmse:0.23815\n",
      "[168]\tvalidation-rmse:0.23591\n",
      "[169]\tvalidation-rmse:0.23373\n",
      "[170]\tvalidation-rmse:0.23165\n",
      "[171]\tvalidation-rmse:0.22946\n",
      "[172]\tvalidation-rmse:0.22740\n",
      "[173]\tvalidation-rmse:0.22529\n",
      "[174]\tvalidation-rmse:0.22333\n",
      "[175]\tvalidation-rmse:0.22124\n",
      "[176]\tvalidation-rmse:0.21922\n",
      "[177]\tvalidation-rmse:0.21718\n",
      "[178]\tvalidation-rmse:0.21516\n",
      "[179]\tvalidation-rmse:0.21320\n",
      "[180]\tvalidation-rmse:0.21124\n",
      "[181]\tvalidation-rmse:0.20933\n",
      "[182]\tvalidation-rmse:0.20764\n",
      "[183]\tvalidation-rmse:0.20584\n",
      "[184]\tvalidation-rmse:0.20401\n",
      "[185]\tvalidation-rmse:0.20212\n",
      "[186]\tvalidation-rmse:0.20024\n",
      "[187]\tvalidation-rmse:0.19855\n",
      "[188]\tvalidation-rmse:0.19671\n",
      "[189]\tvalidation-rmse:0.19489\n",
      "[190]\tvalidation-rmse:0.19308\n",
      "[191]\tvalidation-rmse:0.19134\n",
      "[192]\tvalidation-rmse:0.18958\n",
      "[193]\tvalidation-rmse:0.18787\n",
      "[194]\tvalidation-rmse:0.18616\n",
      "[195]\tvalidation-rmse:0.18450\n",
      "[196]\tvalidation-rmse:0.18279\n",
      "[197]\tvalidation-rmse:0.18112\n",
      "[198]\tvalidation-rmse:0.17947\n",
      "[199]\tvalidation-rmse:0.17796\n",
      "[200]\tvalidation-rmse:0.17639\n",
      "[201]\tvalidation-rmse:0.17479\n",
      "[202]\tvalidation-rmse:0.17322\n",
      "[203]\tvalidation-rmse:0.17166\n",
      "[204]\tvalidation-rmse:0.17014\n",
      "[205]\tvalidation-rmse:0.16861\n",
      "[206]\tvalidation-rmse:0.16708\n",
      "[207]\tvalidation-rmse:0.16557\n",
      "[208]\tvalidation-rmse:0.16410\n",
      "[209]\tvalidation-rmse:0.16274\n",
      "[210]\tvalidation-rmse:0.16130\n",
      "[211]\tvalidation-rmse:0.15983\n",
      "[212]\tvalidation-rmse:0.15854\n",
      "[213]\tvalidation-rmse:0.15709\n",
      "[214]\tvalidation-rmse:0.15569\n",
      "[215]\tvalidation-rmse:0.15426\n",
      "[216]\tvalidation-rmse:0.15287\n",
      "[217]\tvalidation-rmse:0.15157\n",
      "[218]\tvalidation-rmse:0.15019\n",
      "[219]\tvalidation-rmse:0.14898\n",
      "[220]\tvalidation-rmse:0.14763\n",
      "[221]\tvalidation-rmse:0.14632\n",
      "[222]\tvalidation-rmse:0.14519\n",
      "[223]\tvalidation-rmse:0.14389\n",
      "[224]\tvalidation-rmse:0.14265\n",
      "[225]\tvalidation-rmse:0.14144\n",
      "[226]\tvalidation-rmse:0.14055\n",
      "[227]\tvalidation-rmse:0.13930\n",
      "[228]\tvalidation-rmse:0.13805\n",
      "[229]\tvalidation-rmse:0.13700\n",
      "[230]\tvalidation-rmse:0.13583\n",
      "[231]\tvalidation-rmse:0.13468\n",
      "[232]\tvalidation-rmse:0.13350\n",
      "[233]\tvalidation-rmse:0.13229\n",
      "[234]\tvalidation-rmse:0.13114\n",
      "[235]\tvalidation-rmse:0.12998\n",
      "[236]\tvalidation-rmse:0.12883\n",
      "[237]\tvalidation-rmse:0.12773\n",
      "[238]\tvalidation-rmse:0.12660\n",
      "[239]\tvalidation-rmse:0.12554\n",
      "[240]\tvalidation-rmse:0.12440\n",
      "[241]\tvalidation-rmse:0.12331\n",
      "[242]\tvalidation-rmse:0.12225\n",
      "[243]\tvalidation-rmse:0.12119\n",
      "[244]\tvalidation-rmse:0.12014\n",
      "[245]\tvalidation-rmse:0.11907\n",
      "[246]\tvalidation-rmse:0.11806\n",
      "[247]\tvalidation-rmse:0.11708\n",
      "[248]\tvalidation-rmse:0.11609\n",
      "[249]\tvalidation-rmse:0.11514\n",
      "[250]\tvalidation-rmse:0.11418\n",
      "[251]\tvalidation-rmse:0.11322\n",
      "[252]\tvalidation-rmse:0.11224\n",
      "[253]\tvalidation-rmse:0.11129\n",
      "[254]\tvalidation-rmse:0.11034\n",
      "[255]\tvalidation-rmse:0.10938\n",
      "[256]\tvalidation-rmse:0.10852\n",
      "[257]\tvalidation-rmse:0.10762\n",
      "[258]\tvalidation-rmse:0.10671\n",
      "[259]\tvalidation-rmse:0.10580\n",
      "[260]\tvalidation-rmse:0.10487\n",
      "[261]\tvalidation-rmse:0.10403\n",
      "[262]\tvalidation-rmse:0.10329\n",
      "[263]\tvalidation-rmse:0.10239\n",
      "[264]\tvalidation-rmse:0.10163\n",
      "[265]\tvalidation-rmse:0.10077\n",
      "[266]\tvalidation-rmse:0.09992\n",
      "[267]\tvalidation-rmse:0.09914\n",
      "[268]\tvalidation-rmse:0.09830\n",
      "[269]\tvalidation-rmse:0.09752\n",
      "[270]\tvalidation-rmse:0.09669\n",
      "[271]\tvalidation-rmse:0.09590\n",
      "[272]\tvalidation-rmse:0.09508\n",
      "[273]\tvalidation-rmse:0.09432\n",
      "[274]\tvalidation-rmse:0.09354\n",
      "[275]\tvalidation-rmse:0.09275\n",
      "[276]\tvalidation-rmse:0.09201\n",
      "[277]\tvalidation-rmse:0.09137\n",
      "[278]\tvalidation-rmse:0.09076\n",
      "[279]\tvalidation-rmse:0.09005\n",
      "[280]\tvalidation-rmse:0.08931\n",
      "[281]\tvalidation-rmse:0.08856\n",
      "[282]\tvalidation-rmse:0.08785\n",
      "[283]\tvalidation-rmse:0.08713\n",
      "[284]\tvalidation-rmse:0.08650\n",
      "[285]\tvalidation-rmse:0.08584\n",
      "[286]\tvalidation-rmse:0.08515\n",
      "[287]\tvalidation-rmse:0.08455\n",
      "[288]\tvalidation-rmse:0.08386\n",
      "[289]\tvalidation-rmse:0.08316\n",
      "[290]\tvalidation-rmse:0.08248\n",
      "[291]\tvalidation-rmse:0.08185\n",
      "[292]\tvalidation-rmse:0.08119\n",
      "[293]\tvalidation-rmse:0.08053\n",
      "[294]\tvalidation-rmse:0.07988\n",
      "[295]\tvalidation-rmse:0.07924\n",
      "[296]\tvalidation-rmse:0.07861\n",
      "[297]\tvalidation-rmse:0.07797\n",
      "[298]\tvalidation-rmse:0.07736\n",
      "[299]\tvalidation-rmse:0.07679\n",
      "[300]\tvalidation-rmse:0.07630\n",
      "[301]\tvalidation-rmse:0.07569\n",
      "[302]\tvalidation-rmse:0.07509\n",
      "[303]\tvalidation-rmse:0.07448\n",
      "[304]\tvalidation-rmse:0.07393\n",
      "[305]\tvalidation-rmse:0.07337\n",
      "[306]\tvalidation-rmse:0.07282\n",
      "[307]\tvalidation-rmse:0.07227\n",
      "[308]\tvalidation-rmse:0.07170\n",
      "[309]\tvalidation-rmse:0.07121\n",
      "[310]\tvalidation-rmse:0.07066\n",
      "[311]\tvalidation-rmse:0.07011\n",
      "[312]\tvalidation-rmse:0.06957\n",
      "[313]\tvalidation-rmse:0.06906\n",
      "[314]\tvalidation-rmse:0.06857\n",
      "[315]\tvalidation-rmse:0.06807\n",
      "[316]\tvalidation-rmse:0.06756\n",
      "[317]\tvalidation-rmse:0.06708\n",
      "[318]\tvalidation-rmse:0.06658\n",
      "[319]\tvalidation-rmse:0.06609\n",
      "[320]\tvalidation-rmse:0.06561\n",
      "[321]\tvalidation-rmse:0.06513\n",
      "[322]\tvalidation-rmse:0.06464\n",
      "[323]\tvalidation-rmse:0.06418\n",
      "[324]\tvalidation-rmse:0.06372\n",
      "[325]\tvalidation-rmse:0.06324\n",
      "[326]\tvalidation-rmse:0.06277\n",
      "[327]\tvalidation-rmse:0.06231\n",
      "[328]\tvalidation-rmse:0.06189\n",
      "[329]\tvalidation-rmse:0.06143\n",
      "[330]\tvalidation-rmse:0.06105\n",
      "[331]\tvalidation-rmse:0.06062\n",
      "[332]\tvalidation-rmse:0.06019\n",
      "[333]\tvalidation-rmse:0.05981\n",
      "[334]\tvalidation-rmse:0.05949\n",
      "[335]\tvalidation-rmse:0.05908\n",
      "[336]\tvalidation-rmse:0.05865\n",
      "[337]\tvalidation-rmse:0.05825\n",
      "[338]\tvalidation-rmse:0.05786\n",
      "[339]\tvalidation-rmse:0.05744\n",
      "[340]\tvalidation-rmse:0.05705\n",
      "[341]\tvalidation-rmse:0.05672\n",
      "[342]\tvalidation-rmse:0.05633\n",
      "[343]\tvalidation-rmse:0.05594\n",
      "[344]\tvalidation-rmse:0.05559\n",
      "[345]\tvalidation-rmse:0.05524\n",
      "[346]\tvalidation-rmse:0.05486\n",
      "[347]\tvalidation-rmse:0.05452\n",
      "[348]\tvalidation-rmse:0.05414\n",
      "[349]\tvalidation-rmse:0.05379\n",
      "[350]\tvalidation-rmse:0.05345\n",
      "[351]\tvalidation-rmse:0.05311\n",
      "[352]\tvalidation-rmse:0.05277\n",
      "[353]\tvalidation-rmse:0.05244\n",
      "[354]\tvalidation-rmse:0.05211\n",
      "[355]\tvalidation-rmse:0.05182\n",
      "[356]\tvalidation-rmse:0.05149\n",
      "[357]\tvalidation-rmse:0.05115\n",
      "[358]\tvalidation-rmse:0.05085\n",
      "[359]\tvalidation-rmse:0.05055\n",
      "[360]\tvalidation-rmse:0.05021\n",
      "[361]\tvalidation-rmse:0.04992\n",
      "[362]\tvalidation-rmse:0.04959\n",
      "[363]\tvalidation-rmse:0.04927\n",
      "[364]\tvalidation-rmse:0.04896\n",
      "[365]\tvalidation-rmse:0.04866\n",
      "[366]\tvalidation-rmse:0.04837\n",
      "[367]\tvalidation-rmse:0.04815\n",
      "[368]\tvalidation-rmse:0.04788\n",
      "[369]\tvalidation-rmse:0.04760\n",
      "[370]\tvalidation-rmse:0.04734\n",
      "[371]\tvalidation-rmse:0.04705\n",
      "[372]\tvalidation-rmse:0.04678\n",
      "[373]\tvalidation-rmse:0.04651\n",
      "[374]\tvalidation-rmse:0.04625\n",
      "[375]\tvalidation-rmse:0.04599\n",
      "[376]\tvalidation-rmse:0.04570\n",
      "[377]\tvalidation-rmse:0.04548\n",
      "[378]\tvalidation-rmse:0.04524\n",
      "[379]\tvalidation-rmse:0.04498\n",
      "[380]\tvalidation-rmse:0.04476\n",
      "[381]\tvalidation-rmse:0.04451\n",
      "[382]\tvalidation-rmse:0.04428\n",
      "[383]\tvalidation-rmse:0.04404\n",
      "[384]\tvalidation-rmse:0.04382\n",
      "[385]\tvalidation-rmse:0.04359\n",
      "[386]\tvalidation-rmse:0.04336\n",
      "[387]\tvalidation-rmse:0.04311\n",
      "[388]\tvalidation-rmse:0.04287\n",
      "[389]\tvalidation-rmse:0.04263\n",
      "[390]\tvalidation-rmse:0.04242\n",
      "[391]\tvalidation-rmse:0.04221\n",
      "[392]\tvalidation-rmse:0.04202\n",
      "[393]\tvalidation-rmse:0.04180\n",
      "[394]\tvalidation-rmse:0.04161\n",
      "[395]\tvalidation-rmse:0.04139\n",
      "[396]\tvalidation-rmse:0.04117\n",
      "[397]\tvalidation-rmse:0.04097\n",
      "[398]\tvalidation-rmse:0.04076\n",
      "[399]\tvalidation-rmse:0.04055\n",
      "[400]\tvalidation-rmse:0.04037\n",
      "[401]\tvalidation-rmse:0.04017\n",
      "[402]\tvalidation-rmse:0.03997\n",
      "[403]\tvalidation-rmse:0.03979\n",
      "[404]\tvalidation-rmse:0.03964\n",
      "[405]\tvalidation-rmse:0.03946\n",
      "[406]\tvalidation-rmse:0.03927\n",
      "[407]\tvalidation-rmse:0.03910\n",
      "[408]\tvalidation-rmse:0.03892\n",
      "[409]\tvalidation-rmse:0.03874\n",
      "[410]\tvalidation-rmse:0.03855\n",
      "[411]\tvalidation-rmse:0.03839\n",
      "[412]\tvalidation-rmse:0.03824\n",
      "[413]\tvalidation-rmse:0.03806\n",
      "[414]\tvalidation-rmse:0.03791\n",
      "[415]\tvalidation-rmse:0.03775\n",
      "[416]\tvalidation-rmse:0.03758\n",
      "[417]\tvalidation-rmse:0.03745\n",
      "[418]\tvalidation-rmse:0.03728\n",
      "[419]\tvalidation-rmse:0.03717\n",
      "[420]\tvalidation-rmse:0.03701\n",
      "[421]\tvalidation-rmse:0.03687\n",
      "[422]\tvalidation-rmse:0.03672\n",
      "[423]\tvalidation-rmse:0.03657\n",
      "[424]\tvalidation-rmse:0.03641\n",
      "[425]\tvalidation-rmse:0.03627\n",
      "[426]\tvalidation-rmse:0.03612\n",
      "[427]\tvalidation-rmse:0.03597\n",
      "[428]\tvalidation-rmse:0.03584\n",
      "[429]\tvalidation-rmse:0.03569\n",
      "[430]\tvalidation-rmse:0.03554\n",
      "[431]\tvalidation-rmse:0.03541\n",
      "[432]\tvalidation-rmse:0.03529\n",
      "[433]\tvalidation-rmse:0.03516\n",
      "[434]\tvalidation-rmse:0.03504\n",
      "[435]\tvalidation-rmse:0.03494\n",
      "[436]\tvalidation-rmse:0.03482\n",
      "[437]\tvalidation-rmse:0.03470\n",
      "[438]\tvalidation-rmse:0.03458\n",
      "[439]\tvalidation-rmse:0.03446\n",
      "[440]\tvalidation-rmse:0.03436\n",
      "[441]\tvalidation-rmse:0.03422\n",
      "[442]\tvalidation-rmse:0.03410\n",
      "[443]\tvalidation-rmse:0.03398\n",
      "[444]\tvalidation-rmse:0.03388\n",
      "[445]\tvalidation-rmse:0.03376\n",
      "[446]\tvalidation-rmse:0.03366\n",
      "[447]\tvalidation-rmse:0.03354\n",
      "[448]\tvalidation-rmse:0.03343\n",
      "[449]\tvalidation-rmse:0.03333\n",
      "[450]\tvalidation-rmse:0.03322\n",
      "[451]\tvalidation-rmse:0.03311\n",
      "[452]\tvalidation-rmse:0.03303\n",
      "[453]\tvalidation-rmse:0.03294\n",
      "[454]\tvalidation-rmse:0.03285\n",
      "[455]\tvalidation-rmse:0.03274\n",
      "[456]\tvalidation-rmse:0.03265\n",
      "[457]\tvalidation-rmse:0.03256\n",
      "[458]\tvalidation-rmse:0.03245\n",
      "[459]\tvalidation-rmse:0.03236\n",
      "[460]\tvalidation-rmse:0.03226\n",
      "[461]\tvalidation-rmse:0.03216\n",
      "[462]\tvalidation-rmse:0.03207\n",
      "[463]\tvalidation-rmse:0.03199\n",
      "[464]\tvalidation-rmse:0.03190\n",
      "[465]\tvalidation-rmse:0.03180\n",
      "[466]\tvalidation-rmse:0.03171\n",
      "[467]\tvalidation-rmse:0.03164\n",
      "[468]\tvalidation-rmse:0.03155\n",
      "[469]\tvalidation-rmse:0.03146\n",
      "[470]\tvalidation-rmse:0.03137\n",
      "[471]\tvalidation-rmse:0.03130\n",
      "[472]\tvalidation-rmse:0.03121\n",
      "[473]\tvalidation-rmse:0.03113\n",
      "[474]\tvalidation-rmse:0.03106\n",
      "[475]\tvalidation-rmse:0.03097\n",
      "[476]\tvalidation-rmse:0.03089\n",
      "[477]\tvalidation-rmse:0.03081\n",
      "[478]\tvalidation-rmse:0.03074\n",
      "[479]\tvalidation-rmse:0.03066\n",
      "[480]\tvalidation-rmse:0.03059\n",
      "[481]\tvalidation-rmse:0.03051\n",
      "[482]\tvalidation-rmse:0.03044\n",
      "[483]\tvalidation-rmse:0.03036\n",
      "[484]\tvalidation-rmse:0.03029\n",
      "[485]\tvalidation-rmse:0.03023\n",
      "[486]\tvalidation-rmse:0.03017\n",
      "[487]\tvalidation-rmse:0.03010\n",
      "[488]\tvalidation-rmse:0.03004\n",
      "[489]\tvalidation-rmse:0.02997\n",
      "[490]\tvalidation-rmse:0.02991\n",
      "[491]\tvalidation-rmse:0.02985\n",
      "[492]\tvalidation-rmse:0.02979\n",
      "[493]\tvalidation-rmse:0.02974\n",
      "[494]\tvalidation-rmse:0.02967\n",
      "[495]\tvalidation-rmse:0.02960\n",
      "[496]\tvalidation-rmse:0.02955\n",
      "[497]\tvalidation-rmse:0.02949\n",
      "[498]\tvalidation-rmse:0.02942\n",
      "[499]\tvalidation-rmse:0.02937\n",
      "[500]\tvalidation-rmse:0.02932\n",
      "[501]\tvalidation-rmse:0.02927\n",
      "[502]\tvalidation-rmse:0.02921\n",
      "[503]\tvalidation-rmse:0.02914\n",
      "[504]\tvalidation-rmse:0.02909\n",
      "[505]\tvalidation-rmse:0.02904\n",
      "[506]\tvalidation-rmse:0.02898\n",
      "[507]\tvalidation-rmse:0.02893\n",
      "[508]\tvalidation-rmse:0.02888\n",
      "[509]\tvalidation-rmse:0.02883\n",
      "[510]\tvalidation-rmse:0.02877\n",
      "[511]\tvalidation-rmse:0.02873\n",
      "[512]\tvalidation-rmse:0.02868\n",
      "[513]\tvalidation-rmse:0.02864\n",
      "[514]\tvalidation-rmse:0.02859\n",
      "[515]\tvalidation-rmse:0.02855\n",
      "[516]\tvalidation-rmse:0.02851\n",
      "[517]\tvalidation-rmse:0.02846\n",
      "[518]\tvalidation-rmse:0.02841\n",
      "[519]\tvalidation-rmse:0.02836\n",
      "[520]\tvalidation-rmse:0.02832\n",
      "[521]\tvalidation-rmse:0.02827\n",
      "[522]\tvalidation-rmse:0.02825\n",
      "[523]\tvalidation-rmse:0.02821\n",
      "[524]\tvalidation-rmse:0.02817\n",
      "[525]\tvalidation-rmse:0.02812\n",
      "[526]\tvalidation-rmse:0.02808\n",
      "[527]\tvalidation-rmse:0.02804\n",
      "[528]\tvalidation-rmse:0.02800\n",
      "[529]\tvalidation-rmse:0.02796\n",
      "[530]\tvalidation-rmse:0.02793\n",
      "[531]\tvalidation-rmse:0.02788\n",
      "[532]\tvalidation-rmse:0.02785\n",
      "[533]\tvalidation-rmse:0.02781\n",
      "[534]\tvalidation-rmse:0.02777\n",
      "[535]\tvalidation-rmse:0.02774\n",
      "[536]\tvalidation-rmse:0.02769\n",
      "[537]\tvalidation-rmse:0.02765\n",
      "[538]\tvalidation-rmse:0.02762\n",
      "[539]\tvalidation-rmse:0.02759\n",
      "[540]\tvalidation-rmse:0.02755\n",
      "[541]\tvalidation-rmse:0.02751\n",
      "[542]\tvalidation-rmse:0.02747\n",
      "[543]\tvalidation-rmse:0.02744\n",
      "[544]\tvalidation-rmse:0.02740\n",
      "[545]\tvalidation-rmse:0.02737\n",
      "[546]\tvalidation-rmse:0.02734\n",
      "[547]\tvalidation-rmse:0.02730\n",
      "[548]\tvalidation-rmse:0.02727\n",
      "[549]\tvalidation-rmse:0.02723\n",
      "[550]\tvalidation-rmse:0.02721\n",
      "[551]\tvalidation-rmse:0.02718\n",
      "[552]\tvalidation-rmse:0.02715\n",
      "[553]\tvalidation-rmse:0.02712\n",
      "[554]\tvalidation-rmse:0.02709\n",
      "[555]\tvalidation-rmse:0.02706\n",
      "[556]\tvalidation-rmse:0.02703\n",
      "[557]\tvalidation-rmse:0.02701\n",
      "[558]\tvalidation-rmse:0.02697\n",
      "[559]\tvalidation-rmse:0.02695\n",
      "[560]\tvalidation-rmse:0.02693\n",
      "[561]\tvalidation-rmse:0.02689\n",
      "[562]\tvalidation-rmse:0.02686\n",
      "[563]\tvalidation-rmse:0.02683\n",
      "[564]\tvalidation-rmse:0.02680\n",
      "[565]\tvalidation-rmse:0.02677\n",
      "[566]\tvalidation-rmse:0.02674\n",
      "[567]\tvalidation-rmse:0.02671\n",
      "[568]\tvalidation-rmse:0.02669\n",
      "[569]\tvalidation-rmse:0.02666\n",
      "[570]\tvalidation-rmse:0.02664\n",
      "[571]\tvalidation-rmse:0.02661\n",
      "[572]\tvalidation-rmse:0.02658\n",
      "[573]\tvalidation-rmse:0.02656\n",
      "[574]\tvalidation-rmse:0.02653\n",
      "[575]\tvalidation-rmse:0.02651\n",
      "[576]\tvalidation-rmse:0.02649\n",
      "[577]\tvalidation-rmse:0.02647\n",
      "[578]\tvalidation-rmse:0.02644\n",
      "[579]\tvalidation-rmse:0.02642\n",
      "[580]\tvalidation-rmse:0.02639\n",
      "[581]\tvalidation-rmse:0.02637\n",
      "[582]\tvalidation-rmse:0.02634\n",
      "[583]\tvalidation-rmse:0.02632\n",
      "[584]\tvalidation-rmse:0.02630\n",
      "[585]\tvalidation-rmse:0.02628\n",
      "[586]\tvalidation-rmse:0.02627\n",
      "[587]\tvalidation-rmse:0.02625\n",
      "[588]\tvalidation-rmse:0.02623\n",
      "[589]\tvalidation-rmse:0.02621\n",
      "[590]\tvalidation-rmse:0.02618\n",
      "[591]\tvalidation-rmse:0.02616\n",
      "[592]\tvalidation-rmse:0.02614\n",
      "[593]\tvalidation-rmse:0.02612\n",
      "[594]\tvalidation-rmse:0.02610\n",
      "[595]\tvalidation-rmse:0.02609\n",
      "[596]\tvalidation-rmse:0.02607\n",
      "[597]\tvalidation-rmse:0.02606\n",
      "[598]\tvalidation-rmse:0.02604\n",
      "[599]\tvalidation-rmse:0.02602\n",
      "[600]\tvalidation-rmse:0.02601\n",
      "[601]\tvalidation-rmse:0.02599\n",
      "[602]\tvalidation-rmse:0.02598\n",
      "[603]\tvalidation-rmse:0.02597\n",
      "[604]\tvalidation-rmse:0.02595\n",
      "[605]\tvalidation-rmse:0.02593\n",
      "[606]\tvalidation-rmse:0.02591\n",
      "[607]\tvalidation-rmse:0.02590\n",
      "[608]\tvalidation-rmse:0.02589\n",
      "[609]\tvalidation-rmse:0.02588\n",
      "[610]\tvalidation-rmse:0.02586\n",
      "[611]\tvalidation-rmse:0.02584\n",
      "[612]\tvalidation-rmse:0.02582\n",
      "[613]\tvalidation-rmse:0.02581\n",
      "[614]\tvalidation-rmse:0.02580\n",
      "[615]\tvalidation-rmse:0.02579\n",
      "[616]\tvalidation-rmse:0.02578\n",
      "[617]\tvalidation-rmse:0.02576\n",
      "[618]\tvalidation-rmse:0.02575\n",
      "[619]\tvalidation-rmse:0.02574\n",
      "[620]\tvalidation-rmse:0.02573\n",
      "[621]\tvalidation-rmse:0.02571\n",
      "[622]\tvalidation-rmse:0.02569\n",
      "[623]\tvalidation-rmse:0.02568\n",
      "[624]\tvalidation-rmse:0.02567\n",
      "[625]\tvalidation-rmse:0.02566\n",
      "[626]\tvalidation-rmse:0.02564\n",
      "[627]\tvalidation-rmse:0.02562\n",
      "[628]\tvalidation-rmse:0.02561\n",
      "[629]\tvalidation-rmse:0.02560\n",
      "[630]\tvalidation-rmse:0.02558\n",
      "[631]\tvalidation-rmse:0.02556\n",
      "[632]\tvalidation-rmse:0.02554\n",
      "[633]\tvalidation-rmse:0.02553\n",
      "[634]\tvalidation-rmse:0.02552\n",
      "[635]\tvalidation-rmse:0.02551\n",
      "[636]\tvalidation-rmse:0.02549\n",
      "[637]\tvalidation-rmse:0.02548\n",
      "[638]\tvalidation-rmse:0.02547\n",
      "[639]\tvalidation-rmse:0.02546\n",
      "[640]\tvalidation-rmse:0.02544\n",
      "[641]\tvalidation-rmse:0.02544\n",
      "[642]\tvalidation-rmse:0.02542\n",
      "[643]\tvalidation-rmse:0.02542\n",
      "[644]\tvalidation-rmse:0.02540\n",
      "[645]\tvalidation-rmse:0.02540\n",
      "[646]\tvalidation-rmse:0.02538\n",
      "[647]\tvalidation-rmse:0.02537\n",
      "[648]\tvalidation-rmse:0.02536\n",
      "[649]\tvalidation-rmse:0.02535\n",
      "[650]\tvalidation-rmse:0.02533\n",
      "[651]\tvalidation-rmse:0.02533\n",
      "[652]\tvalidation-rmse:0.02532\n",
      "[653]\tvalidation-rmse:0.02531\n",
      "[654]\tvalidation-rmse:0.02531\n",
      "[655]\tvalidation-rmse:0.02530\n",
      "[656]\tvalidation-rmse:0.02529\n",
      "[657]\tvalidation-rmse:0.02528\n",
      "[658]\tvalidation-rmse:0.02527\n",
      "[659]\tvalidation-rmse:0.02526\n",
      "[660]\tvalidation-rmse:0.02524\n",
      "[661]\tvalidation-rmse:0.02523\n",
      "[662]\tvalidation-rmse:0.02522\n",
      "[663]\tvalidation-rmse:0.02521\n",
      "[664]\tvalidation-rmse:0.02520\n",
      "[665]\tvalidation-rmse:0.02519\n",
      "[666]\tvalidation-rmse:0.02518\n",
      "[667]\tvalidation-rmse:0.02517\n",
      "[668]\tvalidation-rmse:0.02517\n",
      "[669]\tvalidation-rmse:0.02516\n",
      "[670]\tvalidation-rmse:0.02514\n",
      "[671]\tvalidation-rmse:0.02514\n",
      "[672]\tvalidation-rmse:0.02513\n",
      "[673]\tvalidation-rmse:0.02511\n",
      "[674]\tvalidation-rmse:0.02510\n",
      "[675]\tvalidation-rmse:0.02509\n",
      "[676]\tvalidation-rmse:0.02509\n",
      "[677]\tvalidation-rmse:0.02508\n",
      "[678]\tvalidation-rmse:0.02507\n",
      "[679]\tvalidation-rmse:0.02505\n",
      "[680]\tvalidation-rmse:0.02505\n",
      "[681]\tvalidation-rmse:0.02504\n",
      "[682]\tvalidation-rmse:0.02503\n",
      "[683]\tvalidation-rmse:0.02502\n",
      "[684]\tvalidation-rmse:0.02501\n",
      "[685]\tvalidation-rmse:0.02500\n",
      "[686]\tvalidation-rmse:0.02499\n",
      "[687]\tvalidation-rmse:0.02498\n",
      "[688]\tvalidation-rmse:0.02496\n",
      "[689]\tvalidation-rmse:0.02496\n",
      "[690]\tvalidation-rmse:0.02494\n",
      "[691]\tvalidation-rmse:0.02493\n",
      "[692]\tvalidation-rmse:0.02493\n",
      "[693]\tvalidation-rmse:0.02492\n",
      "[694]\tvalidation-rmse:0.02492\n",
      "[695]\tvalidation-rmse:0.02490\n",
      "[696]\tvalidation-rmse:0.02490\n",
      "[697]\tvalidation-rmse:0.02488\n",
      "[698]\tvalidation-rmse:0.02487\n",
      "[699]\tvalidation-rmse:0.02487\n",
      "[700]\tvalidation-rmse:0.02487\n",
      "[701]\tvalidation-rmse:0.02486\n",
      "[702]\tvalidation-rmse:0.02485\n",
      "[703]\tvalidation-rmse:0.02484\n",
      "[704]\tvalidation-rmse:0.02483\n",
      "[705]\tvalidation-rmse:0.02483\n",
      "[706]\tvalidation-rmse:0.02482\n",
      "[707]\tvalidation-rmse:0.02481\n",
      "[708]\tvalidation-rmse:0.02480\n",
      "[709]\tvalidation-rmse:0.02480\n",
      "[710]\tvalidation-rmse:0.02479\n",
      "[711]\tvalidation-rmse:0.02478\n",
      "[712]\tvalidation-rmse:0.02477\n",
      "[713]\tvalidation-rmse:0.02477\n",
      "[714]\tvalidation-rmse:0.02475\n",
      "[715]\tvalidation-rmse:0.02475\n",
      "[716]\tvalidation-rmse:0.02474\n",
      "[717]\tvalidation-rmse:0.02473\n",
      "[718]\tvalidation-rmse:0.02473\n",
      "[719]\tvalidation-rmse:0.02472\n",
      "[720]\tvalidation-rmse:0.02471\n",
      "[721]\tvalidation-rmse:0.02470\n",
      "[722]\tvalidation-rmse:0.02469\n",
      "[723]\tvalidation-rmse:0.02468\n",
      "[724]\tvalidation-rmse:0.02467\n",
      "[725]\tvalidation-rmse:0.02467\n",
      "[726]\tvalidation-rmse:0.02466\n",
      "[727]\tvalidation-rmse:0.02464\n",
      "[728]\tvalidation-rmse:0.02464\n",
      "[729]\tvalidation-rmse:0.02463\n",
      "[730]\tvalidation-rmse:0.02463\n",
      "[731]\tvalidation-rmse:0.02462\n",
      "[732]\tvalidation-rmse:0.02461\n",
      "[733]\tvalidation-rmse:0.02461\n",
      "[734]\tvalidation-rmse:0.02459\n",
      "[735]\tvalidation-rmse:0.02459\n",
      "[736]\tvalidation-rmse:0.02459\n",
      "[737]\tvalidation-rmse:0.02458\n",
      "[738]\tvalidation-rmse:0.02457\n",
      "[739]\tvalidation-rmse:0.02456\n",
      "[740]\tvalidation-rmse:0.02456\n",
      "[741]\tvalidation-rmse:0.02455\n",
      "[742]\tvalidation-rmse:0.02454\n",
      "[743]\tvalidation-rmse:0.02454\n",
      "[744]\tvalidation-rmse:0.02453\n",
      "[745]\tvalidation-rmse:0.02452\n",
      "[746]\tvalidation-rmse:0.02452\n",
      "[747]\tvalidation-rmse:0.02451\n",
      "[748]\tvalidation-rmse:0.02451\n",
      "[749]\tvalidation-rmse:0.02450\n",
      "[750]\tvalidation-rmse:0.02449\n",
      "[751]\tvalidation-rmse:0.02449\n",
      "[752]\tvalidation-rmse:0.02448\n",
      "[753]\tvalidation-rmse:0.02448\n",
      "[754]\tvalidation-rmse:0.02448\n",
      "[755]\tvalidation-rmse:0.02447\n",
      "[756]\tvalidation-rmse:0.02447\n",
      "[757]\tvalidation-rmse:0.02446\n",
      "[758]\tvalidation-rmse:0.02446\n",
      "[759]\tvalidation-rmse:0.02445\n",
      "[760]\tvalidation-rmse:0.02445\n",
      "[761]\tvalidation-rmse:0.02445\n",
      "[762]\tvalidation-rmse:0.02444\n",
      "[763]\tvalidation-rmse:0.02444\n",
      "[764]\tvalidation-rmse:0.02443\n",
      "[765]\tvalidation-rmse:0.02443\n",
      "[766]\tvalidation-rmse:0.02442\n",
      "[767]\tvalidation-rmse:0.02442\n",
      "[768]\tvalidation-rmse:0.02441\n",
      "[769]\tvalidation-rmse:0.02440\n",
      "[770]\tvalidation-rmse:0.02439\n",
      "[771]\tvalidation-rmse:0.02439\n",
      "[772]\tvalidation-rmse:0.02438\n",
      "[773]\tvalidation-rmse:0.02438\n",
      "[774]\tvalidation-rmse:0.02438\n",
      "[775]\tvalidation-rmse:0.02437\n",
      "[776]\tvalidation-rmse:0.02437\n",
      "[777]\tvalidation-rmse:0.02436\n",
      "[778]\tvalidation-rmse:0.02436\n",
      "[779]\tvalidation-rmse:0.02435\n",
      "[780]\tvalidation-rmse:0.02434\n",
      "[781]\tvalidation-rmse:0.02434\n",
      "[782]\tvalidation-rmse:0.02434\n",
      "[783]\tvalidation-rmse:0.02434\n",
      "[784]\tvalidation-rmse:0.02433\n",
      "[785]\tvalidation-rmse:0.02433\n",
      "[786]\tvalidation-rmse:0.02432\n",
      "[787]\tvalidation-rmse:0.02431\n",
      "[788]\tvalidation-rmse:0.02431\n",
      "[789]\tvalidation-rmse:0.02431\n",
      "[790]\tvalidation-rmse:0.02430\n",
      "[791]\tvalidation-rmse:0.02430\n",
      "[792]\tvalidation-rmse:0.02430\n",
      "[793]\tvalidation-rmse:0.02430\n",
      "[794]\tvalidation-rmse:0.02430\n",
      "[795]\tvalidation-rmse:0.02429\n",
      "[796]\tvalidation-rmse:0.02429\n",
      "[797]\tvalidation-rmse:0.02428\n",
      "[798]\tvalidation-rmse:0.02428\n",
      "[799]\tvalidation-rmse:0.02427\n",
      "[800]\tvalidation-rmse:0.02427\n",
      "[801]\tvalidation-rmse:0.02427\n",
      "[802]\tvalidation-rmse:0.02426\n",
      "[803]\tvalidation-rmse:0.02426\n",
      "[804]\tvalidation-rmse:0.02425\n",
      "[805]\tvalidation-rmse:0.02425\n",
      "[806]\tvalidation-rmse:0.02424\n",
      "[807]\tvalidation-rmse:0.02424\n",
      "[808]\tvalidation-rmse:0.02424\n",
      "[809]\tvalidation-rmse:0.02424\n",
      "[810]\tvalidation-rmse:0.02423\n",
      "[811]\tvalidation-rmse:0.02423\n",
      "[812]\tvalidation-rmse:0.02423\n",
      "[813]\tvalidation-rmse:0.02423\n",
      "[814]\tvalidation-rmse:0.02422\n",
      "[815]\tvalidation-rmse:0.02422\n",
      "[816]\tvalidation-rmse:0.02422\n",
      "[817]\tvalidation-rmse:0.02421\n",
      "[818]\tvalidation-rmse:0.02421\n",
      "[819]\tvalidation-rmse:0.02421\n",
      "[820]\tvalidation-rmse:0.02420\n",
      "[821]\tvalidation-rmse:0.02421\n",
      "[822]\tvalidation-rmse:0.02420\n",
      "[823]\tvalidation-rmse:0.02420\n",
      "[824]\tvalidation-rmse:0.02420\n",
      "[825]\tvalidation-rmse:0.02420\n",
      "[826]\tvalidation-rmse:0.02419\n",
      "[827]\tvalidation-rmse:0.02419\n",
      "[828]\tvalidation-rmse:0.02418\n",
      "[829]\tvalidation-rmse:0.02418\n",
      "[830]\tvalidation-rmse:0.02418\n",
      "[831]\tvalidation-rmse:0.02417\n",
      "[832]\tvalidation-rmse:0.02418\n",
      "[833]\tvalidation-rmse:0.02417\n",
      "[834]\tvalidation-rmse:0.02417\n",
      "[835]\tvalidation-rmse:0.02416\n",
      "[836]\tvalidation-rmse:0.02416\n",
      "[837]\tvalidation-rmse:0.02416\n",
      "[838]\tvalidation-rmse:0.02416\n",
      "[839]\tvalidation-rmse:0.02415\n",
      "[840]\tvalidation-rmse:0.02415\n",
      "[841]\tvalidation-rmse:0.02415\n",
      "[842]\tvalidation-rmse:0.02415\n",
      "[843]\tvalidation-rmse:0.02415\n",
      "[844]\tvalidation-rmse:0.02415\n",
      "[845]\tvalidation-rmse:0.02414\n",
      "[846]\tvalidation-rmse:0.02414\n",
      "[847]\tvalidation-rmse:0.02414\n",
      "[848]\tvalidation-rmse:0.02413\n",
      "[849]\tvalidation-rmse:0.02413\n",
      "[850]\tvalidation-rmse:0.02412\n",
      "[851]\tvalidation-rmse:0.02412\n",
      "[852]\tvalidation-rmse:0.02412\n",
      "[853]\tvalidation-rmse:0.02412\n",
      "[854]\tvalidation-rmse:0.02412\n",
      "[855]\tvalidation-rmse:0.02411\n",
      "[856]\tvalidation-rmse:0.02409\n",
      "[857]\tvalidation-rmse:0.02410\n",
      "[858]\tvalidation-rmse:0.02409\n",
      "[859]\tvalidation-rmse:0.02409\n",
      "[860]\tvalidation-rmse:0.02408\n",
      "[861]\tvalidation-rmse:0.02408\n",
      "[862]\tvalidation-rmse:0.02408\n",
      "[863]\tvalidation-rmse:0.02407\n",
      "[864]\tvalidation-rmse:0.02407\n",
      "[865]\tvalidation-rmse:0.02407\n",
      "[866]\tvalidation-rmse:0.02407\n",
      "[867]\tvalidation-rmse:0.02407\n",
      "[868]\tvalidation-rmse:0.02407\n",
      "[869]\tvalidation-rmse:0.02406\n",
      "[870]\tvalidation-rmse:0.02406\n",
      "[871]\tvalidation-rmse:0.02405\n",
      "[872]\tvalidation-rmse:0.02405\n",
      "[873]\tvalidation-rmse:0.02405\n",
      "[874]\tvalidation-rmse:0.02405\n",
      "[875]\tvalidation-rmse:0.02405\n",
      "[876]\tvalidation-rmse:0.02404\n",
      "[877]\tvalidation-rmse:0.02404\n",
      "[878]\tvalidation-rmse:0.02403\n",
      "[879]\tvalidation-rmse:0.02403\n",
      "[880]\tvalidation-rmse:0.02402\n",
      "[881]\tvalidation-rmse:0.02402\n",
      "[882]\tvalidation-rmse:0.02402\n",
      "[883]\tvalidation-rmse:0.02401\n",
      "[884]\tvalidation-rmse:0.02401\n",
      "[885]\tvalidation-rmse:0.02400\n",
      "[886]\tvalidation-rmse:0.02400\n",
      "[887]\tvalidation-rmse:0.02400\n",
      "[888]\tvalidation-rmse:0.02400\n",
      "[889]\tvalidation-rmse:0.02400\n",
      "[890]\tvalidation-rmse:0.02399\n",
      "[891]\tvalidation-rmse:0.02399\n",
      "[892]\tvalidation-rmse:0.02399\n",
      "[893]\tvalidation-rmse:0.02398\n",
      "[894]\tvalidation-rmse:0.02398\n",
      "[895]\tvalidation-rmse:0.02398\n",
      "[896]\tvalidation-rmse:0.02398\n",
      "[897]\tvalidation-rmse:0.02397\n",
      "[898]\tvalidation-rmse:0.02397\n",
      "[899]\tvalidation-rmse:0.02397\n",
      "[900]\tvalidation-rmse:0.02396\n",
      "[901]\tvalidation-rmse:0.02396\n",
      "[902]\tvalidation-rmse:0.02396\n",
      "[903]\tvalidation-rmse:0.02396\n",
      "[904]\tvalidation-rmse:0.02396\n",
      "[905]\tvalidation-rmse:0.02395\n",
      "[906]\tvalidation-rmse:0.02395\n",
      "[907]\tvalidation-rmse:0.02395\n",
      "[908]\tvalidation-rmse:0.02394\n",
      "[909]\tvalidation-rmse:0.02394\n",
      "[910]\tvalidation-rmse:0.02394\n",
      "[911]\tvalidation-rmse:0.02393\n",
      "[912]\tvalidation-rmse:0.02393\n",
      "[913]\tvalidation-rmse:0.02393\n",
      "[914]\tvalidation-rmse:0.02392\n",
      "[915]\tvalidation-rmse:0.02392\n",
      "[916]\tvalidation-rmse:0.02392\n",
      "[917]\tvalidation-rmse:0.02392\n",
      "[918]\tvalidation-rmse:0.02391\n",
      "[919]\tvalidation-rmse:0.02391\n",
      "[920]\tvalidation-rmse:0.02391\n",
      "[921]\tvalidation-rmse:0.02391\n",
      "[922]\tvalidation-rmse:0.02390\n",
      "[923]\tvalidation-rmse:0.02390\n",
      "[924]\tvalidation-rmse:0.02390\n",
      "[925]\tvalidation-rmse:0.02390\n",
      "[926]\tvalidation-rmse:0.02389\n",
      "[927]\tvalidation-rmse:0.02389\n",
      "[928]\tvalidation-rmse:0.02389\n",
      "[929]\tvalidation-rmse:0.02388\n",
      "[930]\tvalidation-rmse:0.02388\n",
      "[931]\tvalidation-rmse:0.02388\n",
      "[932]\tvalidation-rmse:0.02387\n",
      "[933]\tvalidation-rmse:0.02387\n",
      "[934]\tvalidation-rmse:0.02387\n",
      "[935]\tvalidation-rmse:0.02387\n",
      "[936]\tvalidation-rmse:0.02387\n",
      "[937]\tvalidation-rmse:0.02387\n",
      "[938]\tvalidation-rmse:0.02387\n",
      "[939]\tvalidation-rmse:0.02387\n",
      "[940]\tvalidation-rmse:0.02386\n",
      "[941]\tvalidation-rmse:0.02386\n",
      "[942]\tvalidation-rmse:0.02386\n",
      "[943]\tvalidation-rmse:0.02385\n",
      "[944]\tvalidation-rmse:0.02385\n",
      "[945]\tvalidation-rmse:0.02385\n",
      "[946]\tvalidation-rmse:0.02384\n",
      "[947]\tvalidation-rmse:0.02384\n",
      "[948]\tvalidation-rmse:0.02384\n",
      "[949]\tvalidation-rmse:0.02384\n",
      "[950]\tvalidation-rmse:0.02384\n",
      "[951]\tvalidation-rmse:0.02383\n",
      "[952]\tvalidation-rmse:0.02383\n",
      "[953]\tvalidation-rmse:0.02383\n",
      "[954]\tvalidation-rmse:0.02383\n",
      "[955]\tvalidation-rmse:0.02382\n",
      "[956]\tvalidation-rmse:0.02382\n",
      "[957]\tvalidation-rmse:0.02382\n",
      "[958]\tvalidation-rmse:0.02382\n",
      "[959]\tvalidation-rmse:0.02382\n",
      "[960]\tvalidation-rmse:0.02382\n",
      "[961]\tvalidation-rmse:0.02382\n",
      "[962]\tvalidation-rmse:0.02381\n",
      "[963]\tvalidation-rmse:0.02381\n",
      "[964]\tvalidation-rmse:0.02381\n",
      "[965]\tvalidation-rmse:0.02381\n",
      "[966]\tvalidation-rmse:0.02380\n",
      "[967]\tvalidation-rmse:0.02380\n",
      "[968]\tvalidation-rmse:0.02380\n",
      "[969]\tvalidation-rmse:0.02380\n",
      "[970]\tvalidation-rmse:0.02379\n",
      "[971]\tvalidation-rmse:0.02379\n",
      "[972]\tvalidation-rmse:0.02379\n",
      "[973]\tvalidation-rmse:0.02379\n",
      "[974]\tvalidation-rmse:0.02378\n",
      "[975]\tvalidation-rmse:0.02379\n",
      "[976]\tvalidation-rmse:0.02378\n",
      "[977]\tvalidation-rmse:0.02378\n",
      "[978]\tvalidation-rmse:0.02378\n",
      "[979]\tvalidation-rmse:0.02377\n",
      "[980]\tvalidation-rmse:0.02377\n",
      "[981]\tvalidation-rmse:0.02377\n",
      "[982]\tvalidation-rmse:0.02377\n",
      "[983]\tvalidation-rmse:0.02376\n",
      "[984]\tvalidation-rmse:0.02376\n",
      "[985]\tvalidation-rmse:0.02376\n",
      "[986]\tvalidation-rmse:0.02376\n",
      "[987]\tvalidation-rmse:0.02376\n",
      "[988]\tvalidation-rmse:0.02375\n",
      "[989]\tvalidation-rmse:0.02375\n",
      "[990]\tvalidation-rmse:0.02375\n",
      "[991]\tvalidation-rmse:0.02375\n",
      "[992]\tvalidation-rmse:0.02374\n",
      "[993]\tvalidation-rmse:0.02374\n",
      "[994]\tvalidation-rmse:0.02374\n",
      "[995]\tvalidation-rmse:0.02374\n",
      "[996]\tvalidation-rmse:0.02374\n",
      "[997]\tvalidation-rmse:0.02374\n",
      "[998]\tvalidation-rmse:0.02373\n",
      "[999]\tvalidation-rmse:0.02373\n",
      "[1000]\tvalidation-rmse:0.02373\n",
      "[1001]\tvalidation-rmse:0.02373\n",
      "[1002]\tvalidation-rmse:0.02373\n",
      "[1003]\tvalidation-rmse:0.02372\n",
      "[1004]\tvalidation-rmse:0.02372\n",
      "[1005]\tvalidation-rmse:0.02372\n",
      "[1006]\tvalidation-rmse:0.02372\n",
      "[1007]\tvalidation-rmse:0.02371\n",
      "[1008]\tvalidation-rmse:0.02371\n",
      "[1009]\tvalidation-rmse:0.02372\n",
      "[1010]\tvalidation-rmse:0.02371\n",
      "[1011]\tvalidation-rmse:0.02371\n",
      "[1012]\tvalidation-rmse:0.02371\n",
      "[1013]\tvalidation-rmse:0.02371\n",
      "[1014]\tvalidation-rmse:0.02371\n",
      "[1015]\tvalidation-rmse:0.02371\n",
      "[1016]\tvalidation-rmse:0.02371\n",
      "[1017]\tvalidation-rmse:0.02371\n",
      "[1018]\tvalidation-rmse:0.02371\n",
      "[1019]\tvalidation-rmse:0.02371\n",
      "[1020]\tvalidation-rmse:0.02371\n",
      "[1021]\tvalidation-rmse:0.02371\n",
      "[1022]\tvalidation-rmse:0.02371\n",
      "[1023]\tvalidation-rmse:0.02371\n",
      "[1024]\tvalidation-rmse:0.02370\n",
      "[1025]\tvalidation-rmse:0.02370\n",
      "[1026]\tvalidation-rmse:0.02370\n",
      "[1027]\tvalidation-rmse:0.02370\n",
      "[1028]\tvalidation-rmse:0.02370\n",
      "[1029]\tvalidation-rmse:0.02370\n",
      "[1030]\tvalidation-rmse:0.02370\n",
      "[1031]\tvalidation-rmse:0.02370\n",
      "[1032]\tvalidation-rmse:0.02370\n",
      "[1033]\tvalidation-rmse:0.02370\n",
      "[1034]\tvalidation-rmse:0.02370\n",
      "[1035]\tvalidation-rmse:0.02370\n",
      "[1036]\tvalidation-rmse:0.02369\n",
      "[1037]\tvalidation-rmse:0.02369\n",
      "[1038]\tvalidation-rmse:0.02369\n",
      "[1039]\tvalidation-rmse:0.02369\n",
      "[1040]\tvalidation-rmse:0.02369\n",
      "[1041]\tvalidation-rmse:0.02369\n",
      "[1042]\tvalidation-rmse:0.02368\n",
      "[1043]\tvalidation-rmse:0.02368\n",
      "[1044]\tvalidation-rmse:0.02369\n",
      "[1045]\tvalidation-rmse:0.02368\n",
      "[1046]\tvalidation-rmse:0.02368\n",
      "[1047]\tvalidation-rmse:0.02368\n",
      "[1048]\tvalidation-rmse:0.02368\n",
      "[1049]\tvalidation-rmse:0.02368\n",
      "[1050]\tvalidation-rmse:0.02367\n",
      "[1051]\tvalidation-rmse:0.02367\n",
      "[1052]\tvalidation-rmse:0.02367\n",
      "[1053]\tvalidation-rmse:0.02367\n",
      "[1054]\tvalidation-rmse:0.02367\n",
      "[1055]\tvalidation-rmse:0.02366\n",
      "[1056]\tvalidation-rmse:0.02366\n",
      "[1057]\tvalidation-rmse:0.02366\n",
      "[1058]\tvalidation-rmse:0.02366\n",
      "[1059]\tvalidation-rmse:0.02366\n",
      "[1060]\tvalidation-rmse:0.02366\n",
      "[1061]\tvalidation-rmse:0.02365\n",
      "[1062]\tvalidation-rmse:0.02365\n",
      "[1063]\tvalidation-rmse:0.02365\n",
      "[1064]\tvalidation-rmse:0.02364\n",
      "[1065]\tvalidation-rmse:0.02364\n",
      "[1066]\tvalidation-rmse:0.02364\n",
      "[1067]\tvalidation-rmse:0.02364\n",
      "[1068]\tvalidation-rmse:0.02364\n",
      "[1069]\tvalidation-rmse:0.02363\n",
      "[1070]\tvalidation-rmse:0.02363\n",
      "[1071]\tvalidation-rmse:0.02363\n",
      "[1072]\tvalidation-rmse:0.02362\n",
      "[1073]\tvalidation-rmse:0.02362\n",
      "[1074]\tvalidation-rmse:0.02362\n",
      "[1075]\tvalidation-rmse:0.02362\n",
      "[1076]\tvalidation-rmse:0.02362\n",
      "[1077]\tvalidation-rmse:0.02362\n",
      "[1078]\tvalidation-rmse:0.02361\n",
      "[1079]\tvalidation-rmse:0.02361\n",
      "[1080]\tvalidation-rmse:0.02361\n",
      "[1081]\tvalidation-rmse:0.02361\n",
      "[1082]\tvalidation-rmse:0.02361\n",
      "[1083]\tvalidation-rmse:0.02361\n",
      "[1084]\tvalidation-rmse:0.02361\n",
      "[1085]\tvalidation-rmse:0.02361\n",
      "[1086]\tvalidation-rmse:0.02361\n",
      "[1087]\tvalidation-rmse:0.02360\n",
      "[1088]\tvalidation-rmse:0.02360\n",
      "[1089]\tvalidation-rmse:0.02360\n",
      "[1090]\tvalidation-rmse:0.02360\n",
      "[1091]\tvalidation-rmse:0.02360\n",
      "[1092]\tvalidation-rmse:0.02359\n",
      "[1093]\tvalidation-rmse:0.02359\n",
      "[1094]\tvalidation-rmse:0.02359\n",
      "[1095]\tvalidation-rmse:0.02359\n",
      "[1096]\tvalidation-rmse:0.02359\n",
      "[1097]\tvalidation-rmse:0.02359\n",
      "[1098]\tvalidation-rmse:0.02358\n",
      "[1099]\tvalidation-rmse:0.02358\n",
      "[1100]\tvalidation-rmse:0.02358\n",
      "[1101]\tvalidation-rmse:0.02358\n",
      "[1102]\tvalidation-rmse:0.02358\n",
      "[1103]\tvalidation-rmse:0.02358\n",
      "[1104]\tvalidation-rmse:0.02357\n",
      "[1105]\tvalidation-rmse:0.02357\n",
      "[1106]\tvalidation-rmse:0.02357\n",
      "[1107]\tvalidation-rmse:0.02357\n",
      "[1108]\tvalidation-rmse:0.02356\n",
      "[1109]\tvalidation-rmse:0.02357\n",
      "[1110]\tvalidation-rmse:0.02356\n",
      "[1111]\tvalidation-rmse:0.02356\n",
      "[1112]\tvalidation-rmse:0.02356\n",
      "[1113]\tvalidation-rmse:0.02355\n",
      "[1114]\tvalidation-rmse:0.02355\n",
      "[1115]\tvalidation-rmse:0.02355\n",
      "[1116]\tvalidation-rmse:0.02355\n",
      "[1117]\tvalidation-rmse:0.02355\n",
      "[1118]\tvalidation-rmse:0.02355\n",
      "[1119]\tvalidation-rmse:0.02355\n",
      "[1120]\tvalidation-rmse:0.02354\n",
      "[1121]\tvalidation-rmse:0.02354\n",
      "[1122]\tvalidation-rmse:0.02354\n",
      "[1123]\tvalidation-rmse:0.02354\n",
      "[1124]\tvalidation-rmse:0.02354\n",
      "[1125]\tvalidation-rmse:0.02354\n",
      "[1126]\tvalidation-rmse:0.02353\n",
      "[1127]\tvalidation-rmse:0.02353\n",
      "[1128]\tvalidation-rmse:0.02353\n",
      "[1129]\tvalidation-rmse:0.02353\n",
      "[1130]\tvalidation-rmse:0.02353\n",
      "[1131]\tvalidation-rmse:0.02353\n",
      "[1132]\tvalidation-rmse:0.02353\n",
      "[1133]\tvalidation-rmse:0.02353\n",
      "[1134]\tvalidation-rmse:0.02353\n",
      "[1135]\tvalidation-rmse:0.02353\n",
      "[1136]\tvalidation-rmse:0.02353\n",
      "[1137]\tvalidation-rmse:0.02353\n",
      "[1138]\tvalidation-rmse:0.02353\n",
      "[1139]\tvalidation-rmse:0.02353\n",
      "[1140]\tvalidation-rmse:0.02353\n",
      "[1141]\tvalidation-rmse:0.02353\n",
      "[1142]\tvalidation-rmse:0.02353\n",
      "[1143]\tvalidation-rmse:0.02353\n",
      "[1144]\tvalidation-rmse:0.02353\n",
      "[1145]\tvalidation-rmse:0.02353\n",
      "[1146]\tvalidation-rmse:0.02353\n",
      "[1147]\tvalidation-rmse:0.02352\n",
      "[1148]\tvalidation-rmse:0.02352\n",
      "[1149]\tvalidation-rmse:0.02352\n",
      "[1150]\tvalidation-rmse:0.02352\n",
      "[1151]\tvalidation-rmse:0.02352\n",
      "[1152]\tvalidation-rmse:0.02352\n",
      "[1153]\tvalidation-rmse:0.02352\n",
      "[1154]\tvalidation-rmse:0.02352\n",
      "[1155]\tvalidation-rmse:0.02352\n",
      "[1156]\tvalidation-rmse:0.02351\n",
      "[1157]\tvalidation-rmse:0.02351\n",
      "[1158]\tvalidation-rmse:0.02351\n",
      "[1159]\tvalidation-rmse:0.02351\n",
      "[1160]\tvalidation-rmse:0.02351\n",
      "[1161]\tvalidation-rmse:0.02351\n",
      "[1162]\tvalidation-rmse:0.02351\n",
      "[1163]\tvalidation-rmse:0.02351\n",
      "[1164]\tvalidation-rmse:0.02351\n",
      "[1165]\tvalidation-rmse:0.02351\n",
      "[1166]\tvalidation-rmse:0.02351\n",
      "[1167]\tvalidation-rmse:0.02350\n",
      "[1168]\tvalidation-rmse:0.02350\n",
      "[1169]\tvalidation-rmse:0.02350\n",
      "[1170]\tvalidation-rmse:0.02350\n",
      "[1171]\tvalidation-rmse:0.02350\n",
      "[1172]\tvalidation-rmse:0.02350\n",
      "[1173]\tvalidation-rmse:0.02350\n",
      "[1174]\tvalidation-rmse:0.02349\n",
      "[1175]\tvalidation-rmse:0.02349\n",
      "[1176]\tvalidation-rmse:0.02349\n",
      "[1177]\tvalidation-rmse:0.02349\n",
      "[1178]\tvalidation-rmse:0.02349\n",
      "[1179]\tvalidation-rmse:0.02349\n",
      "[1180]\tvalidation-rmse:0.02348\n",
      "[1181]\tvalidation-rmse:0.02348\n",
      "[1182]\tvalidation-rmse:0.02348\n",
      "[1183]\tvalidation-rmse:0.02348\n",
      "[1184]\tvalidation-rmse:0.02348\n",
      "[1185]\tvalidation-rmse:0.02348\n",
      "[1186]\tvalidation-rmse:0.02348\n",
      "[1187]\tvalidation-rmse:0.02348\n",
      "[1188]\tvalidation-rmse:0.02347\n",
      "[1189]\tvalidation-rmse:0.02347\n",
      "[1190]\tvalidation-rmse:0.02347\n",
      "[1191]\tvalidation-rmse:0.02347\n",
      "[1192]\tvalidation-rmse:0.02347\n",
      "[1193]\tvalidation-rmse:0.02347\n",
      "[1194]\tvalidation-rmse:0.02347\n",
      "[1195]\tvalidation-rmse:0.02347\n",
      "[1196]\tvalidation-rmse:0.02347\n",
      "[1197]\tvalidation-rmse:0.02347\n",
      "[1198]\tvalidation-rmse:0.02347\n",
      "[1199]\tvalidation-rmse:0.02346\n",
      "[1200]\tvalidation-rmse:0.02346\n",
      "[1201]\tvalidation-rmse:0.02346\n",
      "[1202]\tvalidation-rmse:0.02346\n",
      "[1203]\tvalidation-rmse:0.02346\n",
      "[1204]\tvalidation-rmse:0.02346\n",
      "[1205]\tvalidation-rmse:0.02346\n",
      "[1206]\tvalidation-rmse:0.02346\n",
      "[1207]\tvalidation-rmse:0.02345\n",
      "[1208]\tvalidation-rmse:0.02345\n",
      "[1209]\tvalidation-rmse:0.02345\n",
      "[1210]\tvalidation-rmse:0.02345\n",
      "[1211]\tvalidation-rmse:0.02345\n",
      "[1212]\tvalidation-rmse:0.02345\n",
      "[1213]\tvalidation-rmse:0.02345\n",
      "[1214]\tvalidation-rmse:0.02344\n",
      "[1215]\tvalidation-rmse:0.02344\n",
      "[1216]\tvalidation-rmse:0.02344\n",
      "[1217]\tvalidation-rmse:0.02344\n",
      "[1218]\tvalidation-rmse:0.02344\n",
      "[1219]\tvalidation-rmse:0.02344\n",
      "[1220]\tvalidation-rmse:0.02344\n",
      "[1221]\tvalidation-rmse:0.02344\n",
      "[1222]\tvalidation-rmse:0.02343\n",
      "[1223]\tvalidation-rmse:0.02343\n",
      "[1224]\tvalidation-rmse:0.02343\n",
      "[1225]\tvalidation-rmse:0.02343\n",
      "[1226]\tvalidation-rmse:0.02343\n",
      "[1227]\tvalidation-rmse:0.02342\n",
      "[1228]\tvalidation-rmse:0.02342\n",
      "[1229]\tvalidation-rmse:0.02342\n",
      "[1230]\tvalidation-rmse:0.02342\n",
      "[1231]\tvalidation-rmse:0.02342\n",
      "[1232]\tvalidation-rmse:0.02341\n",
      "[1233]\tvalidation-rmse:0.02342\n",
      "[1234]\tvalidation-rmse:0.02341\n",
      "[1235]\tvalidation-rmse:0.02341\n",
      "[1236]\tvalidation-rmse:0.02341\n",
      "[1237]\tvalidation-rmse:0.02341\n",
      "[1238]\tvalidation-rmse:0.02341\n",
      "[1239]\tvalidation-rmse:0.02341\n",
      "[1240]\tvalidation-rmse:0.02341\n",
      "[1241]\tvalidation-rmse:0.02341\n",
      "[1242]\tvalidation-rmse:0.02341\n",
      "[1243]\tvalidation-rmse:0.02341\n",
      "[1244]\tvalidation-rmse:0.02341\n",
      "[1245]\tvalidation-rmse:0.02341\n",
      "[1246]\tvalidation-rmse:0.02341\n",
      "[1247]\tvalidation-rmse:0.02341\n",
      "[1248]\tvalidation-rmse:0.02340\n",
      "[1249]\tvalidation-rmse:0.02340\n",
      "[1250]\tvalidation-rmse:0.02340\n",
      "[1251]\tvalidation-rmse:0.02340\n",
      "[1252]\tvalidation-rmse:0.02340\n",
      "[1253]\tvalidation-rmse:0.02340\n",
      "[1254]\tvalidation-rmse:0.02340\n",
      "[1255]\tvalidation-rmse:0.02340\n",
      "[1256]\tvalidation-rmse:0.02340\n",
      "[1257]\tvalidation-rmse:0.02339\n",
      "[1258]\tvalidation-rmse:0.02339\n",
      "[1259]\tvalidation-rmse:0.02339\n",
      "[1260]\tvalidation-rmse:0.02339\n",
      "[1261]\tvalidation-rmse:0.02339\n",
      "[1262]\tvalidation-rmse:0.02338\n",
      "[1263]\tvalidation-rmse:0.02338\n",
      "[1264]\tvalidation-rmse:0.02338\n",
      "[1265]\tvalidation-rmse:0.02338\n",
      "[1266]\tvalidation-rmse:0.02338\n",
      "[1267]\tvalidation-rmse:0.02338\n",
      "[1268]\tvalidation-rmse:0.02339\n",
      "[1269]\tvalidation-rmse:0.02339\n",
      "[1270]\tvalidation-rmse:0.02339\n",
      "[1271]\tvalidation-rmse:0.02339\n",
      "[1272]\tvalidation-rmse:0.02339\n",
      "[1273]\tvalidation-rmse:0.02339\n",
      "[1274]\tvalidation-rmse:0.02339\n",
      "[1275]\tvalidation-rmse:0.02339\n",
      "[1276]\tvalidation-rmse:0.02339\n",
      "[1277]\tvalidation-rmse:0.02338\n",
      "[1278]\tvalidation-rmse:0.02339\n",
      "[1279]\tvalidation-rmse:0.02338\n",
      "[1280]\tvalidation-rmse:0.02338\n",
      "[1281]\tvalidation-rmse:0.02338\n",
      "[1282]\tvalidation-rmse:0.02338\n",
      "[1283]\tvalidation-rmse:0.02338\n",
      "[1284]\tvalidation-rmse:0.02338\n",
      "[1285]\tvalidation-rmse:0.02338\n",
      "[1286]\tvalidation-rmse:0.02338\n",
      "[1287]\tvalidation-rmse:0.02338\n",
      "[1288]\tvalidation-rmse:0.02337\n",
      "[1289]\tvalidation-rmse:0.02338\n",
      "[1290]\tvalidation-rmse:0.02338\n",
      "[1291]\tvalidation-rmse:0.02338\n",
      "[1292]\tvalidation-rmse:0.02338\n",
      "[1293]\tvalidation-rmse:0.02338\n",
      "[1294]\tvalidation-rmse:0.02338\n",
      "[1295]\tvalidation-rmse:0.02338\n",
      "[1296]\tvalidation-rmse:0.02337\n",
      "[1297]\tvalidation-rmse:0.02337\n",
      "[1298]\tvalidation-rmse:0.02337\n",
      "[1299]\tvalidation-rmse:0.02337\n",
      "[1300]\tvalidation-rmse:0.02337\n",
      "[1301]\tvalidation-rmse:0.02337\n",
      "[1302]\tvalidation-rmse:0.02336\n",
      "[1303]\tvalidation-rmse:0.02336\n",
      "[1304]\tvalidation-rmse:0.02336\n",
      "[1305]\tvalidation-rmse:0.02336\n",
      "[1306]\tvalidation-rmse:0.02335\n",
      "[1307]\tvalidation-rmse:0.02336\n",
      "[1308]\tvalidation-rmse:0.02335\n",
      "[1309]\tvalidation-rmse:0.02335\n",
      "[1310]\tvalidation-rmse:0.02335\n",
      "[1311]\tvalidation-rmse:0.02335\n",
      "[1312]\tvalidation-rmse:0.02335\n",
      "[1313]\tvalidation-rmse:0.02335\n",
      "[1314]\tvalidation-rmse:0.02335\n",
      "[1315]\tvalidation-rmse:0.02335\n",
      "[1316]\tvalidation-rmse:0.02335\n",
      "[1317]\tvalidation-rmse:0.02335\n",
      "[1318]\tvalidation-rmse:0.02335\n",
      "[1319]\tvalidation-rmse:0.02335\n",
      "[1320]\tvalidation-rmse:0.02335\n",
      "[1321]\tvalidation-rmse:0.02335\n",
      "[1322]\tvalidation-rmse:0.02335\n",
      "[1323]\tvalidation-rmse:0.02334\n",
      "[1324]\tvalidation-rmse:0.02334\n",
      "[1325]\tvalidation-rmse:0.02334\n",
      "[1326]\tvalidation-rmse:0.02334\n",
      "[1327]\tvalidation-rmse:0.02334\n",
      "[1328]\tvalidation-rmse:0.02333\n",
      "[1329]\tvalidation-rmse:0.02333\n",
      "[1330]\tvalidation-rmse:0.02334\n",
      "[1331]\tvalidation-rmse:0.02334\n",
      "[1332]\tvalidation-rmse:0.02333\n",
      "[1333]\tvalidation-rmse:0.02333\n",
      "[1334]\tvalidation-rmse:0.02333\n",
      "[1335]\tvalidation-rmse:0.02334\n",
      "[1336]\tvalidation-rmse:0.02334\n",
      "[1337]\tvalidation-rmse:0.02334\n",
      "[1338]\tvalidation-rmse:0.02333\n",
      "[1339]\tvalidation-rmse:0.02333\n",
      "[1340]\tvalidation-rmse:0.02333\n",
      "[1341]\tvalidation-rmse:0.02333\n",
      "[1342]\tvalidation-rmse:0.02333\n",
      "[1343]\tvalidation-rmse:0.02332\n",
      "[1344]\tvalidation-rmse:0.02332\n",
      "[1345]\tvalidation-rmse:0.02332\n",
      "[1346]\tvalidation-rmse:0.02332\n",
      "[1347]\tvalidation-rmse:0.02332\n",
      "[1348]\tvalidation-rmse:0.02332\n",
      "[1349]\tvalidation-rmse:0.02332\n",
      "[1350]\tvalidation-rmse:0.02332\n",
      "[1351]\tvalidation-rmse:0.02332\n",
      "[1352]\tvalidation-rmse:0.02332\n",
      "[1353]\tvalidation-rmse:0.02332\n",
      "[1354]\tvalidation-rmse:0.02332\n",
      "[1355]\tvalidation-rmse:0.02332\n",
      "[1356]\tvalidation-rmse:0.02332\n",
      "[1357]\tvalidation-rmse:0.02332\n",
      "[1358]\tvalidation-rmse:0.02331\n",
      "[1359]\tvalidation-rmse:0.02331\n",
      "[1360]\tvalidation-rmse:0.02331\n",
      "[1361]\tvalidation-rmse:0.02331\n",
      "[1362]\tvalidation-rmse:0.02331\n",
      "[1363]\tvalidation-rmse:0.02331\n",
      "[1364]\tvalidation-rmse:0.02331\n",
      "[1365]\tvalidation-rmse:0.02331\n",
      "[1366]\tvalidation-rmse:0.02331\n",
      "[1367]\tvalidation-rmse:0.02330\n",
      "[1368]\tvalidation-rmse:0.02331\n",
      "[1369]\tvalidation-rmse:0.02331\n",
      "[1370]\tvalidation-rmse:0.02331\n",
      "[1371]\tvalidation-rmse:0.02331\n",
      "[1372]\tvalidation-rmse:0.02331\n",
      "[1373]\tvalidation-rmse:0.02330\n",
      "[1374]\tvalidation-rmse:0.02331\n",
      "[1375]\tvalidation-rmse:0.02331\n",
      "[1376]\tvalidation-rmse:0.02331\n",
      "[1377]\tvalidation-rmse:0.02331\n",
      "[1378]\tvalidation-rmse:0.02331\n",
      "[1379]\tvalidation-rmse:0.02331\n",
      "[1380]\tvalidation-rmse:0.02331\n",
      "[1381]\tvalidation-rmse:0.02331\n",
      "[1382]\tvalidation-rmse:0.02331\n",
      "[1383]\tvalidation-rmse:0.02331\n",
      "[1384]\tvalidation-rmse:0.02331\n",
      "[1385]\tvalidation-rmse:0.02330\n",
      "[1386]\tvalidation-rmse:0.02330\n",
      "[1387]\tvalidation-rmse:0.02330\n",
      "[1388]\tvalidation-rmse:0.02330\n",
      "[1389]\tvalidation-rmse:0.02330\n",
      "[1390]\tvalidation-rmse:0.02330\n",
      "[1391]\tvalidation-rmse:0.02330\n",
      "[1392]\tvalidation-rmse:0.02329\n",
      "[1393]\tvalidation-rmse:0.02329\n",
      "[1394]\tvalidation-rmse:0.02329\n",
      "[1395]\tvalidation-rmse:0.02329\n",
      "[1396]\tvalidation-rmse:0.02329\n",
      "[1397]\tvalidation-rmse:0.02329\n",
      "[1398]\tvalidation-rmse:0.02329\n",
      "[1399]\tvalidation-rmse:0.02329\n",
      "[1400]\tvalidation-rmse:0.02329\n",
      "[1401]\tvalidation-rmse:0.02329\n",
      "[1402]\tvalidation-rmse:0.02328\n",
      "[1403]\tvalidation-rmse:0.02328\n",
      "[1404]\tvalidation-rmse:0.02328\n",
      "[1405]\tvalidation-rmse:0.02328\n",
      "[1406]\tvalidation-rmse:0.02327\n",
      "[1407]\tvalidation-rmse:0.02327\n",
      "[1408]\tvalidation-rmse:0.02327\n",
      "[1409]\tvalidation-rmse:0.02327\n",
      "[1410]\tvalidation-rmse:0.02327\n",
      "[1411]\tvalidation-rmse:0.02327\n",
      "[1412]\tvalidation-rmse:0.02327\n",
      "[1413]\tvalidation-rmse:0.02327\n",
      "[1414]\tvalidation-rmse:0.02326\n",
      "[1415]\tvalidation-rmse:0.02326\n",
      "[1416]\tvalidation-rmse:0.02326\n",
      "[1417]\tvalidation-rmse:0.02326\n",
      "[1418]\tvalidation-rmse:0.02326\n",
      "[1419]\tvalidation-rmse:0.02326\n",
      "[1420]\tvalidation-rmse:0.02326\n",
      "[1421]\tvalidation-rmse:0.02326\n",
      "[1422]\tvalidation-rmse:0.02326\n",
      "[1423]\tvalidation-rmse:0.02326\n",
      "[1424]\tvalidation-rmse:0.02326\n",
      "[1425]\tvalidation-rmse:0.02326\n",
      "[1426]\tvalidation-rmse:0.02326\n",
      "[1427]\tvalidation-rmse:0.02326\n",
      "[1428]\tvalidation-rmse:0.02326\n",
      "[1429]\tvalidation-rmse:0.02325\n",
      "[1430]\tvalidation-rmse:0.02325\n",
      "[1431]\tvalidation-rmse:0.02325\n",
      "[1432]\tvalidation-rmse:0.02325\n",
      "[1433]\tvalidation-rmse:0.02325\n",
      "[1434]\tvalidation-rmse:0.02325\n",
      "[1435]\tvalidation-rmse:0.02325\n",
      "[1436]\tvalidation-rmse:0.02324\n",
      "[1437]\tvalidation-rmse:0.02324\n",
      "[1438]\tvalidation-rmse:0.02324\n",
      "[1439]\tvalidation-rmse:0.02325\n",
      "[1440]\tvalidation-rmse:0.02325\n",
      "[1441]\tvalidation-rmse:0.02325\n",
      "[1442]\tvalidation-rmse:0.02325\n",
      "[1443]\tvalidation-rmse:0.02325\n",
      "[1444]\tvalidation-rmse:0.02325\n",
      "[1445]\tvalidation-rmse:0.02325\n",
      "[1446]\tvalidation-rmse:0.02325\n",
      "[1447]\tvalidation-rmse:0.02325\n",
      "[1448]\tvalidation-rmse:0.02325\n",
      "[1449]\tvalidation-rmse:0.02325\n",
      "[1450]\tvalidation-rmse:0.02324\n",
      "[1451]\tvalidation-rmse:0.02325\n",
      "[1452]\tvalidation-rmse:0.02324\n",
      "[1453]\tvalidation-rmse:0.02324\n",
      "[1454]\tvalidation-rmse:0.02324\n",
      "[1455]\tvalidation-rmse:0.02324\n",
      "[1456]\tvalidation-rmse:0.02324\n",
      "[1457]\tvalidation-rmse:0.02324\n",
      "[1458]\tvalidation-rmse:0.02324\n",
      "[1459]\tvalidation-rmse:0.02324\n",
      "[1460]\tvalidation-rmse:0.02324\n",
      "[1461]\tvalidation-rmse:0.02324\n",
      "[1462]\tvalidation-rmse:0.02324\n",
      "[1463]\tvalidation-rmse:0.02324\n",
      "[1464]\tvalidation-rmse:0.02324\n",
      "[1465]\tvalidation-rmse:0.02324\n",
      "[1466]\tvalidation-rmse:0.02324\n",
      "[1467]\tvalidation-rmse:0.02324\n",
      "[1468]\tvalidation-rmse:0.02324\n",
      "[1469]\tvalidation-rmse:0.02324\n",
      "[1470]\tvalidation-rmse:0.02324\n",
      "[1471]\tvalidation-rmse:0.02324\n",
      "[1472]\tvalidation-rmse:0.02324\n",
      "[1473]\tvalidation-rmse:0.02323\n",
      "[1474]\tvalidation-rmse:0.02323\n",
      "[1475]\tvalidation-rmse:0.02323\n",
      "[1476]\tvalidation-rmse:0.02323\n",
      "[1477]\tvalidation-rmse:0.02323\n",
      "[1478]\tvalidation-rmse:0.02323\n",
      "[1479]\tvalidation-rmse:0.02323\n",
      "[1480]\tvalidation-rmse:0.02323\n",
      "[1481]\tvalidation-rmse:0.02323\n",
      "[1482]\tvalidation-rmse:0.02323\n",
      "[1483]\tvalidation-rmse:0.02323\n",
      "[1484]\tvalidation-rmse:0.02323\n",
      "[1485]\tvalidation-rmse:0.02323\n",
      "[1486]\tvalidation-rmse:0.02323\n",
      "[1487]\tvalidation-rmse:0.02323\n",
      "[1488]\tvalidation-rmse:0.02323\n",
      "[1489]\tvalidation-rmse:0.02322\n",
      "[1490]\tvalidation-rmse:0.02322\n",
      "[1491]\tvalidation-rmse:0.02322\n",
      "[1492]\tvalidation-rmse:0.02322\n",
      "[1493]\tvalidation-rmse:0.02322\n",
      "[1494]\tvalidation-rmse:0.02322\n",
      "[1495]\tvalidation-rmse:0.02321\n",
      "[1496]\tvalidation-rmse:0.02321\n",
      "[1497]\tvalidation-rmse:0.02321\n",
      "[1498]\tvalidation-rmse:0.02321\n",
      "[1499]\tvalidation-rmse:0.02321\n",
      "[1500]\tvalidation-rmse:0.02321\n",
      "[1501]\tvalidation-rmse:0.02321\n",
      "[1502]\tvalidation-rmse:0.02321\n",
      "[1503]\tvalidation-rmse:0.02321\n",
      "[1504]\tvalidation-rmse:0.02321\n",
      "[1505]\tvalidation-rmse:0.02321\n",
      "[1506]\tvalidation-rmse:0.02321\n",
      "[1507]\tvalidation-rmse:0.02321\n",
      "[1508]\tvalidation-rmse:0.02321\n",
      "[1509]\tvalidation-rmse:0.02321\n",
      "[1510]\tvalidation-rmse:0.02321\n",
      "[1511]\tvalidation-rmse:0.02321\n",
      "[1512]\tvalidation-rmse:0.02321\n",
      "[1513]\tvalidation-rmse:0.02320\n",
      "[1514]\tvalidation-rmse:0.02320\n",
      "[1515]\tvalidation-rmse:0.02320\n",
      "[1516]\tvalidation-rmse:0.02320\n",
      "[1517]\tvalidation-rmse:0.02320\n",
      "[1518]\tvalidation-rmse:0.02320\n",
      "[1519]\tvalidation-rmse:0.02320\n",
      "[1520]\tvalidation-rmse:0.02320\n",
      "[1521]\tvalidation-rmse:0.02320\n",
      "[1522]\tvalidation-rmse:0.02320\n",
      "[1523]\tvalidation-rmse:0.02320\n",
      "[1524]\tvalidation-rmse:0.02320\n",
      "[1525]\tvalidation-rmse:0.02320\n",
      "[1526]\tvalidation-rmse:0.02319\n",
      "[1527]\tvalidation-rmse:0.02320\n",
      "[1528]\tvalidation-rmse:0.02320\n",
      "[1529]\tvalidation-rmse:0.02320\n",
      "[1530]\tvalidation-rmse:0.02319\n",
      "[1531]\tvalidation-rmse:0.02319\n",
      "[1532]\tvalidation-rmse:0.02320\n",
      "[1533]\tvalidation-rmse:0.02319\n",
      "[1534]\tvalidation-rmse:0.02320\n",
      "[1535]\tvalidation-rmse:0.02320\n",
      "[1536]\tvalidation-rmse:0.02319\n",
      "[1537]\tvalidation-rmse:0.02319\n",
      "[1538]\tvalidation-rmse:0.02319\n",
      "[1539]\tvalidation-rmse:0.02319\n",
      "[1540]\tvalidation-rmse:0.02320\n",
      "[1541]\tvalidation-rmse:0.02319\n",
      "[1542]\tvalidation-rmse:0.02319\n",
      "[1543]\tvalidation-rmse:0.02320\n",
      "[1544]\tvalidation-rmse:0.02320\n",
      "[1545]\tvalidation-rmse:0.02320\n",
      "[1546]\tvalidation-rmse:0.02320\n",
      "[1547]\tvalidation-rmse:0.02319\n",
      "[1548]\tvalidation-rmse:0.02319\n",
      "[1549]\tvalidation-rmse:0.02319\n",
      "[1550]\tvalidation-rmse:0.02319\n",
      "[1551]\tvalidation-rmse:0.02319\n",
      "[1552]\tvalidation-rmse:0.02319\n",
      "[1553]\tvalidation-rmse:0.02319\n",
      "[1554]\tvalidation-rmse:0.02319\n",
      "[1555]\tvalidation-rmse:0.02319\n",
      "[1556]\tvalidation-rmse:0.02318\n",
      "[1557]\tvalidation-rmse:0.02319\n",
      "[1558]\tvalidation-rmse:0.02318\n",
      "[1559]\tvalidation-rmse:0.02319\n",
      "[1560]\tvalidation-rmse:0.02319\n",
      "[1561]\tvalidation-rmse:0.02318\n",
      "[1562]\tvalidation-rmse:0.02318\n",
      "[1563]\tvalidation-rmse:0.02318\n",
      "[1564]\tvalidation-rmse:0.02318\n",
      "[1565]\tvalidation-rmse:0.02318\n",
      "[1566]\tvalidation-rmse:0.02318\n",
      "[1567]\tvalidation-rmse:0.02318\n",
      "[1568]\tvalidation-rmse:0.02318\n",
      "[1569]\tvalidation-rmse:0.02318\n",
      "[1570]\tvalidation-rmse:0.02318\n",
      "[1571]\tvalidation-rmse:0.02318\n",
      "[1572]\tvalidation-rmse:0.02318\n",
      "[1573]\tvalidation-rmse:0.02318\n",
      "[1574]\tvalidation-rmse:0.02318\n",
      "[1575]\tvalidation-rmse:0.02318\n",
      "[1576]\tvalidation-rmse:0.02318\n",
      "[1577]\tvalidation-rmse:0.02318\n",
      "[1578]\tvalidation-rmse:0.02318\n",
      "[1579]\tvalidation-rmse:0.02318\n",
      "[1580]\tvalidation-rmse:0.02317\n",
      "[1581]\tvalidation-rmse:0.02318\n",
      "[1582]\tvalidation-rmse:0.02317\n",
      "[1583]\tvalidation-rmse:0.02317\n",
      "[1584]\tvalidation-rmse:0.02317\n",
      "[1585]\tvalidation-rmse:0.02317\n",
      "[1586]\tvalidation-rmse:0.02317\n",
      "[1587]\tvalidation-rmse:0.02317\n",
      "[1588]\tvalidation-rmse:0.02317\n",
      "[1589]\tvalidation-rmse:0.02317\n",
      "[1590]\tvalidation-rmse:0.02317\n",
      "[1591]\tvalidation-rmse:0.02317\n",
      "[1592]\tvalidation-rmse:0.02317\n",
      "[1593]\tvalidation-rmse:0.02317\n",
      "[1594]\tvalidation-rmse:0.02317\n",
      "[1595]\tvalidation-rmse:0.02317\n",
      "[1596]\tvalidation-rmse:0.02317\n",
      "[1597]\tvalidation-rmse:0.02317\n",
      "[1598]\tvalidation-rmse:0.02317\n",
      "[1599]\tvalidation-rmse:0.02317\n",
      "[1600]\tvalidation-rmse:0.02317\n",
      "[1601]\tvalidation-rmse:0.02316\n",
      "[1602]\tvalidation-rmse:0.02317\n",
      "[1603]\tvalidation-rmse:0.02317\n",
      "[1604]\tvalidation-rmse:0.02317\n",
      "[1605]\tvalidation-rmse:0.02317\n",
      "[1606]\tvalidation-rmse:0.02317\n",
      "[1607]\tvalidation-rmse:0.02317\n",
      "[1608]\tvalidation-rmse:0.02316\n",
      "[1609]\tvalidation-rmse:0.02316\n",
      "[1610]\tvalidation-rmse:0.02316\n",
      "[1611]\tvalidation-rmse:0.02316\n",
      "[1612]\tvalidation-rmse:0.02316\n",
      "[1613]\tvalidation-rmse:0.02316\n",
      "[1614]\tvalidation-rmse:0.02316\n",
      "[1615]\tvalidation-rmse:0.02316\n",
      "[1616]\tvalidation-rmse:0.02316\n",
      "[1617]\tvalidation-rmse:0.02316\n",
      "[1618]\tvalidation-rmse:0.02316\n",
      "[1619]\tvalidation-rmse:0.02316\n",
      "[1620]\tvalidation-rmse:0.02316\n",
      "[1621]\tvalidation-rmse:0.02316\n",
      "[1622]\tvalidation-rmse:0.02316\n",
      "[1623]\tvalidation-rmse:0.02316\n",
      "[1624]\tvalidation-rmse:0.02316\n",
      "[1625]\tvalidation-rmse:0.02315\n",
      "[1626]\tvalidation-rmse:0.02315\n",
      "[1627]\tvalidation-rmse:0.02315\n",
      "[1628]\tvalidation-rmse:0.02315\n",
      "[1629]\tvalidation-rmse:0.02316\n",
      "[1630]\tvalidation-rmse:0.02315\n",
      "[1631]\tvalidation-rmse:0.02315\n",
      "[1632]\tvalidation-rmse:0.02315\n",
      "[1633]\tvalidation-rmse:0.02315\n",
      "[1634]\tvalidation-rmse:0.02315\n",
      "[1635]\tvalidation-rmse:0.02315\n",
      "[1636]\tvalidation-rmse:0.02315\n",
      "[1637]\tvalidation-rmse:0.02315\n",
      "[1638]\tvalidation-rmse:0.02315\n",
      "[1639]\tvalidation-rmse:0.02315\n",
      "[1640]\tvalidation-rmse:0.02315\n",
      "[1641]\tvalidation-rmse:0.02315\n",
      "[1642]\tvalidation-rmse:0.02315\n",
      "[1643]\tvalidation-rmse:0.02315\n",
      "[1644]\tvalidation-rmse:0.02315\n",
      "[1645]\tvalidation-rmse:0.02315\n",
      "[1646]\tvalidation-rmse:0.02315\n",
      "[1647]\tvalidation-rmse:0.02315\n",
      "[1648]\tvalidation-rmse:0.02315\n",
      "[1649]\tvalidation-rmse:0.02315\n",
      "[1650]\tvalidation-rmse:0.02314\n",
      "[1651]\tvalidation-rmse:0.02314\n",
      "[1652]\tvalidation-rmse:0.02314\n",
      "[1653]\tvalidation-rmse:0.02314\n",
      "[1654]\tvalidation-rmse:0.02314\n",
      "[1655]\tvalidation-rmse:0.02314\n",
      "[1656]\tvalidation-rmse:0.02314\n",
      "[1657]\tvalidation-rmse:0.02314\n",
      "[1658]\tvalidation-rmse:0.02314\n",
      "[1659]\tvalidation-rmse:0.02314\n",
      "[1660]\tvalidation-rmse:0.02314\n",
      "[1661]\tvalidation-rmse:0.02314\n",
      "[1662]\tvalidation-rmse:0.02314\n",
      "[1663]\tvalidation-rmse:0.02314\n",
      "[1664]\tvalidation-rmse:0.02313\n",
      "[1665]\tvalidation-rmse:0.02313\n",
      "[1666]\tvalidation-rmse:0.02313\n",
      "[1667]\tvalidation-rmse:0.02313\n",
      "[1668]\tvalidation-rmse:0.02313\n",
      "[1669]\tvalidation-rmse:0.02313\n",
      "[1670]\tvalidation-rmse:0.02313\n",
      "[1671]\tvalidation-rmse:0.02313\n",
      "[1672]\tvalidation-rmse:0.02313\n",
      "[1673]\tvalidation-rmse:0.02313\n",
      "[1674]\tvalidation-rmse:0.02313\n",
      "[1675]\tvalidation-rmse:0.02313\n",
      "[1676]\tvalidation-rmse:0.02313\n",
      "[1677]\tvalidation-rmse:0.02313\n",
      "[1678]\tvalidation-rmse:0.02312\n",
      "[1679]\tvalidation-rmse:0.02312\n",
      "[1680]\tvalidation-rmse:0.02312\n",
      "[1681]\tvalidation-rmse:0.02312\n",
      "[1682]\tvalidation-rmse:0.02312\n",
      "[1683]\tvalidation-rmse:0.02312\n",
      "[1684]\tvalidation-rmse:0.02312\n",
      "[1685]\tvalidation-rmse:0.02312\n",
      "[1686]\tvalidation-rmse:0.02312\n",
      "[1687]\tvalidation-rmse:0.02312\n",
      "[1688]\tvalidation-rmse:0.02312\n",
      "[1689]\tvalidation-rmse:0.02312\n",
      "[1690]\tvalidation-rmse:0.02311\n",
      "[1691]\tvalidation-rmse:0.02311\n",
      "[1692]\tvalidation-rmse:0.02311\n",
      "[1693]\tvalidation-rmse:0.02311\n",
      "[1694]\tvalidation-rmse:0.02311\n",
      "[1695]\tvalidation-rmse:0.02311\n",
      "[1696]\tvalidation-rmse:0.02311\n",
      "[1697]\tvalidation-rmse:0.02311\n",
      "[1698]\tvalidation-rmse:0.02311\n",
      "[1699]\tvalidation-rmse:0.02311\n",
      "[1700]\tvalidation-rmse:0.02311\n",
      "[1701]\tvalidation-rmse:0.02311\n",
      "[1702]\tvalidation-rmse:0.02311\n",
      "[1703]\tvalidation-rmse:0.02311\n",
      "[1704]\tvalidation-rmse:0.02311\n",
      "[1705]\tvalidation-rmse:0.02311\n",
      "[1706]\tvalidation-rmse:0.02311\n",
      "[1707]\tvalidation-rmse:0.02311\n",
      "[1708]\tvalidation-rmse:0.02311\n",
      "[1709]\tvalidation-rmse:0.02311\n",
      "[1710]\tvalidation-rmse:0.02311\n",
      "[1711]\tvalidation-rmse:0.02311\n",
      "[1712]\tvalidation-rmse:0.02311\n",
      "[1713]\tvalidation-rmse:0.02311\n",
      "[1714]\tvalidation-rmse:0.02311\n",
      "[1715]\tvalidation-rmse:0.02311\n",
      "[1716]\tvalidation-rmse:0.02311\n",
      "[1717]\tvalidation-rmse:0.02311\n",
      "[1718]\tvalidation-rmse:0.02311\n",
      "[1719]\tvalidation-rmse:0.02311\n",
      "[1720]\tvalidation-rmse:0.02311\n",
      "[1721]\tvalidation-rmse:0.02311\n",
      "[1722]\tvalidation-rmse:0.02311\n",
      "[1723]\tvalidation-rmse:0.02311\n",
      "[1724]\tvalidation-rmse:0.02311\n",
      "[1725]\tvalidation-rmse:0.02311\n",
      "[1726]\tvalidation-rmse:0.02311\n",
      "[1727]\tvalidation-rmse:0.02310\n",
      "[1728]\tvalidation-rmse:0.02310\n",
      "[1729]\tvalidation-rmse:0.02311\n",
      "[1730]\tvalidation-rmse:0.02311\n",
      "[1731]\tvalidation-rmse:0.02311\n",
      "[1732]\tvalidation-rmse:0.02310\n",
      "[1733]\tvalidation-rmse:0.02310\n",
      "[1734]\tvalidation-rmse:0.02310\n",
      "[1735]\tvalidation-rmse:0.02310\n",
      "[1736]\tvalidation-rmse:0.02310\n",
      "[1737]\tvalidation-rmse:0.02310\n",
      "[1738]\tvalidation-rmse:0.02310\n",
      "[1739]\tvalidation-rmse:0.02310\n",
      "[1740]\tvalidation-rmse:0.02310\n",
      "[1741]\tvalidation-rmse:0.02310\n",
      "[1742]\tvalidation-rmse:0.02310\n",
      "[1743]\tvalidation-rmse:0.02310\n",
      "[1744]\tvalidation-rmse:0.02310\n",
      "[1745]\tvalidation-rmse:0.02310\n",
      "[1746]\tvalidation-rmse:0.02309\n",
      "[1747]\tvalidation-rmse:0.02309\n",
      "[1748]\tvalidation-rmse:0.02309\n",
      "[1749]\tvalidation-rmse:0.02309\n",
      "[1750]\tvalidation-rmse:0.02309\n",
      "[1751]\tvalidation-rmse:0.02309\n",
      "[1752]\tvalidation-rmse:0.02309\n",
      "[1753]\tvalidation-rmse:0.02309\n",
      "[1754]\tvalidation-rmse:0.02309\n",
      "[1755]\tvalidation-rmse:0.02309\n",
      "[1756]\tvalidation-rmse:0.02309\n",
      "[1757]\tvalidation-rmse:0.02309\n",
      "[1758]\tvalidation-rmse:0.02309\n",
      "[1759]\tvalidation-rmse:0.02309\n",
      "[1760]\tvalidation-rmse:0.02309\n",
      "[1761]\tvalidation-rmse:0.02309\n",
      "[1762]\tvalidation-rmse:0.02309\n",
      "[1763]\tvalidation-rmse:0.02309\n",
      "[1764]\tvalidation-rmse:0.02309\n",
      "[1765]\tvalidation-rmse:0.02308\n",
      "[1766]\tvalidation-rmse:0.02309\n",
      "[1767]\tvalidation-rmse:0.02309\n",
      "[1768]\tvalidation-rmse:0.02308\n",
      "[1769]\tvalidation-rmse:0.02308\n",
      "[1770]\tvalidation-rmse:0.02308\n",
      "[1771]\tvalidation-rmse:0.02308\n",
      "[1772]\tvalidation-rmse:0.02308\n",
      "[1773]\tvalidation-rmse:0.02308\n",
      "[1774]\tvalidation-rmse:0.02308\n",
      "[1775]\tvalidation-rmse:0.02308\n",
      "[1776]\tvalidation-rmse:0.02308\n",
      "[1777]\tvalidation-rmse:0.02308\n",
      "[1778]\tvalidation-rmse:0.02308\n",
      "[1779]\tvalidation-rmse:0.02308\n",
      "[1780]\tvalidation-rmse:0.02308\n",
      "[1781]\tvalidation-rmse:0.02307\n",
      "[1782]\tvalidation-rmse:0.02307\n",
      "[1783]\tvalidation-rmse:0.02307\n",
      "[1784]\tvalidation-rmse:0.02307\n",
      "[1785]\tvalidation-rmse:0.02307\n",
      "[1786]\tvalidation-rmse:0.02307\n",
      "[1787]\tvalidation-rmse:0.02307\n",
      "[1788]\tvalidation-rmse:0.02307\n",
      "[1789]\tvalidation-rmse:0.02307\n",
      "[1790]\tvalidation-rmse:0.02307\n",
      "[1791]\tvalidation-rmse:0.02306\n",
      "[1792]\tvalidation-rmse:0.02306\n",
      "[1793]\tvalidation-rmse:0.02306\n",
      "[1794]\tvalidation-rmse:0.02306\n",
      "[1795]\tvalidation-rmse:0.02306\n",
      "[1796]\tvalidation-rmse:0.02306\n",
      "[1797]\tvalidation-rmse:0.02306\n",
      "[1798]\tvalidation-rmse:0.02306\n",
      "[1799]\tvalidation-rmse:0.02306\n",
      "[1800]\tvalidation-rmse:0.02306\n",
      "[1801]\tvalidation-rmse:0.02306\n",
      "[1802]\tvalidation-rmse:0.02306\n",
      "[1803]\tvalidation-rmse:0.02306\n",
      "[1804]\tvalidation-rmse:0.02306\n",
      "[1805]\tvalidation-rmse:0.02306\n",
      "[1806]\tvalidation-rmse:0.02306\n",
      "[1807]\tvalidation-rmse:0.02306\n",
      "[1808]\tvalidation-rmse:0.02306\n",
      "[1809]\tvalidation-rmse:0.02306\n",
      "[1810]\tvalidation-rmse:0.02305\n",
      "[1811]\tvalidation-rmse:0.02305\n",
      "[1812]\tvalidation-rmse:0.02305\n",
      "[1813]\tvalidation-rmse:0.02305\n",
      "[1814]\tvalidation-rmse:0.02305\n",
      "[1815]\tvalidation-rmse:0.02305\n",
      "[1816]\tvalidation-rmse:0.02305\n",
      "[1817]\tvalidation-rmse:0.02304\n",
      "[1818]\tvalidation-rmse:0.02305\n",
      "[1819]\tvalidation-rmse:0.02305\n",
      "[1820]\tvalidation-rmse:0.02305\n",
      "[1821]\tvalidation-rmse:0.02305\n",
      "[1822]\tvalidation-rmse:0.02305\n",
      "[1823]\tvalidation-rmse:0.02305\n",
      "[1824]\tvalidation-rmse:0.02305\n",
      "[1825]\tvalidation-rmse:0.02304\n",
      "[1826]\tvalidation-rmse:0.02304\n",
      "[1827]\tvalidation-rmse:0.02304\n",
      "[1828]\tvalidation-rmse:0.02304\n",
      "[1829]\tvalidation-rmse:0.02304\n",
      "[1830]\tvalidation-rmse:0.02304\n",
      "[1831]\tvalidation-rmse:0.02304\n",
      "[1832]\tvalidation-rmse:0.02304\n",
      "[1833]\tvalidation-rmse:0.02304\n",
      "[1834]\tvalidation-rmse:0.02304\n",
      "[1835]\tvalidation-rmse:0.02304\n",
      "[1836]\tvalidation-rmse:0.02304\n",
      "[1837]\tvalidation-rmse:0.02304\n",
      "[1838]\tvalidation-rmse:0.02304\n",
      "[1839]\tvalidation-rmse:0.02304\n",
      "[1840]\tvalidation-rmse:0.02304\n",
      "[1841]\tvalidation-rmse:0.02304\n",
      "[1842]\tvalidation-rmse:0.02304\n",
      "[1843]\tvalidation-rmse:0.02304\n",
      "[1844]\tvalidation-rmse:0.02304\n",
      "[1845]\tvalidation-rmse:0.02304\n",
      "[1846]\tvalidation-rmse:0.02304\n",
      "[1847]\tvalidation-rmse:0.02304\n",
      "[1848]\tvalidation-rmse:0.02304\n",
      "[1849]\tvalidation-rmse:0.02304\n",
      "[1850]\tvalidation-rmse:0.02303\n",
      "[1851]\tvalidation-rmse:0.02303\n",
      "[1852]\tvalidation-rmse:0.02303\n",
      "[1853]\tvalidation-rmse:0.02303\n",
      "[1854]\tvalidation-rmse:0.02303\n",
      "[1855]\tvalidation-rmse:0.02303\n",
      "[1856]\tvalidation-rmse:0.02303\n",
      "[1857]\tvalidation-rmse:0.02303\n",
      "[1858]\tvalidation-rmse:0.02303\n",
      "[1859]\tvalidation-rmse:0.02303\n",
      "[1860]\tvalidation-rmse:0.02303\n",
      "[1861]\tvalidation-rmse:0.02303\n",
      "[1862]\tvalidation-rmse:0.02303\n",
      "[1863]\tvalidation-rmse:0.02303\n",
      "[1864]\tvalidation-rmse:0.02303\n",
      "[1865]\tvalidation-rmse:0.02303\n",
      "[1866]\tvalidation-rmse:0.02303\n",
      "[1867]\tvalidation-rmse:0.02303\n",
      "[1868]\tvalidation-rmse:0.02303\n",
      "[1869]\tvalidation-rmse:0.02303\n",
      "[1870]\tvalidation-rmse:0.02303\n",
      "[1871]\tvalidation-rmse:0.02303\n",
      "[1872]\tvalidation-rmse:0.02303\n",
      "[1873]\tvalidation-rmse:0.02303\n",
      "[1874]\tvalidation-rmse:0.02303\n",
      "[1875]\tvalidation-rmse:0.02303\n",
      "[1876]\tvalidation-rmse:0.02302\n",
      "[1877]\tvalidation-rmse:0.02302\n",
      "[1878]\tvalidation-rmse:0.02302\n",
      "[1879]\tvalidation-rmse:0.02302\n",
      "[1880]\tvalidation-rmse:0.02302\n",
      "[1881]\tvalidation-rmse:0.02302\n",
      "[1882]\tvalidation-rmse:0.02302\n",
      "[1883]\tvalidation-rmse:0.02302\n",
      "[1884]\tvalidation-rmse:0.02302\n",
      "[1885]\tvalidation-rmse:0.02302\n",
      "[1886]\tvalidation-rmse:0.02302\n",
      "[1887]\tvalidation-rmse:0.02301\n",
      "[1888]\tvalidation-rmse:0.02301\n",
      "[1889]\tvalidation-rmse:0.02301\n",
      "[1890]\tvalidation-rmse:0.02301\n",
      "[1891]\tvalidation-rmse:0.02301\n",
      "[1892]\tvalidation-rmse:0.02301\n",
      "[1893]\tvalidation-rmse:0.02301\n",
      "[1894]\tvalidation-rmse:0.02301\n",
      "[1895]\tvalidation-rmse:0.02301\n",
      "[1896]\tvalidation-rmse:0.02301\n",
      "[1897]\tvalidation-rmse:0.02301\n",
      "[1898]\tvalidation-rmse:0.02302\n",
      "[1899]\tvalidation-rmse:0.02301\n",
      "[1900]\tvalidation-rmse:0.02301\n",
      "[1901]\tvalidation-rmse:0.02301\n",
      "[1902]\tvalidation-rmse:0.02301\n",
      "[1903]\tvalidation-rmse:0.02301\n",
      "[1904]\tvalidation-rmse:0.02301\n",
      "[1905]\tvalidation-rmse:0.02301\n",
      "[1906]\tvalidation-rmse:0.02301\n",
      "[1907]\tvalidation-rmse:0.02301\n",
      "[1908]\tvalidation-rmse:0.02301\n",
      "[1909]\tvalidation-rmse:0.02301\n",
      "[1910]\tvalidation-rmse:0.02301\n",
      "[1911]\tvalidation-rmse:0.02301\n",
      "[1912]\tvalidation-rmse:0.02301\n",
      "[1913]\tvalidation-rmse:0.02300\n",
      "[1914]\tvalidation-rmse:0.02300\n",
      "[1915]\tvalidation-rmse:0.02300\n",
      "[1916]\tvalidation-rmse:0.02300\n",
      "[1917]\tvalidation-rmse:0.02300\n",
      "[1918]\tvalidation-rmse:0.02300\n",
      "[1919]\tvalidation-rmse:0.02300\n",
      "[1920]\tvalidation-rmse:0.02300\n",
      "[1921]\tvalidation-rmse:0.02300\n",
      "[1922]\tvalidation-rmse:0.02300\n",
      "[1923]\tvalidation-rmse:0.02300\n",
      "[1924]\tvalidation-rmse:0.02300\n",
      "[1925]\tvalidation-rmse:0.02300\n",
      "[1926]\tvalidation-rmse:0.02300\n",
      "[1927]\tvalidation-rmse:0.02300\n",
      "[1928]\tvalidation-rmse:0.02300\n",
      "[1929]\tvalidation-rmse:0.02300\n",
      "[1930]\tvalidation-rmse:0.02300\n",
      "[1931]\tvalidation-rmse:0.02299\n",
      "[1932]\tvalidation-rmse:0.02299\n",
      "[1933]\tvalidation-rmse:0.02300\n",
      "[1934]\tvalidation-rmse:0.02299\n",
      "[1935]\tvalidation-rmse:0.02299\n",
      "[1936]\tvalidation-rmse:0.02299\n",
      "[1937]\tvalidation-rmse:0.02299\n",
      "[1938]\tvalidation-rmse:0.02299\n",
      "[1939]\tvalidation-rmse:0.02299\n",
      "[1940]\tvalidation-rmse:0.02299\n",
      "[1941]\tvalidation-rmse:0.02299\n",
      "[1942]\tvalidation-rmse:0.02299\n",
      "[1943]\tvalidation-rmse:0.02299\n",
      "[1944]\tvalidation-rmse:0.02298\n",
      "[1945]\tvalidation-rmse:0.02298\n",
      "[1946]\tvalidation-rmse:0.02298\n",
      "[1947]\tvalidation-rmse:0.02298\n",
      "[1948]\tvalidation-rmse:0.02298\n",
      "[1949]\tvalidation-rmse:0.02298\n",
      "[1950]\tvalidation-rmse:0.02298\n",
      "[1951]\tvalidation-rmse:0.02298\n",
      "[1952]\tvalidation-rmse:0.02298\n",
      "[1953]\tvalidation-rmse:0.02298\n",
      "[1954]\tvalidation-rmse:0.02298\n",
      "[1955]\tvalidation-rmse:0.02298\n",
      "[1956]\tvalidation-rmse:0.02298\n",
      "[1957]\tvalidation-rmse:0.02297\n",
      "[1958]\tvalidation-rmse:0.02298\n",
      "[1959]\tvalidation-rmse:0.02298\n",
      "[1960]\tvalidation-rmse:0.02297\n",
      "[1961]\tvalidation-rmse:0.02297\n",
      "[1962]\tvalidation-rmse:0.02297\n",
      "[1963]\tvalidation-rmse:0.02297\n",
      "[1964]\tvalidation-rmse:0.02297\n",
      "[1965]\tvalidation-rmse:0.02297\n",
      "[1966]\tvalidation-rmse:0.02297\n",
      "[1967]\tvalidation-rmse:0.02297\n",
      "[1968]\tvalidation-rmse:0.02297\n",
      "[1969]\tvalidation-rmse:0.02297\n",
      "[1970]\tvalidation-rmse:0.02297\n",
      "[1971]\tvalidation-rmse:0.02297\n",
      "[1972]\tvalidation-rmse:0.02297\n",
      "[1973]\tvalidation-rmse:0.02297\n",
      "[1974]\tvalidation-rmse:0.02297\n",
      "[1975]\tvalidation-rmse:0.02297\n",
      "[1976]\tvalidation-rmse:0.02297\n",
      "[1977]\tvalidation-rmse:0.02297\n",
      "[1978]\tvalidation-rmse:0.02297\n",
      "[1979]\tvalidation-rmse:0.02297\n",
      "[1980]\tvalidation-rmse:0.02297\n",
      "[1981]\tvalidation-rmse:0.02297\n",
      "[1982]\tvalidation-rmse:0.02297\n",
      "[1983]\tvalidation-rmse:0.02297\n",
      "[1984]\tvalidation-rmse:0.02297\n",
      "[1985]\tvalidation-rmse:0.02297\n",
      "[1986]\tvalidation-rmse:0.02296\n",
      "[1987]\tvalidation-rmse:0.02296\n",
      "[1988]\tvalidation-rmse:0.02296\n",
      "[1989]\tvalidation-rmse:0.02296\n",
      "[1990]\tvalidation-rmse:0.02296\n",
      "[1991]\tvalidation-rmse:0.02296\n",
      "[1992]\tvalidation-rmse:0.02296\n",
      "[1993]\tvalidation-rmse:0.02296\n",
      "[1994]\tvalidation-rmse:0.02297\n",
      "[1995]\tvalidation-rmse:0.02297\n",
      "[1996]\tvalidation-rmse:0.02297\n",
      "[1997]\tvalidation-rmse:0.02296\n",
      "[1998]\tvalidation-rmse:0.02296\n",
      "[1999]\tvalidation-rmse:0.02296\n",
      "[2000]\tvalidation-rmse:0.02296\n",
      "[2001]\tvalidation-rmse:0.02296\n",
      "[2002]\tvalidation-rmse:0.02296\n",
      "[2003]\tvalidation-rmse:0.02296\n",
      "[2004]\tvalidation-rmse:0.02296\n",
      "[2005]\tvalidation-rmse:0.02296\n",
      "[2006]\tvalidation-rmse:0.02296\n",
      "[2007]\tvalidation-rmse:0.02296\n",
      "[2008]\tvalidation-rmse:0.02296\n",
      "[2009]\tvalidation-rmse:0.02296\n",
      "[2010]\tvalidation-rmse:0.02296\n",
      "[2011]\tvalidation-rmse:0.02296\n",
      "[2012]\tvalidation-rmse:0.02296\n",
      "[2013]\tvalidation-rmse:0.02295\n",
      "[2014]\tvalidation-rmse:0.02296\n",
      "[2015]\tvalidation-rmse:0.02296\n",
      "[2016]\tvalidation-rmse:0.02296\n",
      "[2017]\tvalidation-rmse:0.02295\n",
      "[2018]\tvalidation-rmse:0.02295\n",
      "[2019]\tvalidation-rmse:0.02295\n",
      "[2020]\tvalidation-rmse:0.02295\n",
      "[2021]\tvalidation-rmse:0.02295\n",
      "[2022]\tvalidation-rmse:0.02295\n",
      "[2023]\tvalidation-rmse:0.02295\n",
      "[2024]\tvalidation-rmse:0.02295\n",
      "[2025]\tvalidation-rmse:0.02295\n",
      "[2026]\tvalidation-rmse:0.02295\n",
      "[2027]\tvalidation-rmse:0.02295\n",
      "[2028]\tvalidation-rmse:0.02295\n",
      "[2029]\tvalidation-rmse:0.02295\n",
      "[2030]\tvalidation-rmse:0.02295\n",
      "[2031]\tvalidation-rmse:0.02295\n",
      "[2032]\tvalidation-rmse:0.02295\n",
      "[2033]\tvalidation-rmse:0.02295\n",
      "[2034]\tvalidation-rmse:0.02295\n",
      "[2035]\tvalidation-rmse:0.02295\n",
      "[2036]\tvalidation-rmse:0.02294\n",
      "[2037]\tvalidation-rmse:0.02295\n",
      "[2038]\tvalidation-rmse:0.02295\n",
      "[2039]\tvalidation-rmse:0.02295\n",
      "[2040]\tvalidation-rmse:0.02295\n",
      "[2041]\tvalidation-rmse:0.02295\n",
      "[2042]\tvalidation-rmse:0.02295\n",
      "[2043]\tvalidation-rmse:0.02295\n",
      "[2044]\tvalidation-rmse:0.02295\n",
      "[2045]\tvalidation-rmse:0.02295\n",
      "[2046]\tvalidation-rmse:0.02295\n",
      "[2047]\tvalidation-rmse:0.02295\n",
      "[2048]\tvalidation-rmse:0.02295\n",
      "[2049]\tvalidation-rmse:0.02295\n",
      "[2050]\tvalidation-rmse:0.02295\n",
      "[2051]\tvalidation-rmse:0.02295\n",
      "[2052]\tvalidation-rmse:0.02295\n",
      "[2053]\tvalidation-rmse:0.02295\n",
      "[2054]\tvalidation-rmse:0.02295\n",
      "[2055]\tvalidation-rmse:0.02295\n",
      "[2056]\tvalidation-rmse:0.02294\n",
      "[2057]\tvalidation-rmse:0.02294\n",
      "[2058]\tvalidation-rmse:0.02294\n",
      "[2059]\tvalidation-rmse:0.02294\n",
      "[2060]\tvalidation-rmse:0.02294\n",
      "[2061]\tvalidation-rmse:0.02294\n",
      "[2062]\tvalidation-rmse:0.02294\n",
      "[2063]\tvalidation-rmse:0.02294\n",
      "[2064]\tvalidation-rmse:0.02294\n",
      "[2065]\tvalidation-rmse:0.02294\n",
      "[2066]\tvalidation-rmse:0.02294\n",
      "[2067]\tvalidation-rmse:0.02294\n",
      "[2068]\tvalidation-rmse:0.02294\n",
      "[2069]\tvalidation-rmse:0.02294\n",
      "[2070]\tvalidation-rmse:0.02294\n",
      "[2071]\tvalidation-rmse:0.02295\n",
      "[2072]\tvalidation-rmse:0.02295\n",
      "[2073]\tvalidation-rmse:0.02295\n",
      "[2074]\tvalidation-rmse:0.02295\n",
      "[2075]\tvalidation-rmse:0.02295\n",
      "[2076]\tvalidation-rmse:0.02295\n",
      "[2077]\tvalidation-rmse:0.02295\n",
      "[2078]\tvalidation-rmse:0.02295\n",
      "[2079]\tvalidation-rmse:0.02295\n",
      "[2080]\tvalidation-rmse:0.02295\n",
      "[2081]\tvalidation-rmse:0.02295\n",
      "[2082]\tvalidation-rmse:0.02295\n",
      "[2083]\tvalidation-rmse:0.02295\n",
      "[2084]\tvalidation-rmse:0.02295\n",
      "[2085]\tvalidation-rmse:0.02295\n",
      "[2086]\tvalidation-rmse:0.02295\n",
      "[2087]\tvalidation-rmse:0.02295\n",
      "[2088]\tvalidation-rmse:0.02294\n",
      "[2089]\tvalidation-rmse:0.02294\n",
      "[2090]\tvalidation-rmse:0.02294\n",
      "[2091]\tvalidation-rmse:0.02294\n",
      "[2092]\tvalidation-rmse:0.02295\n",
      "[2093]\tvalidation-rmse:0.02294\n",
      "[2094]\tvalidation-rmse:0.02294\n",
      "[2095]\tvalidation-rmse:0.02294\n",
      "[2096]\tvalidation-rmse:0.02294\n",
      "[2097]\tvalidation-rmse:0.02294\n",
      "[2098]\tvalidation-rmse:0.02294\n",
      "[2099]\tvalidation-rmse:0.02294\n",
      "[2100]\tvalidation-rmse:0.02294\n",
      "[2101]\tvalidation-rmse:0.02294\n",
      "[2102]\tvalidation-rmse:0.02294\n",
      "[2103]\tvalidation-rmse:0.02294\n",
      "[2104]\tvalidation-rmse:0.02294\n",
      "[2105]\tvalidation-rmse:0.02294\n",
      "[2106]\tvalidation-rmse:0.02294\n",
      "[2107]\tvalidation-rmse:0.02294\n",
      "[2108]\tvalidation-rmse:0.02294\n",
      "[2109]\tvalidation-rmse:0.02294\n",
      "[2110]\tvalidation-rmse:0.02294\n",
      "[2111]\tvalidation-rmse:0.02294\n",
      "[2112]\tvalidation-rmse:0.02294\n",
      "[2113]\tvalidation-rmse:0.02294\n",
      "[2114]\tvalidation-rmse:0.02294\n",
      "[2115]\tvalidation-rmse:0.02294\n",
      "[2116]\tvalidation-rmse:0.02294\n",
      "[2117]\tvalidation-rmse:0.02294\n",
      "[2118]\tvalidation-rmse:0.02294\n",
      "[2119]\tvalidation-rmse:0.02294\n",
      "[2120]\tvalidation-rmse:0.02294\n",
      "[2121]\tvalidation-rmse:0.02294\n",
      "[2122]\tvalidation-rmse:0.02294\n",
      "[2123]\tvalidation-rmse:0.02294\n",
      "[2124]\tvalidation-rmse:0.02294\n",
      "[2125]\tvalidation-rmse:0.02294\n",
      "[2126]\tvalidation-rmse:0.02294\n",
      "[2127]\tvalidation-rmse:0.02294\n",
      "[2128]\tvalidation-rmse:0.02294\n",
      "[2129]\tvalidation-rmse:0.02294\n",
      "[2130]\tvalidation-rmse:0.02294\n",
      "[2131]\tvalidation-rmse:0.02294\n",
      "[2132]\tvalidation-rmse:0.02294\n",
      "[2133]\tvalidation-rmse:0.02293\n",
      "[2134]\tvalidation-rmse:0.02294\n",
      "[2135]\tvalidation-rmse:0.02293\n",
      "[2136]\tvalidation-rmse:0.02293\n",
      "[2137]\tvalidation-rmse:0.02293\n",
      "[2138]\tvalidation-rmse:0.02293\n",
      "[2139]\tvalidation-rmse:0.02293\n",
      "[2140]\tvalidation-rmse:0.02293\n",
      "[2141]\tvalidation-rmse:0.02293\n",
      "[2142]\tvalidation-rmse:0.02293\n",
      "[2143]\tvalidation-rmse:0.02293\n",
      "[2144]\tvalidation-rmse:0.02293\n",
      "[2145]\tvalidation-rmse:0.02293\n",
      "[2146]\tvalidation-rmse:0.02293\n",
      "[2147]\tvalidation-rmse:0.02293\n",
      "[2148]\tvalidation-rmse:0.02293\n",
      "[2149]\tvalidation-rmse:0.02293\n",
      "[2150]\tvalidation-rmse:0.02293\n",
      "[2151]\tvalidation-rmse:0.02293\n",
      "[2152]\tvalidation-rmse:0.02293\n",
      "[2153]\tvalidation-rmse:0.02293\n",
      "[2154]\tvalidation-rmse:0.02293\n",
      "[2155]\tvalidation-rmse:0.02293\n",
      "[2156]\tvalidation-rmse:0.02293\n",
      "[2157]\tvalidation-rmse:0.02293\n",
      "[2158]\tvalidation-rmse:0.02293\n",
      "[2159]\tvalidation-rmse:0.02293\n",
      "[2160]\tvalidation-rmse:0.02293\n",
      "[2161]\tvalidation-rmse:0.02293\n",
      "[2162]\tvalidation-rmse:0.02293\n",
      "[2163]\tvalidation-rmse:0.02293\n",
      "[2164]\tvalidation-rmse:0.02293\n",
      "[2165]\tvalidation-rmse:0.02293\n",
      "[2166]\tvalidation-rmse:0.02293\n",
      "[2167]\tvalidation-rmse:0.02293\n",
      "[2168]\tvalidation-rmse:0.02293\n",
      "[2169]\tvalidation-rmse:0.02293\n",
      "[2170]\tvalidation-rmse:0.02293\n",
      "[2171]\tvalidation-rmse:0.02293\n",
      "[2172]\tvalidation-rmse:0.02293\n",
      "[2173]\tvalidation-rmse:0.02293\n",
      "[2174]\tvalidation-rmse:0.02293\n",
      "[2175]\tvalidation-rmse:0.02293\n",
      "[2176]\tvalidation-rmse:0.02293\n",
      "[2177]\tvalidation-rmse:0.02293\n",
      "[2178]\tvalidation-rmse:0.02292\n",
      "[2179]\tvalidation-rmse:0.02293\n",
      "[2180]\tvalidation-rmse:0.02293\n",
      "[2181]\tvalidation-rmse:0.02293\n",
      "[2182]\tvalidation-rmse:0.02293\n",
      "[2183]\tvalidation-rmse:0.02293\n",
      "[2184]\tvalidation-rmse:0.02293\n",
      "[2185]\tvalidation-rmse:0.02293\n",
      "[2186]\tvalidation-rmse:0.02293\n",
      "[2187]\tvalidation-rmse:0.02293\n",
      "[2188]\tvalidation-rmse:0.02293\n",
      "[2189]\tvalidation-rmse:0.02293\n",
      "[2190]\tvalidation-rmse:0.02293\n",
      "[2191]\tvalidation-rmse:0.02293\n",
      "[2192]\tvalidation-rmse:0.02293\n",
      "[2193]\tvalidation-rmse:0.02293\n",
      "[2194]\tvalidation-rmse:0.02293\n",
      "[2195]\tvalidation-rmse:0.02293\n",
      "[2196]\tvalidation-rmse:0.02293\n",
      "[2197]\tvalidation-rmse:0.02292\n",
      "[2198]\tvalidation-rmse:0.02292\n",
      "[2199]\tvalidation-rmse:0.02292\n",
      "[2200]\tvalidation-rmse:0.02292\n",
      "[2201]\tvalidation-rmse:0.02292\n",
      "[2202]\tvalidation-rmse:0.02292\n",
      "[2203]\tvalidation-rmse:0.02292\n",
      "[2204]\tvalidation-rmse:0.02292\n",
      "[2205]\tvalidation-rmse:0.02292\n",
      "[2206]\tvalidation-rmse:0.02291\n",
      "[2207]\tvalidation-rmse:0.02291\n",
      "[2208]\tvalidation-rmse:0.02291\n",
      "[2209]\tvalidation-rmse:0.02291\n",
      "[2210]\tvalidation-rmse:0.02291\n",
      "[2211]\tvalidation-rmse:0.02291\n",
      "[2212]\tvalidation-rmse:0.02291\n",
      "[2213]\tvalidation-rmse:0.02291\n",
      "[2214]\tvalidation-rmse:0.02291\n",
      "[2215]\tvalidation-rmse:0.02291\n",
      "[2216]\tvalidation-rmse:0.02291\n",
      "[2217]\tvalidation-rmse:0.02291\n",
      "[2218]\tvalidation-rmse:0.02291\n",
      "[2219]\tvalidation-rmse:0.02291\n",
      "[2220]\tvalidation-rmse:0.02291\n",
      "[2221]\tvalidation-rmse:0.02291\n",
      "[2222]\tvalidation-rmse:0.02291\n",
      "[2223]\tvalidation-rmse:0.02291\n",
      "[2224]\tvalidation-rmse:0.02291\n",
      "[2225]\tvalidation-rmse:0.02290\n",
      "[2226]\tvalidation-rmse:0.02290\n",
      "[2227]\tvalidation-rmse:0.02290\n",
      "[2228]\tvalidation-rmse:0.02290\n",
      "[2229]\tvalidation-rmse:0.02290\n",
      "[2230]\tvalidation-rmse:0.02290\n",
      "[2231]\tvalidation-rmse:0.02290\n",
      "[2232]\tvalidation-rmse:0.02290\n",
      "[2233]\tvalidation-rmse:0.02290\n",
      "[2234]\tvalidation-rmse:0.02290\n",
      "[2235]\tvalidation-rmse:0.02290\n",
      "[2236]\tvalidation-rmse:0.02290\n",
      "[2237]\tvalidation-rmse:0.02290\n",
      "[2238]\tvalidation-rmse:0.02290\n",
      "[2239]\tvalidation-rmse:0.02289\n",
      "[2240]\tvalidation-rmse:0.02289\n",
      "[2241]\tvalidation-rmse:0.02289\n",
      "[2242]\tvalidation-rmse:0.02289\n",
      "[2243]\tvalidation-rmse:0.02289\n",
      "[2244]\tvalidation-rmse:0.02289\n",
      "[2245]\tvalidation-rmse:0.02289\n",
      "[2246]\tvalidation-rmse:0.02289\n",
      "[2247]\tvalidation-rmse:0.02289\n",
      "[2248]\tvalidation-rmse:0.02289\n",
      "[2249]\tvalidation-rmse:0.02289\n",
      "[2250]\tvalidation-rmse:0.02289\n",
      "[2251]\tvalidation-rmse:0.02289\n",
      "[2252]\tvalidation-rmse:0.02289\n",
      "[2253]\tvalidation-rmse:0.02289\n",
      "[2254]\tvalidation-rmse:0.02289\n",
      "[2255]\tvalidation-rmse:0.02289\n",
      "[2256]\tvalidation-rmse:0.02289\n",
      "[2257]\tvalidation-rmse:0.02289\n",
      "[2258]\tvalidation-rmse:0.02289\n",
      "[2259]\tvalidation-rmse:0.02289\n",
      "[2260]\tvalidation-rmse:0.02289\n",
      "[2261]\tvalidation-rmse:0.02289\n",
      "[2262]\tvalidation-rmse:0.02289\n",
      "[2263]\tvalidation-rmse:0.02289\n",
      "[2264]\tvalidation-rmse:0.02289\n",
      "[2265]\tvalidation-rmse:0.02289\n",
      "[2266]\tvalidation-rmse:0.02289\n",
      "[2267]\tvalidation-rmse:0.02288\n",
      "[2268]\tvalidation-rmse:0.02288\n",
      "[2269]\tvalidation-rmse:0.02289\n",
      "[2270]\tvalidation-rmse:0.02289\n",
      "[2271]\tvalidation-rmse:0.02289\n",
      "[2272]\tvalidation-rmse:0.02289\n",
      "[2273]\tvalidation-rmse:0.02289\n",
      "[2274]\tvalidation-rmse:0.02289\n",
      "[2275]\tvalidation-rmse:0.02289\n",
      "[2276]\tvalidation-rmse:0.02289\n",
      "[2277]\tvalidation-rmse:0.02289\n",
      "[2278]\tvalidation-rmse:0.02289\n",
      "[2279]\tvalidation-rmse:0.02289\n",
      "[2280]\tvalidation-rmse:0.02288\n",
      "[2281]\tvalidation-rmse:0.02288\n",
      "[2282]\tvalidation-rmse:0.02288\n",
      "[2283]\tvalidation-rmse:0.02288\n",
      "[2284]\tvalidation-rmse:0.02288\n",
      "[2285]\tvalidation-rmse:0.02288\n",
      "[2286]\tvalidation-rmse:0.02288\n",
      "[2287]\tvalidation-rmse:0.02288\n",
      "[2288]\tvalidation-rmse:0.02288\n",
      "[2289]\tvalidation-rmse:0.02288\n",
      "[2290]\tvalidation-rmse:0.02288\n",
      "[2291]\tvalidation-rmse:0.02288\n",
      "[2292]\tvalidation-rmse:0.02288\n",
      "[2293]\tvalidation-rmse:0.02288\n",
      "[2294]\tvalidation-rmse:0.02288\n",
      "[2295]\tvalidation-rmse:0.02288\n",
      "[2296]\tvalidation-rmse:0.02288\n",
      "[2297]\tvalidation-rmse:0.02288\n",
      "[2298]\tvalidation-rmse:0.02288\n",
      "[2299]\tvalidation-rmse:0.02288\n",
      "[2300]\tvalidation-rmse:0.02287\n",
      "[2301]\tvalidation-rmse:0.02287\n",
      "[2302]\tvalidation-rmse:0.02287\n",
      "[2303]\tvalidation-rmse:0.02287\n",
      "[2304]\tvalidation-rmse:0.02287\n",
      "[2305]\tvalidation-rmse:0.02287\n",
      "[2306]\tvalidation-rmse:0.02287\n",
      "[2307]\tvalidation-rmse:0.02287\n",
      "[2308]\tvalidation-rmse:0.02287\n",
      "[2309]\tvalidation-rmse:0.02287\n",
      "[2310]\tvalidation-rmse:0.02287\n",
      "[2311]\tvalidation-rmse:0.02287\n",
      "[2312]\tvalidation-rmse:0.02287\n",
      "[2313]\tvalidation-rmse:0.02287\n",
      "[2314]\tvalidation-rmse:0.02287\n",
      "[2315]\tvalidation-rmse:0.02286\n",
      "[2316]\tvalidation-rmse:0.02286\n",
      "[2317]\tvalidation-rmse:0.02286\n",
      "[2318]\tvalidation-rmse:0.02286\n",
      "[2319]\tvalidation-rmse:0.02286\n",
      "[2320]\tvalidation-rmse:0.02286\n",
      "[2321]\tvalidation-rmse:0.02286\n",
      "[2322]\tvalidation-rmse:0.02286\n",
      "[2323]\tvalidation-rmse:0.02286\n",
      "[2324]\tvalidation-rmse:0.02286\n",
      "[2325]\tvalidation-rmse:0.02286\n",
      "[2326]\tvalidation-rmse:0.02286\n",
      "[2327]\tvalidation-rmse:0.02286\n",
      "[2328]\tvalidation-rmse:0.02286\n",
      "[2329]\tvalidation-rmse:0.02286\n",
      "[2330]\tvalidation-rmse:0.02286\n",
      "[2331]\tvalidation-rmse:0.02285\n",
      "[2332]\tvalidation-rmse:0.02285\n",
      "[2333]\tvalidation-rmse:0.02285\n",
      "[2334]\tvalidation-rmse:0.02285\n",
      "[2335]\tvalidation-rmse:0.02285\n",
      "[2336]\tvalidation-rmse:0.02285\n",
      "[2337]\tvalidation-rmse:0.02285\n",
      "[2338]\tvalidation-rmse:0.02285\n",
      "[2339]\tvalidation-rmse:0.02285\n",
      "[2340]\tvalidation-rmse:0.02285\n",
      "[2341]\tvalidation-rmse:0.02285\n",
      "[2342]\tvalidation-rmse:0.02286\n",
      "[2343]\tvalidation-rmse:0.02286\n",
      "[2344]\tvalidation-rmse:0.02286\n",
      "[2345]\tvalidation-rmse:0.02286\n",
      "[2346]\tvalidation-rmse:0.02286\n",
      "[2347]\tvalidation-rmse:0.02286\n",
      "[2348]\tvalidation-rmse:0.02286\n",
      "[2349]\tvalidation-rmse:0.02286\n",
      "[2350]\tvalidation-rmse:0.02285\n",
      "[2351]\tvalidation-rmse:0.02286\n",
      "[2352]\tvalidation-rmse:0.02286\n",
      "[2353]\tvalidation-rmse:0.02285\n",
      "[2354]\tvalidation-rmse:0.02285\n",
      "[2355]\tvalidation-rmse:0.02286\n",
      "[2356]\tvalidation-rmse:0.02285\n",
      "[2357]\tvalidation-rmse:0.02285\n",
      "[2358]\tvalidation-rmse:0.02285\n",
      "[2359]\tvalidation-rmse:0.02285\n",
      "[2360]\tvalidation-rmse:0.02285\n",
      "[2361]\tvalidation-rmse:0.02285\n",
      "[2362]\tvalidation-rmse:0.02285\n",
      "[2363]\tvalidation-rmse:0.02285\n",
      "[2364]\tvalidation-rmse:0.02285\n",
      "[2365]\tvalidation-rmse:0.02285\n",
      "[2366]\tvalidation-rmse:0.02285\n",
      "[2367]\tvalidation-rmse:0.02285\n",
      "[2368]\tvalidation-rmse:0.02285\n",
      "[2369]\tvalidation-rmse:0.02285\n",
      "[2370]\tvalidation-rmse:0.02285\n",
      "[2371]\tvalidation-rmse:0.02285\n",
      "[2372]\tvalidation-rmse:0.02285\n",
      "[2373]\tvalidation-rmse:0.02285\n",
      "[2374]\tvalidation-rmse:0.02285\n",
      "[2375]\tvalidation-rmse:0.02284\n",
      "[2376]\tvalidation-rmse:0.02284\n",
      "[2377]\tvalidation-rmse:0.02284\n",
      "[2378]\tvalidation-rmse:0.02284\n",
      "[2379]\tvalidation-rmse:0.02284\n",
      "[2380]\tvalidation-rmse:0.02284\n",
      "[2381]\tvalidation-rmse:0.02284\n",
      "[2382]\tvalidation-rmse:0.02284\n",
      "[2383]\tvalidation-rmse:0.02284\n",
      "[2384]\tvalidation-rmse:0.02284\n",
      "[2385]\tvalidation-rmse:0.02284\n",
      "[2386]\tvalidation-rmse:0.02284\n",
      "[2387]\tvalidation-rmse:0.02284\n",
      "[2388]\tvalidation-rmse:0.02284\n",
      "[2389]\tvalidation-rmse:0.02284\n",
      "[2390]\tvalidation-rmse:0.02284\n",
      "[2391]\tvalidation-rmse:0.02284\n",
      "[2392]\tvalidation-rmse:0.02284\n",
      "[2393]\tvalidation-rmse:0.02284\n",
      "[2394]\tvalidation-rmse:0.02284\n",
      "[2395]\tvalidation-rmse:0.02285\n",
      "[2396]\tvalidation-rmse:0.02285\n",
      "[2397]\tvalidation-rmse:0.02285\n",
      "[2398]\tvalidation-rmse:0.02285\n",
      "[2399]\tvalidation-rmse:0.02285\n",
      "[2400]\tvalidation-rmse:0.02285\n",
      "[2401]\tvalidation-rmse:0.02285\n",
      "[2402]\tvalidation-rmse:0.02285\n",
      "[2403]\tvalidation-rmse:0.02285\n",
      "[2404]\tvalidation-rmse:0.02285\n",
      "[2405]\tvalidation-rmse:0.02285\n",
      "[2406]\tvalidation-rmse:0.02285\n",
      "[2407]\tvalidation-rmse:0.02284\n",
      "[2408]\tvalidation-rmse:0.02284\n",
      "[2409]\tvalidation-rmse:0.02284\n",
      "[2410]\tvalidation-rmse:0.02284\n",
      "[2411]\tvalidation-rmse:0.02284\n",
      "[2412]\tvalidation-rmse:0.02284\n",
      "[2413]\tvalidation-rmse:0.02284\n",
      "[2414]\tvalidation-rmse:0.02284\n",
      "[2415]\tvalidation-rmse:0.02284\n",
      "[2416]\tvalidation-rmse:0.02284\n",
      "[2417]\tvalidation-rmse:0.02284\n",
      "[2418]\tvalidation-rmse:0.02284\n",
      "[2419]\tvalidation-rmse:0.02284\n",
      "[2420]\tvalidation-rmse:0.02284\n",
      "[2421]\tvalidation-rmse:0.02284\n",
      "[2422]\tvalidation-rmse:0.02284\n",
      "[2423]\tvalidation-rmse:0.02284\n",
      "[2424]\tvalidation-rmse:0.02284\n",
      "[2425]\tvalidation-rmse:0.02284\n",
      "[2426]\tvalidation-rmse:0.02284\n",
      "[2427]\tvalidation-rmse:0.02284\n",
      "[2428]\tvalidation-rmse:0.02284\n",
      "[2429]\tvalidation-rmse:0.02284\n",
      "[2430]\tvalidation-rmse:0.02284\n",
      "[2431]\tvalidation-rmse:0.02284\n",
      "[2432]\tvalidation-rmse:0.02284\n",
      "[2433]\tvalidation-rmse:0.02284\n",
      "[2434]\tvalidation-rmse:0.02284\n",
      "[2435]\tvalidation-rmse:0.02284\n",
      "[2436]\tvalidation-rmse:0.02284\n",
      "[2437]\tvalidation-rmse:0.02284\n",
      "[2438]\tvalidation-rmse:0.02284\n",
      "[2439]\tvalidation-rmse:0.02283\n",
      "[2440]\tvalidation-rmse:0.02283\n",
      "[2441]\tvalidation-rmse:0.02283\n",
      "[2442]\tvalidation-rmse:0.02283\n",
      "[2443]\tvalidation-rmse:0.02283\n",
      "[2444]\tvalidation-rmse:0.02283\n",
      "[2445]\tvalidation-rmse:0.02283\n",
      "[2446]\tvalidation-rmse:0.02283\n",
      "[2447]\tvalidation-rmse:0.02283\n",
      "[2448]\tvalidation-rmse:0.02283\n",
      "[2449]\tvalidation-rmse:0.02283\n",
      "[2450]\tvalidation-rmse:0.02283\n",
      "[2451]\tvalidation-rmse:0.02283\n",
      "[2452]\tvalidation-rmse:0.02283\n",
      "[2453]\tvalidation-rmse:0.02283\n",
      "[2454]\tvalidation-rmse:0.02283\n",
      "[2455]\tvalidation-rmse:0.02283\n",
      "[2456]\tvalidation-rmse:0.02283\n",
      "[2457]\tvalidation-rmse:0.02283\n",
      "[2458]\tvalidation-rmse:0.02283\n",
      "[2459]\tvalidation-rmse:0.02283\n",
      "[2460]\tvalidation-rmse:0.02283\n",
      "[2461]\tvalidation-rmse:0.02283\n",
      "[2462]\tvalidation-rmse:0.02283\n",
      "[2463]\tvalidation-rmse:0.02283\n",
      "[2464]\tvalidation-rmse:0.02283\n",
      "[2465]\tvalidation-rmse:0.02283\n",
      "[2466]\tvalidation-rmse:0.02283\n",
      "[2467]\tvalidation-rmse:0.02283\n",
      "[2468]\tvalidation-rmse:0.02283\n",
      "[2469]\tvalidation-rmse:0.02283\n",
      "[2470]\tvalidation-rmse:0.02283\n",
      "[2471]\tvalidation-rmse:0.02283\n",
      "[2472]\tvalidation-rmse:0.02283\n",
      "[2473]\tvalidation-rmse:0.02283\n",
      "[2474]\tvalidation-rmse:0.02283\n",
      "[2475]\tvalidation-rmse:0.02283\n",
      "[2476]\tvalidation-rmse:0.02282\n",
      "[2477]\tvalidation-rmse:0.02282\n",
      "[2478]\tvalidation-rmse:0.02282\n",
      "[2479]\tvalidation-rmse:0.02282\n",
      "[2480]\tvalidation-rmse:0.02282\n",
      "[2481]\tvalidation-rmse:0.02282\n",
      "[2482]\tvalidation-rmse:0.02282\n",
      "[2483]\tvalidation-rmse:0.02282\n",
      "[2484]\tvalidation-rmse:0.02282\n",
      "[2485]\tvalidation-rmse:0.02282\n",
      "[2486]\tvalidation-rmse:0.02282\n",
      "[2487]\tvalidation-rmse:0.02282\n",
      "[2488]\tvalidation-rmse:0.02282\n",
      "[2489]\tvalidation-rmse:0.02282\n",
      "[2490]\tvalidation-rmse:0.02282\n",
      "[2491]\tvalidation-rmse:0.02282\n",
      "[2492]\tvalidation-rmse:0.02282\n",
      "[2493]\tvalidation-rmse:0.02282\n",
      "[2494]\tvalidation-rmse:0.02282\n",
      "[2495]\tvalidation-rmse:0.02282\n",
      "[2496]\tvalidation-rmse:0.02282\n",
      "[2497]\tvalidation-rmse:0.02282\n",
      "[2498]\tvalidation-rmse:0.02282\n",
      "[2499]\tvalidation-rmse:0.02282\n",
      "[2500]\tvalidation-rmse:0.02282\n",
      "[2501]\tvalidation-rmse:0.02282\n",
      "[2502]\tvalidation-rmse:0.02282\n",
      "[2503]\tvalidation-rmse:0.02282\n",
      "[2504]\tvalidation-rmse:0.02282\n",
      "[2505]\tvalidation-rmse:0.02282\n",
      "[2506]\tvalidation-rmse:0.02282\n",
      "[2507]\tvalidation-rmse:0.02282\n",
      "[2508]\tvalidation-rmse:0.02282\n",
      "[2509]\tvalidation-rmse:0.02282\n",
      "[2510]\tvalidation-rmse:0.02282\n",
      "[2511]\tvalidation-rmse:0.02282\n",
      "[2512]\tvalidation-rmse:0.02282\n",
      "[2513]\tvalidation-rmse:0.02282\n",
      "[2514]\tvalidation-rmse:0.02282\n",
      "[2515]\tvalidation-rmse:0.02282\n",
      "[2516]\tvalidation-rmse:0.02282\n",
      "[2517]\tvalidation-rmse:0.02282\n",
      "[2518]\tvalidation-rmse:0.02282\n",
      "[2519]\tvalidation-rmse:0.02282\n",
      "[2520]\tvalidation-rmse:0.02282\n",
      "[2521]\tvalidation-rmse:0.02282\n",
      "[2522]\tvalidation-rmse:0.02282\n",
      "[2523]\tvalidation-rmse:0.02282\n",
      "[2524]\tvalidation-rmse:0.02282\n",
      "[2525]\tvalidation-rmse:0.02282\n",
      "[2526]\tvalidation-rmse:0.02282\n",
      "[2527]\tvalidation-rmse:0.02282\n",
      "[2528]\tvalidation-rmse:0.02282\n",
      "[2529]\tvalidation-rmse:0.02281\n",
      "[2530]\tvalidation-rmse:0.02281\n",
      "[2531]\tvalidation-rmse:0.02281\n",
      "[2532]\tvalidation-rmse:0.02281\n",
      "[2533]\tvalidation-rmse:0.02281\n",
      "[2534]\tvalidation-rmse:0.02281\n",
      "[2535]\tvalidation-rmse:0.02281\n",
      "[2536]\tvalidation-rmse:0.02281\n",
      "[2537]\tvalidation-rmse:0.02281\n",
      "[2538]\tvalidation-rmse:0.02281\n",
      "[2539]\tvalidation-rmse:0.02281\n",
      "[2540]\tvalidation-rmse:0.02281\n",
      "[2541]\tvalidation-rmse:0.02281\n",
      "[2542]\tvalidation-rmse:0.02281\n",
      "[2543]\tvalidation-rmse:0.02281\n",
      "[2544]\tvalidation-rmse:0.02281\n",
      "[2545]\tvalidation-rmse:0.02281\n",
      "[2546]\tvalidation-rmse:0.02281\n",
      "[2547]\tvalidation-rmse:0.02281\n",
      "[2548]\tvalidation-rmse:0.02281\n",
      "[2549]\tvalidation-rmse:0.02281\n",
      "[2550]\tvalidation-rmse:0.02281\n",
      "[2551]\tvalidation-rmse:0.02281\n",
      "[2552]\tvalidation-rmse:0.02281\n",
      "[2553]\tvalidation-rmse:0.02280\n",
      "[2554]\tvalidation-rmse:0.02280\n",
      "[2555]\tvalidation-rmse:0.02280\n",
      "[2556]\tvalidation-rmse:0.02280\n",
      "[2557]\tvalidation-rmse:0.02280\n",
      "[2558]\tvalidation-rmse:0.02280\n",
      "[2559]\tvalidation-rmse:0.02280\n",
      "[2560]\tvalidation-rmse:0.02280\n",
      "[2561]\tvalidation-rmse:0.02280\n",
      "[2562]\tvalidation-rmse:0.02280\n",
      "[2563]\tvalidation-rmse:0.02281\n",
      "[2564]\tvalidation-rmse:0.02281\n",
      "[2565]\tvalidation-rmse:0.02280\n",
      "[2566]\tvalidation-rmse:0.02280\n",
      "[2567]\tvalidation-rmse:0.02280\n",
      "[2568]\tvalidation-rmse:0.02280\n",
      "[2569]\tvalidation-rmse:0.02280\n",
      "[2570]\tvalidation-rmse:0.02280\n",
      "[2571]\tvalidation-rmse:0.02280\n",
      "[2572]\tvalidation-rmse:0.02280\n",
      "[2573]\tvalidation-rmse:0.02280\n",
      "[2574]\tvalidation-rmse:0.02280\n",
      "[2575]\tvalidation-rmse:0.02280\n",
      "[2576]\tvalidation-rmse:0.02280\n",
      "[2577]\tvalidation-rmse:0.02280\n",
      "[2578]\tvalidation-rmse:0.02280\n",
      "[2579]\tvalidation-rmse:0.02280\n",
      "[2580]\tvalidation-rmse:0.02280\n",
      "[2581]\tvalidation-rmse:0.02280\n",
      "[2582]\tvalidation-rmse:0.02280\n",
      "[2583]\tvalidation-rmse:0.02280\n",
      "[2584]\tvalidation-rmse:0.02280\n",
      "[2585]\tvalidation-rmse:0.02280\n",
      "[2586]\tvalidation-rmse:0.02280\n",
      "[2587]\tvalidation-rmse:0.02280\n",
      "[2588]\tvalidation-rmse:0.02280\n",
      "[2589]\tvalidation-rmse:0.02280\n",
      "[2590]\tvalidation-rmse:0.02280\n",
      "[2591]\tvalidation-rmse:0.02279\n",
      "[2592]\tvalidation-rmse:0.02279\n",
      "[2593]\tvalidation-rmse:0.02279\n",
      "[2594]\tvalidation-rmse:0.02279\n",
      "[2595]\tvalidation-rmse:0.02279\n",
      "[2596]\tvalidation-rmse:0.02279\n",
      "[2597]\tvalidation-rmse:0.02279\n",
      "[2598]\tvalidation-rmse:0.02279\n",
      "[2599]\tvalidation-rmse:0.02279\n",
      "[2600]\tvalidation-rmse:0.02279\n",
      "[2601]\tvalidation-rmse:0.02279\n",
      "[2602]\tvalidation-rmse:0.02279\n",
      "[2603]\tvalidation-rmse:0.02279\n",
      "[2604]\tvalidation-rmse:0.02279\n",
      "[2605]\tvalidation-rmse:0.02279\n",
      "[2606]\tvalidation-rmse:0.02279\n",
      "[2607]\tvalidation-rmse:0.02279\n",
      "[2608]\tvalidation-rmse:0.02279\n",
      "[2609]\tvalidation-rmse:0.02279\n",
      "[2610]\tvalidation-rmse:0.02279\n",
      "[2611]\tvalidation-rmse:0.02279\n",
      "[2612]\tvalidation-rmse:0.02279\n",
      "[2613]\tvalidation-rmse:0.02279\n",
      "[2614]\tvalidation-rmse:0.02279\n",
      "[2615]\tvalidation-rmse:0.02279\n",
      "[2616]\tvalidation-rmse:0.02279\n",
      "[2617]\tvalidation-rmse:0.02279\n",
      "[2618]\tvalidation-rmse:0.02279\n",
      "[2619]\tvalidation-rmse:0.02279\n",
      "[2620]\tvalidation-rmse:0.02279\n",
      "[2621]\tvalidation-rmse:0.02279\n",
      "[2622]\tvalidation-rmse:0.02279\n",
      "[2623]\tvalidation-rmse:0.02279\n",
      "[2624]\tvalidation-rmse:0.02279\n",
      "[2625]\tvalidation-rmse:0.02279\n",
      "[2626]\tvalidation-rmse:0.02279\n",
      "[2627]\tvalidation-rmse:0.02279\n",
      "[2628]\tvalidation-rmse:0.02278\n",
      "[2629]\tvalidation-rmse:0.02278\n",
      "[2630]\tvalidation-rmse:0.02279\n",
      "[2631]\tvalidation-rmse:0.02279\n",
      "[2632]\tvalidation-rmse:0.02279\n",
      "[2633]\tvalidation-rmse:0.02278\n",
      "[2634]\tvalidation-rmse:0.02278\n",
      "[2635]\tvalidation-rmse:0.02278\n",
      "[2636]\tvalidation-rmse:0.02278\n",
      "[2637]\tvalidation-rmse:0.02278\n",
      "[2638]\tvalidation-rmse:0.02278\n",
      "[2639]\tvalidation-rmse:0.02278\n",
      "[2640]\tvalidation-rmse:0.02278\n",
      "[2641]\tvalidation-rmse:0.02278\n",
      "[2642]\tvalidation-rmse:0.02278\n",
      "[2643]\tvalidation-rmse:0.02278\n",
      "[2644]\tvalidation-rmse:0.02278\n",
      "[2645]\tvalidation-rmse:0.02278\n",
      "[2646]\tvalidation-rmse:0.02278\n",
      "[2647]\tvalidation-rmse:0.02278\n",
      "[2648]\tvalidation-rmse:0.02278\n",
      "[2649]\tvalidation-rmse:0.02278\n",
      "[2650]\tvalidation-rmse:0.02278\n",
      "[2651]\tvalidation-rmse:0.02278\n",
      "[2652]\tvalidation-rmse:0.02278\n",
      "[2653]\tvalidation-rmse:0.02278\n",
      "[2654]\tvalidation-rmse:0.02278\n",
      "[2655]\tvalidation-rmse:0.02279\n",
      "[2656]\tvalidation-rmse:0.02279\n",
      "[2657]\tvalidation-rmse:0.02279\n",
      "[2658]\tvalidation-rmse:0.02279\n",
      "[2659]\tvalidation-rmse:0.02279\n",
      "[2660]\tvalidation-rmse:0.02279\n",
      "[2661]\tvalidation-rmse:0.02279\n",
      "[2662]\tvalidation-rmse:0.02279\n",
      "[2663]\tvalidation-rmse:0.02279\n",
      "[2664]\tvalidation-rmse:0.02279\n",
      "[2665]\tvalidation-rmse:0.02279\n",
      "[2666]\tvalidation-rmse:0.02279\n",
      "[2667]\tvalidation-rmse:0.02279\n",
      "[2668]\tvalidation-rmse:0.02279\n",
      "[2669]\tvalidation-rmse:0.02279\n",
      "[2670]\tvalidation-rmse:0.02279\n",
      "[2671]\tvalidation-rmse:0.02279\n",
      "[2672]\tvalidation-rmse:0.02278\n",
      "[2673]\tvalidation-rmse:0.02278\n",
      "[2674]\tvalidation-rmse:0.02278\n",
      "[2675]\tvalidation-rmse:0.02278\n",
      "[2676]\tvalidation-rmse:0.02278\n",
      "[2677]\tvalidation-rmse:0.02278\n",
      "[2678]\tvalidation-rmse:0.02278\n",
      "[2679]\tvalidation-rmse:0.02278\n",
      "[2680]\tvalidation-rmse:0.02278\n",
      "[2681]\tvalidation-rmse:0.02278\n",
      "[2682]\tvalidation-rmse:0.02278\n",
      "[2683]\tvalidation-rmse:0.02278\n",
      "[2684]\tvalidation-rmse:0.02278\n",
      "[2685]\tvalidation-rmse:0.02278\n",
      "[2686]\tvalidation-rmse:0.02278\n",
      "[2687]\tvalidation-rmse:0.02278\n",
      "[2688]\tvalidation-rmse:0.02278\n",
      "[2689]\tvalidation-rmse:0.02278\n",
      "[2690]\tvalidation-rmse:0.02278\n",
      "[2691]\tvalidation-rmse:0.02278\n",
      "[2692]\tvalidation-rmse:0.02278\n",
      "[2693]\tvalidation-rmse:0.02278\n",
      "[2694]\tvalidation-rmse:0.02278\n",
      "[2695]\tvalidation-rmse:0.02278\n",
      "[2696]\tvalidation-rmse:0.02278\n",
      "[2697]\tvalidation-rmse:0.02278\n",
      "[2698]\tvalidation-rmse:0.02278\n",
      "[2699]\tvalidation-rmse:0.02278\n",
      "[2700]\tvalidation-rmse:0.02278\n",
      "[2701]\tvalidation-rmse:0.02278\n",
      "[2702]\tvalidation-rmse:0.02278\n",
      "[2703]\tvalidation-rmse:0.02278\n",
      "[2704]\tvalidation-rmse:0.02278\n",
      "[2705]\tvalidation-rmse:0.02278\n",
      "[2706]\tvalidation-rmse:0.02278\n",
      "[2707]\tvalidation-rmse:0.02277\n",
      "[2708]\tvalidation-rmse:0.02278\n",
      "[2709]\tvalidation-rmse:0.02277\n",
      "[2710]\tvalidation-rmse:0.02277\n",
      "[2711]\tvalidation-rmse:0.02277\n",
      "[2712]\tvalidation-rmse:0.02278\n",
      "[2713]\tvalidation-rmse:0.02278\n",
      "[2714]\tvalidation-rmse:0.02278\n",
      "[2715]\tvalidation-rmse:0.02278\n",
      "[2716]\tvalidation-rmse:0.02278\n",
      "[2717]\tvalidation-rmse:0.02278\n",
      "[2718]\tvalidation-rmse:0.02278\n",
      "[2719]\tvalidation-rmse:0.02278\n",
      "[2720]\tvalidation-rmse:0.02278\n",
      "[2721]\tvalidation-rmse:0.02278\n",
      "[2722]\tvalidation-rmse:0.02279\n",
      "[2723]\tvalidation-rmse:0.02279\n",
      "[2724]\tvalidation-rmse:0.02279\n",
      "[2725]\tvalidation-rmse:0.02279\n",
      "[2726]\tvalidation-rmse:0.02279\n",
      "[2727]\tvalidation-rmse:0.02279\n",
      "[2728]\tvalidation-rmse:0.02279\n",
      "[2729]\tvalidation-rmse:0.02279\n",
      "[2730]\tvalidation-rmse:0.02279\n",
      "[2731]\tvalidation-rmse:0.02279\n",
      "[2732]\tvalidation-rmse:0.02279\n",
      "[2733]\tvalidation-rmse:0.02279\n",
      "[2734]\tvalidation-rmse:0.02279\n",
      "[2735]\tvalidation-rmse:0.02279\n",
      "[2736]\tvalidation-rmse:0.02279\n",
      "[2737]\tvalidation-rmse:0.02279\n",
      "[2738]\tvalidation-rmse:0.02279\n",
      "[2739]\tvalidation-rmse:0.02279\n",
      "[2740]\tvalidation-rmse:0.02279\n",
      "[2741]\tvalidation-rmse:0.02279\n",
      "[2742]\tvalidation-rmse:0.02279\n",
      "[2743]\tvalidation-rmse:0.02279\n",
      "[2744]\tvalidation-rmse:0.02279\n",
      "[2745]\tvalidation-rmse:0.02279\n",
      "[2746]\tvalidation-rmse:0.02278\n",
      "[2747]\tvalidation-rmse:0.02278\n",
      "[2748]\tvalidation-rmse:0.02278\n",
      "[2749]\tvalidation-rmse:0.02278\n",
      "[2750]\tvalidation-rmse:0.02278\n",
      "[2751]\tvalidation-rmse:0.02278\n",
      "[2752]\tvalidation-rmse:0.02278\n",
      "[2753]\tvalidation-rmse:0.02278\n",
      "[2754]\tvalidation-rmse:0.02278\n",
      "[2755]\tvalidation-rmse:0.02278\n",
      "[2756]\tvalidation-rmse:0.02278\n",
      "[2757]\tvalidation-rmse:0.02278\n",
      "[2758]\tvalidation-rmse:0.02278\n",
      "[2759]\tvalidation-rmse:0.02278\n",
      "[2760]\tvalidation-rmse:0.02278\n",
      "[2761]\tvalidation-rmse:0.02278\n",
      "[2762]\tvalidation-rmse:0.02278\n",
      "[2763]\tvalidation-rmse:0.02278\n",
      "[2764]\tvalidation-rmse:0.02278\n",
      "[2765]\tvalidation-rmse:0.02278\n",
      "[2766]\tvalidation-rmse:0.02278\n",
      "[2767]\tvalidation-rmse:0.02278\n",
      "[2768]\tvalidation-rmse:0.02278\n",
      "[2769]\tvalidation-rmse:0.02278\n",
      "[2770]\tvalidation-rmse:0.02278\n",
      "[2771]\tvalidation-rmse:0.02278\n",
      "[2772]\tvalidation-rmse:0.02278\n",
      "[2773]\tvalidation-rmse:0.02278\n",
      "[2774]\tvalidation-rmse:0.02278\n",
      "[2775]\tvalidation-rmse:0.02278\n",
      "[2776]\tvalidation-rmse:0.02278\n",
      "[2777]\tvalidation-rmse:0.02278\n",
      "[2778]\tvalidation-rmse:0.02278\n",
      "[2779]\tvalidation-rmse:0.02278\n",
      "[2780]\tvalidation-rmse:0.02278\n",
      "[2781]\tvalidation-rmse:0.02278\n",
      "[2782]\tvalidation-rmse:0.02278\n",
      "[2783]\tvalidation-rmse:0.02278\n",
      "[2784]\tvalidation-rmse:0.02278\n",
      "[2785]\tvalidation-rmse:0.02278\n",
      "[2786]\tvalidation-rmse:0.02278\n",
      "[2787]\tvalidation-rmse:0.02278\n",
      "[2788]\tvalidation-rmse:0.02278\n",
      "[2789]\tvalidation-rmse:0.02278\n",
      "[2790]\tvalidation-rmse:0.02278\n",
      "[2791]\tvalidation-rmse:0.02278\n",
      "[2792]\tvalidation-rmse:0.02277\n",
      "[2793]\tvalidation-rmse:0.02277\n",
      "[2794]\tvalidation-rmse:0.02277\n",
      "[2795]\tvalidation-rmse:0.02277\n",
      "[2796]\tvalidation-rmse:0.02277\n",
      "[2797]\tvalidation-rmse:0.02277\n",
      "[2798]\tvalidation-rmse:0.02277\n",
      "[2799]\tvalidation-rmse:0.02277\n",
      "[2800]\tvalidation-rmse:0.02277\n",
      "[2801]\tvalidation-rmse:0.02277\n",
      "[2802]\tvalidation-rmse:0.02278\n",
      "[2803]\tvalidation-rmse:0.02278\n",
      "[2804]\tvalidation-rmse:0.02277\n",
      "[2805]\tvalidation-rmse:0.02277\n",
      "[2806]\tvalidation-rmse:0.02277\n",
      "[2807]\tvalidation-rmse:0.02277\n",
      "[2808]\tvalidation-rmse:0.02277\n",
      "[2809]\tvalidation-rmse:0.02277\n",
      "[2810]\tvalidation-rmse:0.02277\n",
      "[2811]\tvalidation-rmse:0.02277\n",
      "[2812]\tvalidation-rmse:0.02277\n",
      "[2813]\tvalidation-rmse:0.02277\n",
      "[2814]\tvalidation-rmse:0.02277\n",
      "[2815]\tvalidation-rmse:0.02277\n",
      "[2816]\tvalidation-rmse:0.02277\n",
      "[2817]\tvalidation-rmse:0.02277\n",
      "[2818]\tvalidation-rmse:0.02277\n",
      "[2819]\tvalidation-rmse:0.02277\n",
      "[2820]\tvalidation-rmse:0.02277\n",
      "[2821]\tvalidation-rmse:0.02277\n",
      "[2822]\tvalidation-rmse:0.02277\n",
      "[2823]\tvalidation-rmse:0.02277\n",
      "[2824]\tvalidation-rmse:0.02276\n",
      "[2825]\tvalidation-rmse:0.02276\n",
      "[2826]\tvalidation-rmse:0.02276\n",
      "[2827]\tvalidation-rmse:0.02276\n",
      "[2828]\tvalidation-rmse:0.02276\n",
      "[2829]\tvalidation-rmse:0.02276\n",
      "[2830]\tvalidation-rmse:0.02277\n",
      "[2831]\tvalidation-rmse:0.02277\n",
      "[2832]\tvalidation-rmse:0.02277\n",
      "[2833]\tvalidation-rmse:0.02276\n",
      "[2834]\tvalidation-rmse:0.02276\n",
      "[2835]\tvalidation-rmse:0.02276\n",
      "[2836]\tvalidation-rmse:0.02276\n",
      "[2837]\tvalidation-rmse:0.02276\n",
      "[2838]\tvalidation-rmse:0.02276\n",
      "[2839]\tvalidation-rmse:0.02276\n",
      "[2840]\tvalidation-rmse:0.02276\n",
      "[2841]\tvalidation-rmse:0.02276\n",
      "[2842]\tvalidation-rmse:0.02276\n",
      "[2843]\tvalidation-rmse:0.02276\n",
      "[2844]\tvalidation-rmse:0.02276\n",
      "[2845]\tvalidation-rmse:0.02276\n",
      "[2846]\tvalidation-rmse:0.02276\n",
      "[2847]\tvalidation-rmse:0.02276\n",
      "[2848]\tvalidation-rmse:0.02276\n",
      "[2849]\tvalidation-rmse:0.02276\n",
      "[2850]\tvalidation-rmse:0.02276\n",
      "[2851]\tvalidation-rmse:0.02276\n",
      "[2852]\tvalidation-rmse:0.02277\n",
      "[2853]\tvalidation-rmse:0.02277\n",
      "[2854]\tvalidation-rmse:0.02277\n",
      "[2855]\tvalidation-rmse:0.02277\n",
      "[2856]\tvalidation-rmse:0.02277\n",
      "[2857]\tvalidation-rmse:0.02277\n",
      "[2858]\tvalidation-rmse:0.02277\n",
      "[2859]\tvalidation-rmse:0.02277\n",
      "[2860]\tvalidation-rmse:0.02277\n",
      "[2861]\tvalidation-rmse:0.02277\n",
      "[2862]\tvalidation-rmse:0.02277\n",
      "[2863]\tvalidation-rmse:0.02277\n",
      "[2864]\tvalidation-rmse:0.02277\n",
      "[2865]\tvalidation-rmse:0.02277\n",
      "[2866]\tvalidation-rmse:0.02277\n",
      "[2867]\tvalidation-rmse:0.02277\n",
      "[2868]\tvalidation-rmse:0.02277\n",
      "[2869]\tvalidation-rmse:0.02277\n",
      "[2870]\tvalidation-rmse:0.02277\n",
      "[2871]\tvalidation-rmse:0.02277\n",
      "[2872]\tvalidation-rmse:0.02277\n",
      "[2873]\tvalidation-rmse:0.02277\n",
      "[2874]\tvalidation-rmse:0.02277\n",
      "[2875]\tvalidation-rmse:0.02277\n",
      "[2876]\tvalidation-rmse:0.02277\n",
      "[2877]\tvalidation-rmse:0.02277\n",
      "[2878]\tvalidation-rmse:0.02277\n",
      "[2879]\tvalidation-rmse:0.02277\n",
      "[2880]\tvalidation-rmse:0.02277\n",
      "[2881]\tvalidation-rmse:0.02277\n",
      "[2882]\tvalidation-rmse:0.02277\n",
      "[2883]\tvalidation-rmse:0.02277\n",
      "[2884]\tvalidation-rmse:0.02277\n",
      "[2885]\tvalidation-rmse:0.02277\n",
      "[2886]\tvalidation-rmse:0.02277\n",
      "[2887]\tvalidation-rmse:0.02277\n",
      "[2888]\tvalidation-rmse:0.02277\n",
      "[2889]\tvalidation-rmse:0.02277\n",
      "[2890]\tvalidation-rmse:0.02277\n",
      "[2891]\tvalidation-rmse:0.02277\n",
      "[2892]\tvalidation-rmse:0.02277\n",
      "[2893]\tvalidation-rmse:0.02277\n",
      "[2894]\tvalidation-rmse:0.02277\n",
      "[2895]\tvalidation-rmse:0.02277\n",
      "[2896]\tvalidation-rmse:0.02277\n",
      "[2897]\tvalidation-rmse:0.02277\n",
      "[2898]\tvalidation-rmse:0.02277\n",
      "[2899]\tvalidation-rmse:0.02277\n",
      "[2900]\tvalidation-rmse:0.02277\n",
      "[2901]\tvalidation-rmse:0.02277\n",
      "[2902]\tvalidation-rmse:0.02277\n",
      "[2903]\tvalidation-rmse:0.02277\n",
      "[2904]\tvalidation-rmse:0.02277\n",
      "[2905]\tvalidation-rmse:0.02277\n",
      "[2906]\tvalidation-rmse:0.02277\n",
      "[2907]\tvalidation-rmse:0.02277\n",
      "[2908]\tvalidation-rmse:0.02276\n",
      "[2909]\tvalidation-rmse:0.02276\n",
      "[2910]\tvalidation-rmse:0.02276\n",
      "[2911]\tvalidation-rmse:0.02276\n",
      "[2912]\tvalidation-rmse:0.02276\n",
      "[2913]\tvalidation-rmse:0.02276\n",
      "[2914]\tvalidation-rmse:0.02276\n",
      "[2915]\tvalidation-rmse:0.02276\n",
      "[2916]\tvalidation-rmse:0.02276\n",
      "[2917]\tvalidation-rmse:0.02276\n",
      "[2918]\tvalidation-rmse:0.02276\n",
      "[2919]\tvalidation-rmse:0.02276\n",
      "[2920]\tvalidation-rmse:0.02276\n",
      "[2921]\tvalidation-rmse:0.02276\n",
      "[2922]\tvalidation-rmse:0.02277\n",
      "[2923]\tvalidation-rmse:0.02277\n",
      "[2924]\tvalidation-rmse:0.02277\n",
      "[2925]\tvalidation-rmse:0.02277\n",
      "[2926]\tvalidation-rmse:0.02277\n",
      "[2927]\tvalidation-rmse:0.02277\n",
      "[2928]\tvalidation-rmse:0.02276\n",
      "[2929]\tvalidation-rmse:0.02276\n",
      "[2930]\tvalidation-rmse:0.02277\n",
      "[2931]\tvalidation-rmse:0.02277\n",
      "[2932]\tvalidation-rmse:0.02277\n",
      "[2933]\tvalidation-rmse:0.02277\n",
      "[2934]\tvalidation-rmse:0.02277\n",
      "[2935]\tvalidation-rmse:0.02277\n",
      "[2936]\tvalidation-rmse:0.02277\n",
      "[2937]\tvalidation-rmse:0.02277\n",
      "[2938]\tvalidation-rmse:0.02276\n",
      "[2939]\tvalidation-rmse:0.02276\n",
      "[2940]\tvalidation-rmse:0.02276\n",
      "[2941]\tvalidation-rmse:0.02276\n",
      "[2942]\tvalidation-rmse:0.02276\n",
      "[2943]\tvalidation-rmse:0.02276\n",
      "[2944]\tvalidation-rmse:0.02276\n",
      "[2945]\tvalidation-rmse:0.02276\n",
      "[2946]\tvalidation-rmse:0.02276\n",
      "[2947]\tvalidation-rmse:0.02276\n",
      "[2948]\tvalidation-rmse:0.02276\n",
      "[2949]\tvalidation-rmse:0.02276\n",
      "Model for Y stopped at best iteration: 2849\n",
      "\n",
      "--- Training Z-coordinate model ---\n",
      "[0]\tvalidation-rmse:0.51432\n",
      "[1]\tvalidation-rmse:0.50933\n",
      "[2]\tvalidation-rmse:0.50433\n",
      "[3]\tvalidation-rmse:0.49938\n",
      "[4]\tvalidation-rmse:0.49451\n",
      "[5]\tvalidation-rmse:0.48969\n",
      "[6]\tvalidation-rmse:0.48493\n",
      "[7]\tvalidation-rmse:0.48022\n",
      "[8]\tvalidation-rmse:0.47688\n",
      "[9]\tvalidation-rmse:0.47310\n",
      "[10]\tvalidation-rmse:0.46849\n",
      "[11]\tvalidation-rmse:0.46398\n",
      "[12]\tvalidation-rmse:0.45943\n",
      "[13]\tvalidation-rmse:0.45492\n",
      "[14]\tvalidation-rmse:0.45050\n",
      "[15]\tvalidation-rmse:0.44625\n",
      "[16]\tvalidation-rmse:0.44189\n",
      "[17]\tvalidation-rmse:0.43813\n",
      "[18]\tvalidation-rmse:0.43388\n",
      "[19]\tvalidation-rmse:0.42964\n",
      "[20]\tvalidation-rmse:0.42620\n",
      "[21]\tvalidation-rmse:0.42243\n",
      "[22]\tvalidation-rmse:0.41882\n",
      "[23]\tvalidation-rmse:0.41484\n",
      "[24]\tvalidation-rmse:0.41093\n",
      "[25]\tvalidation-rmse:0.40694\n",
      "[26]\tvalidation-rmse:0.40299\n",
      "[27]\tvalidation-rmse:0.39962\n",
      "[28]\tvalidation-rmse:0.39589\n",
      "[29]\tvalidation-rmse:0.39206\n",
      "[30]\tvalidation-rmse:0.38838\n",
      "[31]\tvalidation-rmse:0.38469\n",
      "[32]\tvalidation-rmse:0.38109\n",
      "[33]\tvalidation-rmse:0.37742\n",
      "[34]\tvalidation-rmse:0.37376\n",
      "[35]\tvalidation-rmse:0.37027\n",
      "[36]\tvalidation-rmse:0.36693\n",
      "[37]\tvalidation-rmse:0.36411\n",
      "[38]\tvalidation-rmse:0.36059\n",
      "[39]\tvalidation-rmse:0.35715\n",
      "[40]\tvalidation-rmse:0.35368\n",
      "[41]\tvalidation-rmse:0.35027\n",
      "[42]\tvalidation-rmse:0.34700\n",
      "[43]\tvalidation-rmse:0.34368\n",
      "[44]\tvalidation-rmse:0.34037\n",
      "[45]\tvalidation-rmse:0.33718\n",
      "[46]\tvalidation-rmse:0.33400\n",
      "[47]\tvalidation-rmse:0.33074\n",
      "[48]\tvalidation-rmse:0.32767\n",
      "[49]\tvalidation-rmse:0.32450\n",
      "[50]\tvalidation-rmse:0.32136\n",
      "[51]\tvalidation-rmse:0.31825\n",
      "[52]\tvalidation-rmse:0.31519\n",
      "[53]\tvalidation-rmse:0.31213\n",
      "[54]\tvalidation-rmse:0.30912\n",
      "[55]\tvalidation-rmse:0.30624\n",
      "[56]\tvalidation-rmse:0.30330\n",
      "[57]\tvalidation-rmse:0.30039\n",
      "[58]\tvalidation-rmse:0.29750\n",
      "[59]\tvalidation-rmse:0.29478\n",
      "[60]\tvalidation-rmse:0.29203\n",
      "[61]\tvalidation-rmse:0.28922\n",
      "[62]\tvalidation-rmse:0.28648\n",
      "[63]\tvalidation-rmse:0.28390\n",
      "[64]\tvalidation-rmse:0.28144\n",
      "[65]\tvalidation-rmse:0.27874\n",
      "[66]\tvalidation-rmse:0.27678\n",
      "[67]\tvalidation-rmse:0.27413\n",
      "[68]\tvalidation-rmse:0.27152\n",
      "[69]\tvalidation-rmse:0.26892\n",
      "[70]\tvalidation-rmse:0.26644\n",
      "[71]\tvalidation-rmse:0.26397\n",
      "[72]\tvalidation-rmse:0.26144\n",
      "[73]\tvalidation-rmse:0.25913\n",
      "[74]\tvalidation-rmse:0.25674\n",
      "[75]\tvalidation-rmse:0.25438\n",
      "[76]\tvalidation-rmse:0.25212\n",
      "[77]\tvalidation-rmse:0.24968\n",
      "[78]\tvalidation-rmse:0.24731\n",
      "[79]\tvalidation-rmse:0.24494\n",
      "[80]\tvalidation-rmse:0.24265\n",
      "[81]\tvalidation-rmse:0.24034\n",
      "[82]\tvalidation-rmse:0.23804\n",
      "[83]\tvalidation-rmse:0.23578\n",
      "[84]\tvalidation-rmse:0.23354\n",
      "[85]\tvalidation-rmse:0.23134\n",
      "[86]\tvalidation-rmse:0.22915\n",
      "[87]\tvalidation-rmse:0.22699\n",
      "[88]\tvalidation-rmse:0.22490\n",
      "[89]\tvalidation-rmse:0.22274\n",
      "[90]\tvalidation-rmse:0.22063\n",
      "[91]\tvalidation-rmse:0.21868\n",
      "[92]\tvalidation-rmse:0.21695\n",
      "[93]\tvalidation-rmse:0.21489\n",
      "[94]\tvalidation-rmse:0.21285\n",
      "[95]\tvalidation-rmse:0.21083\n",
      "[96]\tvalidation-rmse:0.20887\n",
      "[97]\tvalidation-rmse:0.20690\n",
      "[98]\tvalidation-rmse:0.20567\n",
      "[99]\tvalidation-rmse:0.20373\n",
      "[100]\tvalidation-rmse:0.20184\n",
      "[101]\tvalidation-rmse:0.20011\n",
      "[102]\tvalidation-rmse:0.19822\n",
      "[103]\tvalidation-rmse:0.19640\n",
      "[104]\tvalidation-rmse:0.19473\n",
      "[105]\tvalidation-rmse:0.19295\n",
      "[106]\tvalidation-rmse:0.19115\n",
      "[107]\tvalidation-rmse:0.18936\n",
      "[108]\tvalidation-rmse:0.18757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/callback.py:386: UserWarning: [16:32:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109]\tvalidation-rmse:0.18586\n",
      "[110]\tvalidation-rmse:0.18433\n",
      "[111]\tvalidation-rmse:0.18257\n",
      "[112]\tvalidation-rmse:0.18095\n",
      "[113]\tvalidation-rmse:0.17925\n",
      "[114]\tvalidation-rmse:0.17780\n",
      "[115]\tvalidation-rmse:0.17627\n",
      "[116]\tvalidation-rmse:0.17482\n",
      "[117]\tvalidation-rmse:0.17327\n",
      "[118]\tvalidation-rmse:0.17164\n",
      "[119]\tvalidation-rmse:0.17009\n",
      "[120]\tvalidation-rmse:0.16852\n",
      "[121]\tvalidation-rmse:0.16692\n",
      "[122]\tvalidation-rmse:0.16535\n",
      "[123]\tvalidation-rmse:0.16387\n",
      "[124]\tvalidation-rmse:0.16234\n",
      "[125]\tvalidation-rmse:0.16082\n",
      "[126]\tvalidation-rmse:0.15932\n",
      "[127]\tvalidation-rmse:0.15785\n",
      "[128]\tvalidation-rmse:0.15645\n",
      "[129]\tvalidation-rmse:0.15498\n",
      "[130]\tvalidation-rmse:0.15355\n",
      "[131]\tvalidation-rmse:0.15212\n",
      "[132]\tvalidation-rmse:0.15074\n",
      "[133]\tvalidation-rmse:0.14933\n",
      "[134]\tvalidation-rmse:0.14794\n",
      "[135]\tvalidation-rmse:0.14660\n",
      "[136]\tvalidation-rmse:0.14524\n",
      "[137]\tvalidation-rmse:0.14401\n",
      "[138]\tvalidation-rmse:0.14277\n",
      "[139]\tvalidation-rmse:0.14154\n",
      "[140]\tvalidation-rmse:0.14027\n",
      "[141]\tvalidation-rmse:0.13899\n",
      "[142]\tvalidation-rmse:0.13773\n",
      "[143]\tvalidation-rmse:0.13645\n",
      "[144]\tvalidation-rmse:0.13520\n",
      "[145]\tvalidation-rmse:0.13396\n",
      "[146]\tvalidation-rmse:0.13272\n",
      "[147]\tvalidation-rmse:0.13163\n",
      "[148]\tvalidation-rmse:0.13043\n",
      "[149]\tvalidation-rmse:0.12922\n",
      "[150]\tvalidation-rmse:0.12803\n",
      "[151]\tvalidation-rmse:0.12698\n",
      "[152]\tvalidation-rmse:0.12584\n",
      "[153]\tvalidation-rmse:0.12469\n",
      "[154]\tvalidation-rmse:0.12355\n",
      "[155]\tvalidation-rmse:0.12242\n",
      "[156]\tvalidation-rmse:0.12129\n",
      "[157]\tvalidation-rmse:0.12018\n",
      "[158]\tvalidation-rmse:0.11909\n",
      "[159]\tvalidation-rmse:0.11799\n",
      "[160]\tvalidation-rmse:0.11693\n",
      "[161]\tvalidation-rmse:0.11598\n",
      "[162]\tvalidation-rmse:0.11493\n",
      "[163]\tvalidation-rmse:0.11388\n",
      "[164]\tvalidation-rmse:0.11287\n",
      "[165]\tvalidation-rmse:0.11187\n",
      "[166]\tvalidation-rmse:0.11091\n",
      "[167]\tvalidation-rmse:0.10998\n",
      "[168]\tvalidation-rmse:0.10899\n",
      "[169]\tvalidation-rmse:0.10798\n",
      "[170]\tvalidation-rmse:0.10717\n",
      "[171]\tvalidation-rmse:0.10619\n",
      "[172]\tvalidation-rmse:0.10527\n",
      "[173]\tvalidation-rmse:0.10432\n",
      "[174]\tvalidation-rmse:0.10344\n",
      "[175]\tvalidation-rmse:0.10252\n",
      "[176]\tvalidation-rmse:0.10162\n",
      "[177]\tvalidation-rmse:0.10071\n",
      "[178]\tvalidation-rmse:0.09982\n",
      "[179]\tvalidation-rmse:0.09894\n",
      "[180]\tvalidation-rmse:0.09806\n",
      "[181]\tvalidation-rmse:0.09722\n",
      "[182]\tvalidation-rmse:0.09651\n",
      "[183]\tvalidation-rmse:0.09582\n",
      "[184]\tvalidation-rmse:0.09509\n",
      "[185]\tvalidation-rmse:0.09426\n",
      "[186]\tvalidation-rmse:0.09343\n",
      "[187]\tvalidation-rmse:0.09269\n",
      "[188]\tvalidation-rmse:0.09188\n",
      "[189]\tvalidation-rmse:0.09107\n",
      "[190]\tvalidation-rmse:0.09027\n",
      "[191]\tvalidation-rmse:0.08949\n",
      "[192]\tvalidation-rmse:0.08869\n",
      "[193]\tvalidation-rmse:0.08792\n",
      "[194]\tvalidation-rmse:0.08715\n",
      "[195]\tvalidation-rmse:0.08642\n",
      "[196]\tvalidation-rmse:0.08567\n",
      "[197]\tvalidation-rmse:0.08492\n",
      "[198]\tvalidation-rmse:0.08418\n",
      "[199]\tvalidation-rmse:0.08357\n",
      "[200]\tvalidation-rmse:0.08296\n",
      "[201]\tvalidation-rmse:0.08224\n",
      "[202]\tvalidation-rmse:0.08156\n",
      "[203]\tvalidation-rmse:0.08086\n",
      "[204]\tvalidation-rmse:0.08018\n",
      "[205]\tvalidation-rmse:0.07950\n",
      "[206]\tvalidation-rmse:0.07882\n",
      "[207]\tvalidation-rmse:0.07814\n",
      "[208]\tvalidation-rmse:0.07757\n",
      "[209]\tvalidation-rmse:0.07694\n",
      "[210]\tvalidation-rmse:0.07631\n",
      "[211]\tvalidation-rmse:0.07564\n",
      "[212]\tvalidation-rmse:0.07507\n",
      "[213]\tvalidation-rmse:0.07443\n",
      "[214]\tvalidation-rmse:0.07378\n",
      "[215]\tvalidation-rmse:0.07315\n",
      "[216]\tvalidation-rmse:0.07253\n",
      "[217]\tvalidation-rmse:0.07193\n",
      "[218]\tvalidation-rmse:0.07131\n",
      "[219]\tvalidation-rmse:0.07078\n",
      "[220]\tvalidation-rmse:0.07018\n",
      "[221]\tvalidation-rmse:0.06959\n",
      "[222]\tvalidation-rmse:0.06904\n",
      "[223]\tvalidation-rmse:0.06847\n",
      "[224]\tvalidation-rmse:0.06791\n",
      "[225]\tvalidation-rmse:0.06737\n",
      "[226]\tvalidation-rmse:0.06698\n",
      "[227]\tvalidation-rmse:0.06642\n",
      "[228]\tvalidation-rmse:0.06587\n",
      "[229]\tvalidation-rmse:0.06545\n",
      "[230]\tvalidation-rmse:0.06492\n",
      "[231]\tvalidation-rmse:0.06441\n",
      "[232]\tvalidation-rmse:0.06388\n",
      "[233]\tvalidation-rmse:0.06335\n",
      "[234]\tvalidation-rmse:0.06282\n",
      "[235]\tvalidation-rmse:0.06230\n",
      "[236]\tvalidation-rmse:0.06179\n",
      "[237]\tvalidation-rmse:0.06129\n",
      "[238]\tvalidation-rmse:0.06079\n",
      "[239]\tvalidation-rmse:0.06035\n",
      "[240]\tvalidation-rmse:0.05986\n",
      "[241]\tvalidation-rmse:0.05937\n",
      "[242]\tvalidation-rmse:0.05891\n",
      "[243]\tvalidation-rmse:0.05844\n",
      "[244]\tvalidation-rmse:0.05796\n",
      "[245]\tvalidation-rmse:0.05749\n",
      "[246]\tvalidation-rmse:0.05710\n",
      "[247]\tvalidation-rmse:0.05671\n",
      "[248]\tvalidation-rmse:0.05628\n",
      "[249]\tvalidation-rmse:0.05591\n",
      "[250]\tvalidation-rmse:0.05547\n",
      "[251]\tvalidation-rmse:0.05503\n",
      "[252]\tvalidation-rmse:0.05460\n",
      "[253]\tvalidation-rmse:0.05417\n",
      "[254]\tvalidation-rmse:0.05374\n",
      "[255]\tvalidation-rmse:0.05331\n",
      "[256]\tvalidation-rmse:0.05294\n",
      "[257]\tvalidation-rmse:0.05254\n",
      "[258]\tvalidation-rmse:0.05213\n",
      "[259]\tvalidation-rmse:0.05172\n",
      "[260]\tvalidation-rmse:0.05133\n",
      "[261]\tvalidation-rmse:0.05099\n",
      "[262]\tvalidation-rmse:0.05067\n",
      "[263]\tvalidation-rmse:0.05027\n",
      "[264]\tvalidation-rmse:0.04992\n",
      "[265]\tvalidation-rmse:0.04954\n",
      "[266]\tvalidation-rmse:0.04916\n",
      "[267]\tvalidation-rmse:0.04880\n",
      "[268]\tvalidation-rmse:0.04842\n",
      "[269]\tvalidation-rmse:0.04808\n",
      "[270]\tvalidation-rmse:0.04772\n",
      "[271]\tvalidation-rmse:0.04737\n",
      "[272]\tvalidation-rmse:0.04701\n",
      "[273]\tvalidation-rmse:0.04668\n",
      "[274]\tvalidation-rmse:0.04633\n",
      "[275]\tvalidation-rmse:0.04599\n",
      "[276]\tvalidation-rmse:0.04564\n",
      "[277]\tvalidation-rmse:0.04534\n",
      "[278]\tvalidation-rmse:0.04508\n",
      "[279]\tvalidation-rmse:0.04477\n",
      "[280]\tvalidation-rmse:0.04445\n",
      "[281]\tvalidation-rmse:0.04412\n",
      "[282]\tvalidation-rmse:0.04380\n",
      "[283]\tvalidation-rmse:0.04349\n",
      "[284]\tvalidation-rmse:0.04325\n",
      "[285]\tvalidation-rmse:0.04296\n",
      "[286]\tvalidation-rmse:0.04266\n",
      "[287]\tvalidation-rmse:0.04242\n",
      "[288]\tvalidation-rmse:0.04212\n",
      "[289]\tvalidation-rmse:0.04182\n",
      "[290]\tvalidation-rmse:0.04152\n",
      "[291]\tvalidation-rmse:0.04125\n",
      "[292]\tvalidation-rmse:0.04095\n",
      "[293]\tvalidation-rmse:0.04067\n",
      "[294]\tvalidation-rmse:0.04038\n",
      "[295]\tvalidation-rmse:0.04010\n",
      "[296]\tvalidation-rmse:0.03982\n",
      "[297]\tvalidation-rmse:0.03955\n",
      "[298]\tvalidation-rmse:0.03930\n",
      "[299]\tvalidation-rmse:0.03906\n",
      "[300]\tvalidation-rmse:0.03885\n",
      "[301]\tvalidation-rmse:0.03859\n",
      "[302]\tvalidation-rmse:0.03833\n",
      "[303]\tvalidation-rmse:0.03807\n",
      "[304]\tvalidation-rmse:0.03784\n",
      "[305]\tvalidation-rmse:0.03759\n",
      "[306]\tvalidation-rmse:0.03735\n",
      "[307]\tvalidation-rmse:0.03710\n",
      "[308]\tvalidation-rmse:0.03685\n",
      "[309]\tvalidation-rmse:0.03663\n",
      "[310]\tvalidation-rmse:0.03639\n",
      "[311]\tvalidation-rmse:0.03615\n",
      "[312]\tvalidation-rmse:0.03591\n",
      "[313]\tvalidation-rmse:0.03569\n",
      "[314]\tvalidation-rmse:0.03548\n",
      "[315]\tvalidation-rmse:0.03527\n",
      "[316]\tvalidation-rmse:0.03504\n",
      "[317]\tvalidation-rmse:0.03486\n",
      "[318]\tvalidation-rmse:0.03464\n",
      "[319]\tvalidation-rmse:0.03442\n",
      "[320]\tvalidation-rmse:0.03422\n",
      "[321]\tvalidation-rmse:0.03401\n",
      "[322]\tvalidation-rmse:0.03380\n",
      "[323]\tvalidation-rmse:0.03362\n",
      "[324]\tvalidation-rmse:0.03343\n",
      "[325]\tvalidation-rmse:0.03323\n",
      "[326]\tvalidation-rmse:0.03302\n",
      "[327]\tvalidation-rmse:0.03282\n",
      "[328]\tvalidation-rmse:0.03262\n",
      "[329]\tvalidation-rmse:0.03243\n",
      "[330]\tvalidation-rmse:0.03228\n",
      "[331]\tvalidation-rmse:0.03209\n",
      "[332]\tvalidation-rmse:0.03191\n",
      "[333]\tvalidation-rmse:0.03173\n",
      "[334]\tvalidation-rmse:0.03157\n",
      "[335]\tvalidation-rmse:0.03141\n",
      "[336]\tvalidation-rmse:0.03123\n",
      "[337]\tvalidation-rmse:0.03109\n",
      "[338]\tvalidation-rmse:0.03093\n",
      "[339]\tvalidation-rmse:0.03074\n",
      "[340]\tvalidation-rmse:0.03058\n",
      "[341]\tvalidation-rmse:0.03042\n",
      "[342]\tvalidation-rmse:0.03025\n",
      "[343]\tvalidation-rmse:0.03009\n",
      "[344]\tvalidation-rmse:0.02994\n",
      "[345]\tvalidation-rmse:0.02978\n",
      "[346]\tvalidation-rmse:0.02963\n",
      "[347]\tvalidation-rmse:0.02951\n",
      "[348]\tvalidation-rmse:0.02934\n",
      "[349]\tvalidation-rmse:0.02919\n",
      "[350]\tvalidation-rmse:0.02903\n",
      "[351]\tvalidation-rmse:0.02888\n",
      "[352]\tvalidation-rmse:0.02873\n",
      "[353]\tvalidation-rmse:0.02859\n",
      "[354]\tvalidation-rmse:0.02845\n",
      "[355]\tvalidation-rmse:0.02832\n",
      "[356]\tvalidation-rmse:0.02819\n",
      "[357]\tvalidation-rmse:0.02805\n",
      "[358]\tvalidation-rmse:0.02794\n",
      "[359]\tvalidation-rmse:0.02783\n",
      "[360]\tvalidation-rmse:0.02769\n",
      "[361]\tvalidation-rmse:0.02757\n",
      "[362]\tvalidation-rmse:0.02743\n",
      "[363]\tvalidation-rmse:0.02729\n",
      "[364]\tvalidation-rmse:0.02716\n",
      "[365]\tvalidation-rmse:0.02704\n",
      "[366]\tvalidation-rmse:0.02692\n",
      "[367]\tvalidation-rmse:0.02684\n",
      "[368]\tvalidation-rmse:0.02671\n",
      "[369]\tvalidation-rmse:0.02660\n",
      "[370]\tvalidation-rmse:0.02647\n",
      "[371]\tvalidation-rmse:0.02634\n",
      "[372]\tvalidation-rmse:0.02623\n",
      "[373]\tvalidation-rmse:0.02612\n",
      "[374]\tvalidation-rmse:0.02601\n",
      "[375]\tvalidation-rmse:0.02591\n",
      "[376]\tvalidation-rmse:0.02580\n",
      "[377]\tvalidation-rmse:0.02571\n",
      "[378]\tvalidation-rmse:0.02560\n",
      "[379]\tvalidation-rmse:0.02549\n",
      "[380]\tvalidation-rmse:0.02538\n",
      "[381]\tvalidation-rmse:0.02528\n",
      "[382]\tvalidation-rmse:0.02518\n",
      "[383]\tvalidation-rmse:0.02507\n",
      "[384]\tvalidation-rmse:0.02499\n",
      "[385]\tvalidation-rmse:0.02489\n",
      "[386]\tvalidation-rmse:0.02479\n",
      "[387]\tvalidation-rmse:0.02469\n",
      "[388]\tvalidation-rmse:0.02459\n",
      "[389]\tvalidation-rmse:0.02449\n",
      "[390]\tvalidation-rmse:0.02441\n",
      "[391]\tvalidation-rmse:0.02432\n",
      "[392]\tvalidation-rmse:0.02424\n",
      "[393]\tvalidation-rmse:0.02415\n",
      "[394]\tvalidation-rmse:0.02409\n",
      "[395]\tvalidation-rmse:0.02399\n",
      "[396]\tvalidation-rmse:0.02391\n",
      "[397]\tvalidation-rmse:0.02383\n",
      "[398]\tvalidation-rmse:0.02374\n",
      "[399]\tvalidation-rmse:0.02365\n",
      "[400]\tvalidation-rmse:0.02358\n",
      "[401]\tvalidation-rmse:0.02350\n",
      "[402]\tvalidation-rmse:0.02342\n",
      "[403]\tvalidation-rmse:0.02334\n",
      "[404]\tvalidation-rmse:0.02327\n",
      "[405]\tvalidation-rmse:0.02319\n",
      "[406]\tvalidation-rmse:0.02311\n",
      "[407]\tvalidation-rmse:0.02303\n",
      "[408]\tvalidation-rmse:0.02296\n",
      "[409]\tvalidation-rmse:0.02289\n",
      "[410]\tvalidation-rmse:0.02281\n",
      "[411]\tvalidation-rmse:0.02274\n",
      "[412]\tvalidation-rmse:0.02267\n",
      "[413]\tvalidation-rmse:0.02260\n",
      "[414]\tvalidation-rmse:0.02254\n",
      "[415]\tvalidation-rmse:0.02247\n",
      "[416]\tvalidation-rmse:0.02241\n",
      "[417]\tvalidation-rmse:0.02235\n",
      "[418]\tvalidation-rmse:0.02228\n",
      "[419]\tvalidation-rmse:0.02222\n",
      "[420]\tvalidation-rmse:0.02215\n",
      "[421]\tvalidation-rmse:0.02209\n",
      "[422]\tvalidation-rmse:0.02203\n",
      "[423]\tvalidation-rmse:0.02197\n",
      "[424]\tvalidation-rmse:0.02190\n",
      "[425]\tvalidation-rmse:0.02184\n",
      "[426]\tvalidation-rmse:0.02177\n",
      "[427]\tvalidation-rmse:0.02171\n",
      "[428]\tvalidation-rmse:0.02165\n",
      "[429]\tvalidation-rmse:0.02159\n",
      "[430]\tvalidation-rmse:0.02153\n",
      "[431]\tvalidation-rmse:0.02147\n",
      "[432]\tvalidation-rmse:0.02142\n",
      "[433]\tvalidation-rmse:0.02137\n",
      "[434]\tvalidation-rmse:0.02131\n",
      "[435]\tvalidation-rmse:0.02126\n",
      "[436]\tvalidation-rmse:0.02120\n",
      "[437]\tvalidation-rmse:0.02115\n",
      "[438]\tvalidation-rmse:0.02111\n",
      "[439]\tvalidation-rmse:0.02105\n",
      "[440]\tvalidation-rmse:0.02101\n",
      "[441]\tvalidation-rmse:0.02096\n",
      "[442]\tvalidation-rmse:0.02091\n",
      "[443]\tvalidation-rmse:0.02087\n",
      "[444]\tvalidation-rmse:0.02082\n",
      "[445]\tvalidation-rmse:0.02076\n",
      "[446]\tvalidation-rmse:0.02072\n",
      "[447]\tvalidation-rmse:0.02068\n",
      "[448]\tvalidation-rmse:0.02063\n",
      "[449]\tvalidation-rmse:0.02058\n",
      "[450]\tvalidation-rmse:0.02055\n",
      "[451]\tvalidation-rmse:0.02051\n",
      "[452]\tvalidation-rmse:0.02047\n",
      "[453]\tvalidation-rmse:0.02043\n",
      "[454]\tvalidation-rmse:0.02039\n",
      "[455]\tvalidation-rmse:0.02035\n",
      "[456]\tvalidation-rmse:0.02032\n",
      "[457]\tvalidation-rmse:0.02028\n",
      "[458]\tvalidation-rmse:0.02023\n",
      "[459]\tvalidation-rmse:0.02019\n",
      "[460]\tvalidation-rmse:0.02015\n",
      "[461]\tvalidation-rmse:0.02011\n",
      "[462]\tvalidation-rmse:0.02008\n",
      "[463]\tvalidation-rmse:0.02004\n",
      "[464]\tvalidation-rmse:0.02000\n",
      "[465]\tvalidation-rmse:0.01996\n",
      "[466]\tvalidation-rmse:0.01993\n",
      "[467]\tvalidation-rmse:0.01989\n",
      "[468]\tvalidation-rmse:0.01985\n",
      "[469]\tvalidation-rmse:0.01981\n",
      "[470]\tvalidation-rmse:0.01978\n",
      "[471]\tvalidation-rmse:0.01974\n",
      "[472]\tvalidation-rmse:0.01971\n",
      "[473]\tvalidation-rmse:0.01967\n",
      "[474]\tvalidation-rmse:0.01964\n",
      "[475]\tvalidation-rmse:0.01960\n",
      "[476]\tvalidation-rmse:0.01957\n",
      "[477]\tvalidation-rmse:0.01954\n",
      "[478]\tvalidation-rmse:0.01951\n",
      "[479]\tvalidation-rmse:0.01947\n",
      "[480]\tvalidation-rmse:0.01944\n",
      "[481]\tvalidation-rmse:0.01942\n",
      "[482]\tvalidation-rmse:0.01939\n",
      "[483]\tvalidation-rmse:0.01936\n",
      "[484]\tvalidation-rmse:0.01932\n",
      "[485]\tvalidation-rmse:0.01929\n",
      "[486]\tvalidation-rmse:0.01927\n",
      "[487]\tvalidation-rmse:0.01923\n",
      "[488]\tvalidation-rmse:0.01921\n",
      "[489]\tvalidation-rmse:0.01918\n",
      "[490]\tvalidation-rmse:0.01915\n",
      "[491]\tvalidation-rmse:0.01914\n",
      "[492]\tvalidation-rmse:0.01911\n",
      "[493]\tvalidation-rmse:0.01909\n",
      "[494]\tvalidation-rmse:0.01906\n",
      "[495]\tvalidation-rmse:0.01903\n",
      "[496]\tvalidation-rmse:0.01902\n",
      "[497]\tvalidation-rmse:0.01899\n",
      "[498]\tvalidation-rmse:0.01896\n",
      "[499]\tvalidation-rmse:0.01894\n",
      "[500]\tvalidation-rmse:0.01892\n",
      "[501]\tvalidation-rmse:0.01889\n",
      "[502]\tvalidation-rmse:0.01886\n",
      "[503]\tvalidation-rmse:0.01884\n",
      "[504]\tvalidation-rmse:0.01882\n",
      "[505]\tvalidation-rmse:0.01880\n",
      "[506]\tvalidation-rmse:0.01877\n",
      "[507]\tvalidation-rmse:0.01876\n",
      "[508]\tvalidation-rmse:0.01874\n",
      "[509]\tvalidation-rmse:0.01871\n",
      "[510]\tvalidation-rmse:0.01870\n",
      "[511]\tvalidation-rmse:0.01867\n",
      "[512]\tvalidation-rmse:0.01865\n",
      "[513]\tvalidation-rmse:0.01863\n",
      "[514]\tvalidation-rmse:0.01861\n",
      "[515]\tvalidation-rmse:0.01860\n",
      "[516]\tvalidation-rmse:0.01858\n",
      "[517]\tvalidation-rmse:0.01856\n",
      "[518]\tvalidation-rmse:0.01854\n",
      "[519]\tvalidation-rmse:0.01853\n",
      "[520]\tvalidation-rmse:0.01850\n",
      "[521]\tvalidation-rmse:0.01848\n",
      "[522]\tvalidation-rmse:0.01847\n",
      "[523]\tvalidation-rmse:0.01845\n",
      "[524]\tvalidation-rmse:0.01843\n",
      "[525]\tvalidation-rmse:0.01841\n",
      "[526]\tvalidation-rmse:0.01839\n",
      "[527]\tvalidation-rmse:0.01838\n",
      "[528]\tvalidation-rmse:0.01836\n",
      "[529]\tvalidation-rmse:0.01834\n",
      "[530]\tvalidation-rmse:0.01832\n",
      "[531]\tvalidation-rmse:0.01831\n",
      "[532]\tvalidation-rmse:0.01830\n",
      "[533]\tvalidation-rmse:0.01828\n",
      "[534]\tvalidation-rmse:0.01826\n",
      "[535]\tvalidation-rmse:0.01825\n",
      "[536]\tvalidation-rmse:0.01823\n",
      "[537]\tvalidation-rmse:0.01822\n",
      "[538]\tvalidation-rmse:0.01820\n",
      "[539]\tvalidation-rmse:0.01818\n",
      "[540]\tvalidation-rmse:0.01817\n",
      "[541]\tvalidation-rmse:0.01815\n",
      "[542]\tvalidation-rmse:0.01814\n",
      "[543]\tvalidation-rmse:0.01813\n",
      "[544]\tvalidation-rmse:0.01811\n",
      "[545]\tvalidation-rmse:0.01809\n",
      "[546]\tvalidation-rmse:0.01808\n",
      "[547]\tvalidation-rmse:0.01807\n",
      "[548]\tvalidation-rmse:0.01806\n",
      "[549]\tvalidation-rmse:0.01804\n",
      "[550]\tvalidation-rmse:0.01803\n",
      "[551]\tvalidation-rmse:0.01802\n",
      "[552]\tvalidation-rmse:0.01800\n",
      "[553]\tvalidation-rmse:0.01799\n",
      "[554]\tvalidation-rmse:0.01798\n",
      "[555]\tvalidation-rmse:0.01796\n",
      "[556]\tvalidation-rmse:0.01794\n",
      "[557]\tvalidation-rmse:0.01794\n",
      "[558]\tvalidation-rmse:0.01792\n",
      "[559]\tvalidation-rmse:0.01791\n",
      "[560]\tvalidation-rmse:0.01789\n",
      "[561]\tvalidation-rmse:0.01788\n",
      "[562]\tvalidation-rmse:0.01787\n",
      "[563]\tvalidation-rmse:0.01786\n",
      "[564]\tvalidation-rmse:0.01785\n",
      "[565]\tvalidation-rmse:0.01784\n",
      "[566]\tvalidation-rmse:0.01783\n",
      "[567]\tvalidation-rmse:0.01782\n",
      "[568]\tvalidation-rmse:0.01781\n",
      "[569]\tvalidation-rmse:0.01780\n",
      "[570]\tvalidation-rmse:0.01779\n",
      "[571]\tvalidation-rmse:0.01777\n",
      "[572]\tvalidation-rmse:0.01777\n",
      "[573]\tvalidation-rmse:0.01776\n",
      "[574]\tvalidation-rmse:0.01775\n",
      "[575]\tvalidation-rmse:0.01774\n",
      "[576]\tvalidation-rmse:0.01774\n",
      "[577]\tvalidation-rmse:0.01772\n",
      "[578]\tvalidation-rmse:0.01771\n",
      "[579]\tvalidation-rmse:0.01770\n",
      "[580]\tvalidation-rmse:0.01769\n",
      "[581]\tvalidation-rmse:0.01768\n",
      "[582]\tvalidation-rmse:0.01768\n",
      "[583]\tvalidation-rmse:0.01767\n",
      "[584]\tvalidation-rmse:0.01766\n",
      "[585]\tvalidation-rmse:0.01765\n",
      "[586]\tvalidation-rmse:0.01765\n",
      "[587]\tvalidation-rmse:0.01764\n",
      "[588]\tvalidation-rmse:0.01763\n",
      "[589]\tvalidation-rmse:0.01763\n",
      "[590]\tvalidation-rmse:0.01762\n",
      "[591]\tvalidation-rmse:0.01761\n",
      "[592]\tvalidation-rmse:0.01761\n",
      "[593]\tvalidation-rmse:0.01760\n",
      "[594]\tvalidation-rmse:0.01759\n",
      "[595]\tvalidation-rmse:0.01758\n",
      "[596]\tvalidation-rmse:0.01757\n",
      "[597]\tvalidation-rmse:0.01757\n",
      "[598]\tvalidation-rmse:0.01756\n",
      "[599]\tvalidation-rmse:0.01755\n",
      "[600]\tvalidation-rmse:0.01754\n",
      "[601]\tvalidation-rmse:0.01754\n",
      "[602]\tvalidation-rmse:0.01753\n",
      "[603]\tvalidation-rmse:0.01753\n",
      "[604]\tvalidation-rmse:0.01752\n",
      "[605]\tvalidation-rmse:0.01751\n",
      "[606]\tvalidation-rmse:0.01751\n",
      "[607]\tvalidation-rmse:0.01750\n",
      "[608]\tvalidation-rmse:0.01750\n",
      "[609]\tvalidation-rmse:0.01749\n",
      "[610]\tvalidation-rmse:0.01748\n",
      "[611]\tvalidation-rmse:0.01747\n",
      "[612]\tvalidation-rmse:0.01747\n",
      "[613]\tvalidation-rmse:0.01745\n",
      "[614]\tvalidation-rmse:0.01744\n",
      "[615]\tvalidation-rmse:0.01744\n",
      "[616]\tvalidation-rmse:0.01743\n",
      "[617]\tvalidation-rmse:0.01742\n",
      "[618]\tvalidation-rmse:0.01742\n",
      "[619]\tvalidation-rmse:0.01741\n",
      "[620]\tvalidation-rmse:0.01740\n",
      "[621]\tvalidation-rmse:0.01739\n",
      "[622]\tvalidation-rmse:0.01739\n",
      "[623]\tvalidation-rmse:0.01738\n",
      "[624]\tvalidation-rmse:0.01738\n",
      "[625]\tvalidation-rmse:0.01737\n",
      "[626]\tvalidation-rmse:0.01737\n",
      "[627]\tvalidation-rmse:0.01736\n",
      "[628]\tvalidation-rmse:0.01735\n",
      "[629]\tvalidation-rmse:0.01735\n",
      "[630]\tvalidation-rmse:0.01734\n",
      "[631]\tvalidation-rmse:0.01733\n",
      "[632]\tvalidation-rmse:0.01733\n",
      "[633]\tvalidation-rmse:0.01732\n",
      "[634]\tvalidation-rmse:0.01732\n",
      "[635]\tvalidation-rmse:0.01731\n",
      "[636]\tvalidation-rmse:0.01731\n",
      "[637]\tvalidation-rmse:0.01731\n",
      "[638]\tvalidation-rmse:0.01730\n",
      "[639]\tvalidation-rmse:0.01730\n",
      "[640]\tvalidation-rmse:0.01729\n",
      "[641]\tvalidation-rmse:0.01729\n",
      "[642]\tvalidation-rmse:0.01728\n",
      "[643]\tvalidation-rmse:0.01728\n",
      "[644]\tvalidation-rmse:0.01727\n",
      "[645]\tvalidation-rmse:0.01727\n",
      "[646]\tvalidation-rmse:0.01726\n",
      "[647]\tvalidation-rmse:0.01726\n",
      "[648]\tvalidation-rmse:0.01726\n",
      "[649]\tvalidation-rmse:0.01725\n",
      "[650]\tvalidation-rmse:0.01725\n",
      "[651]\tvalidation-rmse:0.01724\n",
      "[652]\tvalidation-rmse:0.01724\n",
      "[653]\tvalidation-rmse:0.01723\n",
      "[654]\tvalidation-rmse:0.01723\n",
      "[655]\tvalidation-rmse:0.01722\n",
      "[656]\tvalidation-rmse:0.01721\n",
      "[657]\tvalidation-rmse:0.01721\n",
      "[658]\tvalidation-rmse:0.01721\n",
      "[659]\tvalidation-rmse:0.01720\n",
      "[660]\tvalidation-rmse:0.01720\n",
      "[661]\tvalidation-rmse:0.01720\n",
      "[662]\tvalidation-rmse:0.01720\n",
      "[663]\tvalidation-rmse:0.01719\n",
      "[664]\tvalidation-rmse:0.01719\n",
      "[665]\tvalidation-rmse:0.01719\n",
      "[666]\tvalidation-rmse:0.01718\n",
      "[667]\tvalidation-rmse:0.01717\n",
      "[668]\tvalidation-rmse:0.01717\n",
      "[669]\tvalidation-rmse:0.01716\n",
      "[670]\tvalidation-rmse:0.01715\n",
      "[671]\tvalidation-rmse:0.01715\n",
      "[672]\tvalidation-rmse:0.01715\n",
      "[673]\tvalidation-rmse:0.01714\n",
      "[674]\tvalidation-rmse:0.01714\n",
      "[675]\tvalidation-rmse:0.01713\n",
      "[676]\tvalidation-rmse:0.01713\n",
      "[677]\tvalidation-rmse:0.01713\n",
      "[678]\tvalidation-rmse:0.01713\n",
      "[679]\tvalidation-rmse:0.01712\n",
      "[680]\tvalidation-rmse:0.01712\n",
      "[681]\tvalidation-rmse:0.01711\n",
      "[682]\tvalidation-rmse:0.01711\n",
      "[683]\tvalidation-rmse:0.01711\n",
      "[684]\tvalidation-rmse:0.01711\n",
      "[685]\tvalidation-rmse:0.01710\n",
      "[686]\tvalidation-rmse:0.01710\n",
      "[687]\tvalidation-rmse:0.01710\n",
      "[688]\tvalidation-rmse:0.01710\n",
      "[689]\tvalidation-rmse:0.01709\n",
      "[690]\tvalidation-rmse:0.01709\n",
      "[691]\tvalidation-rmse:0.01708\n",
      "[692]\tvalidation-rmse:0.01708\n",
      "[693]\tvalidation-rmse:0.01708\n",
      "[694]\tvalidation-rmse:0.01707\n",
      "[695]\tvalidation-rmse:0.01707\n",
      "[696]\tvalidation-rmse:0.01707\n",
      "[697]\tvalidation-rmse:0.01707\n",
      "[698]\tvalidation-rmse:0.01706\n",
      "[699]\tvalidation-rmse:0.01706\n",
      "[700]\tvalidation-rmse:0.01706\n",
      "[701]\tvalidation-rmse:0.01706\n",
      "[702]\tvalidation-rmse:0.01705\n",
      "[703]\tvalidation-rmse:0.01705\n",
      "[704]\tvalidation-rmse:0.01705\n",
      "[705]\tvalidation-rmse:0.01704\n",
      "[706]\tvalidation-rmse:0.01704\n",
      "[707]\tvalidation-rmse:0.01704\n",
      "[708]\tvalidation-rmse:0.01703\n",
      "[709]\tvalidation-rmse:0.01703\n",
      "[710]\tvalidation-rmse:0.01703\n",
      "[711]\tvalidation-rmse:0.01702\n",
      "[712]\tvalidation-rmse:0.01702\n",
      "[713]\tvalidation-rmse:0.01701\n",
      "[714]\tvalidation-rmse:0.01701\n",
      "[715]\tvalidation-rmse:0.01701\n",
      "[716]\tvalidation-rmse:0.01701\n",
      "[717]\tvalidation-rmse:0.01700\n",
      "[718]\tvalidation-rmse:0.01700\n",
      "[719]\tvalidation-rmse:0.01700\n",
      "[720]\tvalidation-rmse:0.01699\n",
      "[721]\tvalidation-rmse:0.01699\n",
      "[722]\tvalidation-rmse:0.01699\n",
      "[723]\tvalidation-rmse:0.01698\n",
      "[724]\tvalidation-rmse:0.01698\n",
      "[725]\tvalidation-rmse:0.01698\n",
      "[726]\tvalidation-rmse:0.01697\n",
      "[727]\tvalidation-rmse:0.01697\n",
      "[728]\tvalidation-rmse:0.01697\n",
      "[729]\tvalidation-rmse:0.01697\n",
      "[730]\tvalidation-rmse:0.01697\n",
      "[731]\tvalidation-rmse:0.01696\n",
      "[732]\tvalidation-rmse:0.01696\n",
      "[733]\tvalidation-rmse:0.01696\n",
      "[734]\tvalidation-rmse:0.01696\n",
      "[735]\tvalidation-rmse:0.01695\n",
      "[736]\tvalidation-rmse:0.01695\n",
      "[737]\tvalidation-rmse:0.01695\n",
      "[738]\tvalidation-rmse:0.01695\n",
      "[739]\tvalidation-rmse:0.01694\n",
      "[740]\tvalidation-rmse:0.01694\n",
      "[741]\tvalidation-rmse:0.01694\n",
      "[742]\tvalidation-rmse:0.01694\n",
      "[743]\tvalidation-rmse:0.01694\n",
      "[744]\tvalidation-rmse:0.01693\n",
      "[745]\tvalidation-rmse:0.01693\n",
      "[746]\tvalidation-rmse:0.01692\n",
      "[747]\tvalidation-rmse:0.01692\n",
      "[748]\tvalidation-rmse:0.01692\n",
      "[749]\tvalidation-rmse:0.01692\n",
      "[750]\tvalidation-rmse:0.01692\n",
      "[751]\tvalidation-rmse:0.01692\n",
      "[752]\tvalidation-rmse:0.01692\n",
      "[753]\tvalidation-rmse:0.01692\n",
      "[754]\tvalidation-rmse:0.01691\n",
      "[755]\tvalidation-rmse:0.01691\n",
      "[756]\tvalidation-rmse:0.01691\n",
      "[757]\tvalidation-rmse:0.01691\n",
      "[758]\tvalidation-rmse:0.01691\n",
      "[759]\tvalidation-rmse:0.01690\n",
      "[760]\tvalidation-rmse:0.01690\n",
      "[761]\tvalidation-rmse:0.01690\n",
      "[762]\tvalidation-rmse:0.01690\n",
      "[763]\tvalidation-rmse:0.01689\n",
      "[764]\tvalidation-rmse:0.01689\n",
      "[765]\tvalidation-rmse:0.01689\n",
      "[766]\tvalidation-rmse:0.01689\n",
      "[767]\tvalidation-rmse:0.01689\n",
      "[768]\tvalidation-rmse:0.01689\n",
      "[769]\tvalidation-rmse:0.01689\n",
      "[770]\tvalidation-rmse:0.01689\n",
      "[771]\tvalidation-rmse:0.01688\n",
      "[772]\tvalidation-rmse:0.01688\n",
      "[773]\tvalidation-rmse:0.01688\n",
      "[774]\tvalidation-rmse:0.01688\n",
      "[775]\tvalidation-rmse:0.01688\n",
      "[776]\tvalidation-rmse:0.01688\n",
      "[777]\tvalidation-rmse:0.01687\n",
      "[778]\tvalidation-rmse:0.01687\n",
      "[779]\tvalidation-rmse:0.01687\n",
      "[780]\tvalidation-rmse:0.01686\n",
      "[781]\tvalidation-rmse:0.01686\n",
      "[782]\tvalidation-rmse:0.01687\n",
      "[783]\tvalidation-rmse:0.01686\n",
      "[784]\tvalidation-rmse:0.01686\n",
      "[785]\tvalidation-rmse:0.01686\n",
      "[786]\tvalidation-rmse:0.01686\n",
      "[787]\tvalidation-rmse:0.01686\n",
      "[788]\tvalidation-rmse:0.01686\n",
      "[789]\tvalidation-rmse:0.01686\n",
      "[790]\tvalidation-rmse:0.01685\n",
      "[791]\tvalidation-rmse:0.01685\n",
      "[792]\tvalidation-rmse:0.01685\n",
      "[793]\tvalidation-rmse:0.01685\n",
      "[794]\tvalidation-rmse:0.01685\n",
      "[795]\tvalidation-rmse:0.01684\n",
      "[796]\tvalidation-rmse:0.01684\n",
      "[797]\tvalidation-rmse:0.01684\n",
      "[798]\tvalidation-rmse:0.01684\n",
      "[799]\tvalidation-rmse:0.01684\n",
      "[800]\tvalidation-rmse:0.01684\n",
      "[801]\tvalidation-rmse:0.01684\n",
      "[802]\tvalidation-rmse:0.01683\n",
      "[803]\tvalidation-rmse:0.01683\n",
      "[804]\tvalidation-rmse:0.01683\n",
      "[805]\tvalidation-rmse:0.01683\n",
      "[806]\tvalidation-rmse:0.01683\n",
      "[807]\tvalidation-rmse:0.01684\n",
      "[808]\tvalidation-rmse:0.01684\n",
      "[809]\tvalidation-rmse:0.01684\n",
      "[810]\tvalidation-rmse:0.01683\n",
      "[811]\tvalidation-rmse:0.01683\n",
      "[812]\tvalidation-rmse:0.01683\n",
      "[813]\tvalidation-rmse:0.01683\n",
      "[814]\tvalidation-rmse:0.01682\n",
      "[815]\tvalidation-rmse:0.01683\n",
      "[816]\tvalidation-rmse:0.01683\n",
      "[817]\tvalidation-rmse:0.01682\n",
      "[818]\tvalidation-rmse:0.01682\n",
      "[819]\tvalidation-rmse:0.01682\n",
      "[820]\tvalidation-rmse:0.01682\n",
      "[821]\tvalidation-rmse:0.01682\n",
      "[822]\tvalidation-rmse:0.01682\n",
      "[823]\tvalidation-rmse:0.01682\n",
      "[824]\tvalidation-rmse:0.01682\n",
      "[825]\tvalidation-rmse:0.01682\n",
      "[826]\tvalidation-rmse:0.01681\n",
      "[827]\tvalidation-rmse:0.01681\n",
      "[828]\tvalidation-rmse:0.01681\n",
      "[829]\tvalidation-rmse:0.01681\n",
      "[830]\tvalidation-rmse:0.01681\n",
      "[831]\tvalidation-rmse:0.01681\n",
      "[832]\tvalidation-rmse:0.01681\n",
      "[833]\tvalidation-rmse:0.01681\n",
      "[834]\tvalidation-rmse:0.01680\n",
      "[835]\tvalidation-rmse:0.01680\n",
      "[836]\tvalidation-rmse:0.01680\n",
      "[837]\tvalidation-rmse:0.01679\n",
      "[838]\tvalidation-rmse:0.01679\n",
      "[839]\tvalidation-rmse:0.01679\n",
      "[840]\tvalidation-rmse:0.01679\n",
      "[841]\tvalidation-rmse:0.01679\n",
      "[842]\tvalidation-rmse:0.01679\n",
      "[843]\tvalidation-rmse:0.01679\n",
      "[844]\tvalidation-rmse:0.01679\n",
      "[845]\tvalidation-rmse:0.01678\n",
      "[846]\tvalidation-rmse:0.01678\n",
      "[847]\tvalidation-rmse:0.01678\n",
      "[848]\tvalidation-rmse:0.01678\n",
      "[849]\tvalidation-rmse:0.01678\n",
      "[850]\tvalidation-rmse:0.01678\n",
      "[851]\tvalidation-rmse:0.01678\n",
      "[852]\tvalidation-rmse:0.01678\n",
      "[853]\tvalidation-rmse:0.01678\n",
      "[854]\tvalidation-rmse:0.01678\n",
      "[855]\tvalidation-rmse:0.01678\n",
      "[856]\tvalidation-rmse:0.01678\n",
      "[857]\tvalidation-rmse:0.01677\n",
      "[858]\tvalidation-rmse:0.01677\n",
      "[859]\tvalidation-rmse:0.01677\n",
      "[860]\tvalidation-rmse:0.01677\n",
      "[861]\tvalidation-rmse:0.01677\n",
      "[862]\tvalidation-rmse:0.01677\n",
      "[863]\tvalidation-rmse:0.01677\n",
      "[864]\tvalidation-rmse:0.01677\n",
      "[865]\tvalidation-rmse:0.01676\n",
      "[866]\tvalidation-rmse:0.01676\n",
      "[867]\tvalidation-rmse:0.01676\n",
      "[868]\tvalidation-rmse:0.01676\n",
      "[869]\tvalidation-rmse:0.01676\n",
      "[870]\tvalidation-rmse:0.01676\n",
      "[871]\tvalidation-rmse:0.01676\n",
      "[872]\tvalidation-rmse:0.01676\n",
      "[873]\tvalidation-rmse:0.01676\n",
      "[874]\tvalidation-rmse:0.01676\n",
      "[875]\tvalidation-rmse:0.01676\n",
      "[876]\tvalidation-rmse:0.01675\n",
      "[877]\tvalidation-rmse:0.01676\n",
      "[878]\tvalidation-rmse:0.01676\n",
      "[879]\tvalidation-rmse:0.01676\n",
      "[880]\tvalidation-rmse:0.01675\n",
      "[881]\tvalidation-rmse:0.01675\n",
      "[882]\tvalidation-rmse:0.01675\n",
      "[883]\tvalidation-rmse:0.01675\n",
      "[884]\tvalidation-rmse:0.01676\n",
      "[885]\tvalidation-rmse:0.01676\n",
      "[886]\tvalidation-rmse:0.01675\n",
      "[887]\tvalidation-rmse:0.01675\n",
      "[888]\tvalidation-rmse:0.01675\n",
      "[889]\tvalidation-rmse:0.01675\n",
      "[890]\tvalidation-rmse:0.01675\n",
      "[891]\tvalidation-rmse:0.01675\n",
      "[892]\tvalidation-rmse:0.01675\n",
      "[893]\tvalidation-rmse:0.01675\n",
      "[894]\tvalidation-rmse:0.01675\n",
      "[895]\tvalidation-rmse:0.01675\n",
      "[896]\tvalidation-rmse:0.01674\n",
      "[897]\tvalidation-rmse:0.01674\n",
      "[898]\tvalidation-rmse:0.01674\n",
      "[899]\tvalidation-rmse:0.01674\n",
      "[900]\tvalidation-rmse:0.01674\n",
      "[901]\tvalidation-rmse:0.01674\n",
      "[902]\tvalidation-rmse:0.01674\n",
      "[903]\tvalidation-rmse:0.01674\n",
      "[904]\tvalidation-rmse:0.01674\n",
      "[905]\tvalidation-rmse:0.01674\n",
      "[906]\tvalidation-rmse:0.01674\n",
      "[907]\tvalidation-rmse:0.01674\n",
      "[908]\tvalidation-rmse:0.01674\n",
      "[909]\tvalidation-rmse:0.01674\n",
      "[910]\tvalidation-rmse:0.01674\n",
      "[911]\tvalidation-rmse:0.01674\n",
      "[912]\tvalidation-rmse:0.01674\n",
      "[913]\tvalidation-rmse:0.01673\n",
      "[914]\tvalidation-rmse:0.01673\n",
      "[915]\tvalidation-rmse:0.01673\n",
      "[916]\tvalidation-rmse:0.01673\n",
      "[917]\tvalidation-rmse:0.01673\n",
      "[918]\tvalidation-rmse:0.01673\n",
      "[919]\tvalidation-rmse:0.01673\n",
      "[920]\tvalidation-rmse:0.01673\n",
      "[921]\tvalidation-rmse:0.01673\n",
      "[922]\tvalidation-rmse:0.01673\n",
      "[923]\tvalidation-rmse:0.01673\n",
      "[924]\tvalidation-rmse:0.01673\n",
      "[925]\tvalidation-rmse:0.01673\n",
      "[926]\tvalidation-rmse:0.01673\n",
      "[927]\tvalidation-rmse:0.01673\n",
      "[928]\tvalidation-rmse:0.01673\n",
      "[929]\tvalidation-rmse:0.01673\n",
      "[930]\tvalidation-rmse:0.01672\n",
      "[931]\tvalidation-rmse:0.01672\n",
      "[932]\tvalidation-rmse:0.01672\n",
      "[933]\tvalidation-rmse:0.01673\n",
      "[934]\tvalidation-rmse:0.01672\n",
      "[935]\tvalidation-rmse:0.01672\n",
      "[936]\tvalidation-rmse:0.01672\n",
      "[937]\tvalidation-rmse:0.01672\n",
      "[938]\tvalidation-rmse:0.01672\n",
      "[939]\tvalidation-rmse:0.01672\n",
      "[940]\tvalidation-rmse:0.01672\n",
      "[941]\tvalidation-rmse:0.01672\n",
      "[942]\tvalidation-rmse:0.01672\n",
      "[943]\tvalidation-rmse:0.01672\n",
      "[944]\tvalidation-rmse:0.01672\n",
      "[945]\tvalidation-rmse:0.01672\n",
      "[946]\tvalidation-rmse:0.01672\n",
      "[947]\tvalidation-rmse:0.01672\n",
      "[948]\tvalidation-rmse:0.01671\n",
      "[949]\tvalidation-rmse:0.01671\n",
      "[950]\tvalidation-rmse:0.01671\n",
      "[951]\tvalidation-rmse:0.01671\n",
      "[952]\tvalidation-rmse:0.01671\n",
      "[953]\tvalidation-rmse:0.01671\n",
      "[954]\tvalidation-rmse:0.01671\n",
      "[955]\tvalidation-rmse:0.01671\n",
      "[956]\tvalidation-rmse:0.01671\n",
      "[957]\tvalidation-rmse:0.01671\n",
      "[958]\tvalidation-rmse:0.01670\n",
      "[959]\tvalidation-rmse:0.01670\n",
      "[960]\tvalidation-rmse:0.01670\n",
      "[961]\tvalidation-rmse:0.01670\n",
      "[962]\tvalidation-rmse:0.01670\n",
      "[963]\tvalidation-rmse:0.01670\n",
      "[964]\tvalidation-rmse:0.01670\n",
      "[965]\tvalidation-rmse:0.01670\n",
      "[966]\tvalidation-rmse:0.01670\n",
      "[967]\tvalidation-rmse:0.01670\n",
      "[968]\tvalidation-rmse:0.01670\n",
      "[969]\tvalidation-rmse:0.01670\n",
      "[970]\tvalidation-rmse:0.01670\n",
      "[971]\tvalidation-rmse:0.01670\n",
      "[972]\tvalidation-rmse:0.01670\n",
      "[973]\tvalidation-rmse:0.01670\n",
      "[974]\tvalidation-rmse:0.01670\n",
      "[975]\tvalidation-rmse:0.01670\n",
      "[976]\tvalidation-rmse:0.01670\n",
      "[977]\tvalidation-rmse:0.01670\n",
      "[978]\tvalidation-rmse:0.01670\n",
      "[979]\tvalidation-rmse:0.01670\n",
      "[980]\tvalidation-rmse:0.01670\n",
      "[981]\tvalidation-rmse:0.01670\n",
      "[982]\tvalidation-rmse:0.01670\n",
      "[983]\tvalidation-rmse:0.01669\n",
      "[984]\tvalidation-rmse:0.01669\n",
      "[985]\tvalidation-rmse:0.01669\n",
      "[986]\tvalidation-rmse:0.01669\n",
      "[987]\tvalidation-rmse:0.01669\n",
      "[988]\tvalidation-rmse:0.01669\n",
      "[989]\tvalidation-rmse:0.01669\n",
      "[990]\tvalidation-rmse:0.01669\n",
      "[991]\tvalidation-rmse:0.01669\n",
      "[992]\tvalidation-rmse:0.01669\n",
      "[993]\tvalidation-rmse:0.01669\n",
      "[994]\tvalidation-rmse:0.01669\n",
      "[995]\tvalidation-rmse:0.01668\n",
      "[996]\tvalidation-rmse:0.01668\n",
      "[997]\tvalidation-rmse:0.01668\n",
      "[998]\tvalidation-rmse:0.01668\n",
      "[999]\tvalidation-rmse:0.01668\n",
      "[1000]\tvalidation-rmse:0.01668\n",
      "[1001]\tvalidation-rmse:0.01668\n",
      "[1002]\tvalidation-rmse:0.01668\n",
      "[1003]\tvalidation-rmse:0.01668\n",
      "[1004]\tvalidation-rmse:0.01668\n",
      "[1005]\tvalidation-rmse:0.01668\n",
      "[1006]\tvalidation-rmse:0.01668\n",
      "[1007]\tvalidation-rmse:0.01668\n",
      "[1008]\tvalidation-rmse:0.01668\n",
      "[1009]\tvalidation-rmse:0.01668\n",
      "[1010]\tvalidation-rmse:0.01668\n",
      "[1011]\tvalidation-rmse:0.01667\n",
      "[1012]\tvalidation-rmse:0.01667\n",
      "[1013]\tvalidation-rmse:0.01667\n",
      "[1014]\tvalidation-rmse:0.01667\n",
      "[1015]\tvalidation-rmse:0.01667\n",
      "[1016]\tvalidation-rmse:0.01667\n",
      "[1017]\tvalidation-rmse:0.01667\n",
      "[1018]\tvalidation-rmse:0.01667\n",
      "[1019]\tvalidation-rmse:0.01667\n",
      "[1020]\tvalidation-rmse:0.01667\n",
      "[1021]\tvalidation-rmse:0.01666\n",
      "[1022]\tvalidation-rmse:0.01666\n",
      "[1023]\tvalidation-rmse:0.01666\n",
      "[1024]\tvalidation-rmse:0.01666\n",
      "[1025]\tvalidation-rmse:0.01666\n",
      "[1026]\tvalidation-rmse:0.01666\n",
      "[1027]\tvalidation-rmse:0.01666\n",
      "[1028]\tvalidation-rmse:0.01666\n",
      "[1029]\tvalidation-rmse:0.01666\n",
      "[1030]\tvalidation-rmse:0.01666\n",
      "[1031]\tvalidation-rmse:0.01666\n",
      "[1032]\tvalidation-rmse:0.01666\n",
      "[1033]\tvalidation-rmse:0.01666\n",
      "[1034]\tvalidation-rmse:0.01666\n",
      "[1035]\tvalidation-rmse:0.01665\n",
      "[1036]\tvalidation-rmse:0.01665\n",
      "[1037]\tvalidation-rmse:0.01665\n",
      "[1038]\tvalidation-rmse:0.01665\n",
      "[1039]\tvalidation-rmse:0.01665\n",
      "[1040]\tvalidation-rmse:0.01665\n",
      "[1041]\tvalidation-rmse:0.01665\n",
      "[1042]\tvalidation-rmse:0.01665\n",
      "[1043]\tvalidation-rmse:0.01665\n",
      "[1044]\tvalidation-rmse:0.01665\n",
      "[1045]\tvalidation-rmse:0.01665\n",
      "[1046]\tvalidation-rmse:0.01664\n",
      "[1047]\tvalidation-rmse:0.01664\n",
      "[1048]\tvalidation-rmse:0.01664\n",
      "[1049]\tvalidation-rmse:0.01664\n",
      "[1050]\tvalidation-rmse:0.01664\n",
      "[1051]\tvalidation-rmse:0.01663\n",
      "[1052]\tvalidation-rmse:0.01663\n",
      "[1053]\tvalidation-rmse:0.01663\n",
      "[1054]\tvalidation-rmse:0.01663\n",
      "[1055]\tvalidation-rmse:0.01663\n",
      "[1056]\tvalidation-rmse:0.01663\n",
      "[1057]\tvalidation-rmse:0.01663\n",
      "[1058]\tvalidation-rmse:0.01663\n",
      "[1059]\tvalidation-rmse:0.01663\n",
      "[1060]\tvalidation-rmse:0.01663\n",
      "[1061]\tvalidation-rmse:0.01663\n",
      "[1062]\tvalidation-rmse:0.01663\n",
      "[1063]\tvalidation-rmse:0.01663\n",
      "[1064]\tvalidation-rmse:0.01663\n",
      "[1065]\tvalidation-rmse:0.01663\n",
      "[1066]\tvalidation-rmse:0.01663\n",
      "[1067]\tvalidation-rmse:0.01663\n",
      "[1068]\tvalidation-rmse:0.01663\n",
      "[1069]\tvalidation-rmse:0.01663\n",
      "[1070]\tvalidation-rmse:0.01663\n",
      "[1071]\tvalidation-rmse:0.01663\n",
      "[1072]\tvalidation-rmse:0.01663\n",
      "[1073]\tvalidation-rmse:0.01663\n",
      "[1074]\tvalidation-rmse:0.01663\n",
      "[1075]\tvalidation-rmse:0.01663\n",
      "[1076]\tvalidation-rmse:0.01663\n",
      "[1077]\tvalidation-rmse:0.01662\n",
      "[1078]\tvalidation-rmse:0.01662\n",
      "[1079]\tvalidation-rmse:0.01662\n",
      "[1080]\tvalidation-rmse:0.01662\n",
      "[1081]\tvalidation-rmse:0.01662\n",
      "[1082]\tvalidation-rmse:0.01662\n",
      "[1083]\tvalidation-rmse:0.01662\n",
      "[1084]\tvalidation-rmse:0.01662\n",
      "[1085]\tvalidation-rmse:0.01662\n",
      "[1086]\tvalidation-rmse:0.01662\n",
      "[1087]\tvalidation-rmse:0.01662\n",
      "[1088]\tvalidation-rmse:0.01662\n",
      "[1089]\tvalidation-rmse:0.01662\n",
      "[1090]\tvalidation-rmse:0.01662\n",
      "[1091]\tvalidation-rmse:0.01661\n",
      "[1092]\tvalidation-rmse:0.01661\n",
      "[1093]\tvalidation-rmse:0.01661\n",
      "[1094]\tvalidation-rmse:0.01661\n",
      "[1095]\tvalidation-rmse:0.01661\n",
      "[1096]\tvalidation-rmse:0.01661\n",
      "[1097]\tvalidation-rmse:0.01661\n",
      "[1098]\tvalidation-rmse:0.01661\n",
      "[1099]\tvalidation-rmse:0.01661\n",
      "[1100]\tvalidation-rmse:0.01661\n",
      "[1101]\tvalidation-rmse:0.01661\n",
      "[1102]\tvalidation-rmse:0.01661\n",
      "[1103]\tvalidation-rmse:0.01661\n",
      "[1104]\tvalidation-rmse:0.01661\n",
      "[1105]\tvalidation-rmse:0.01661\n",
      "[1106]\tvalidation-rmse:0.01660\n",
      "[1107]\tvalidation-rmse:0.01660\n",
      "[1108]\tvalidation-rmse:0.01660\n",
      "[1109]\tvalidation-rmse:0.01660\n",
      "[1110]\tvalidation-rmse:0.01660\n",
      "[1111]\tvalidation-rmse:0.01660\n",
      "[1112]\tvalidation-rmse:0.01660\n",
      "[1113]\tvalidation-rmse:0.01660\n",
      "[1114]\tvalidation-rmse:0.01660\n",
      "[1115]\tvalidation-rmse:0.01660\n",
      "[1116]\tvalidation-rmse:0.01660\n",
      "[1117]\tvalidation-rmse:0.01660\n",
      "[1118]\tvalidation-rmse:0.01659\n",
      "[1119]\tvalidation-rmse:0.01659\n",
      "[1120]\tvalidation-rmse:0.01659\n",
      "[1121]\tvalidation-rmse:0.01659\n",
      "[1122]\tvalidation-rmse:0.01659\n",
      "[1123]\tvalidation-rmse:0.01659\n",
      "[1124]\tvalidation-rmse:0.01659\n",
      "[1125]\tvalidation-rmse:0.01659\n",
      "[1126]\tvalidation-rmse:0.01659\n",
      "[1127]\tvalidation-rmse:0.01659\n",
      "[1128]\tvalidation-rmse:0.01659\n",
      "[1129]\tvalidation-rmse:0.01659\n",
      "[1130]\tvalidation-rmse:0.01659\n",
      "[1131]\tvalidation-rmse:0.01659\n",
      "[1132]\tvalidation-rmse:0.01659\n",
      "[1133]\tvalidation-rmse:0.01659\n",
      "[1134]\tvalidation-rmse:0.01659\n",
      "[1135]\tvalidation-rmse:0.01659\n",
      "[1136]\tvalidation-rmse:0.01658\n",
      "[1137]\tvalidation-rmse:0.01658\n",
      "[1138]\tvalidation-rmse:0.01658\n",
      "[1139]\tvalidation-rmse:0.01658\n",
      "[1140]\tvalidation-rmse:0.01658\n",
      "[1141]\tvalidation-rmse:0.01658\n",
      "[1142]\tvalidation-rmse:0.01658\n",
      "[1143]\tvalidation-rmse:0.01658\n",
      "[1144]\tvalidation-rmse:0.01658\n",
      "[1145]\tvalidation-rmse:0.01658\n",
      "[1146]\tvalidation-rmse:0.01659\n",
      "[1147]\tvalidation-rmse:0.01659\n",
      "[1148]\tvalidation-rmse:0.01658\n",
      "[1149]\tvalidation-rmse:0.01658\n",
      "[1150]\tvalidation-rmse:0.01658\n",
      "[1151]\tvalidation-rmse:0.01658\n",
      "[1152]\tvalidation-rmse:0.01658\n",
      "[1153]\tvalidation-rmse:0.01658\n",
      "[1154]\tvalidation-rmse:0.01658\n",
      "[1155]\tvalidation-rmse:0.01658\n",
      "[1156]\tvalidation-rmse:0.01658\n",
      "[1157]\tvalidation-rmse:0.01658\n",
      "[1158]\tvalidation-rmse:0.01658\n",
      "[1159]\tvalidation-rmse:0.01658\n",
      "[1160]\tvalidation-rmse:0.01658\n",
      "[1161]\tvalidation-rmse:0.01658\n",
      "[1162]\tvalidation-rmse:0.01658\n",
      "[1163]\tvalidation-rmse:0.01658\n",
      "[1164]\tvalidation-rmse:0.01658\n",
      "[1165]\tvalidation-rmse:0.01658\n",
      "[1166]\tvalidation-rmse:0.01657\n",
      "[1167]\tvalidation-rmse:0.01657\n",
      "[1168]\tvalidation-rmse:0.01657\n",
      "[1169]\tvalidation-rmse:0.01657\n",
      "[1170]\tvalidation-rmse:0.01658\n",
      "[1171]\tvalidation-rmse:0.01657\n",
      "[1172]\tvalidation-rmse:0.01657\n",
      "[1173]\tvalidation-rmse:0.01658\n",
      "[1174]\tvalidation-rmse:0.01658\n",
      "[1175]\tvalidation-rmse:0.01657\n",
      "[1176]\tvalidation-rmse:0.01657\n",
      "[1177]\tvalidation-rmse:0.01657\n",
      "[1178]\tvalidation-rmse:0.01657\n",
      "[1179]\tvalidation-rmse:0.01657\n",
      "[1180]\tvalidation-rmse:0.01657\n",
      "[1181]\tvalidation-rmse:0.01658\n",
      "[1182]\tvalidation-rmse:0.01658\n",
      "[1183]\tvalidation-rmse:0.01658\n",
      "[1184]\tvalidation-rmse:0.01658\n",
      "[1185]\tvalidation-rmse:0.01658\n",
      "[1186]\tvalidation-rmse:0.01658\n",
      "[1187]\tvalidation-rmse:0.01658\n",
      "[1188]\tvalidation-rmse:0.01657\n",
      "[1189]\tvalidation-rmse:0.01658\n",
      "[1190]\tvalidation-rmse:0.01658\n",
      "[1191]\tvalidation-rmse:0.01657\n",
      "[1192]\tvalidation-rmse:0.01657\n",
      "[1193]\tvalidation-rmse:0.01657\n",
      "[1194]\tvalidation-rmse:0.01657\n",
      "[1195]\tvalidation-rmse:0.01657\n",
      "[1196]\tvalidation-rmse:0.01657\n",
      "[1197]\tvalidation-rmse:0.01657\n",
      "[1198]\tvalidation-rmse:0.01657\n",
      "[1199]\tvalidation-rmse:0.01657\n",
      "[1200]\tvalidation-rmse:0.01657\n",
      "[1201]\tvalidation-rmse:0.01657\n",
      "[1202]\tvalidation-rmse:0.01657\n",
      "[1203]\tvalidation-rmse:0.01657\n",
      "[1204]\tvalidation-rmse:0.01658\n",
      "[1205]\tvalidation-rmse:0.01658\n",
      "[1206]\tvalidation-rmse:0.01658\n",
      "[1207]\tvalidation-rmse:0.01657\n",
      "[1208]\tvalidation-rmse:0.01657\n",
      "[1209]\tvalidation-rmse:0.01657\n",
      "[1210]\tvalidation-rmse:0.01657\n",
      "[1211]\tvalidation-rmse:0.01657\n",
      "[1212]\tvalidation-rmse:0.01657\n",
      "[1213]\tvalidation-rmse:0.01657\n",
      "[1214]\tvalidation-rmse:0.01657\n",
      "[1215]\tvalidation-rmse:0.01657\n",
      "[1216]\tvalidation-rmse:0.01657\n",
      "[1217]\tvalidation-rmse:0.01657\n",
      "[1218]\tvalidation-rmse:0.01657\n",
      "[1219]\tvalidation-rmse:0.01657\n",
      "[1220]\tvalidation-rmse:0.01657\n",
      "[1221]\tvalidation-rmse:0.01657\n",
      "[1222]\tvalidation-rmse:0.01657\n",
      "[1223]\tvalidation-rmse:0.01657\n",
      "[1224]\tvalidation-rmse:0.01656\n",
      "[1225]\tvalidation-rmse:0.01656\n",
      "[1226]\tvalidation-rmse:0.01656\n",
      "[1227]\tvalidation-rmse:0.01656\n",
      "[1228]\tvalidation-rmse:0.01656\n",
      "[1229]\tvalidation-rmse:0.01655\n",
      "[1230]\tvalidation-rmse:0.01655\n",
      "[1231]\tvalidation-rmse:0.01655\n",
      "[1232]\tvalidation-rmse:0.01655\n",
      "[1233]\tvalidation-rmse:0.01655\n",
      "[1234]\tvalidation-rmse:0.01655\n",
      "[1235]\tvalidation-rmse:0.01655\n",
      "[1236]\tvalidation-rmse:0.01655\n",
      "[1237]\tvalidation-rmse:0.01655\n",
      "[1238]\tvalidation-rmse:0.01655\n",
      "[1239]\tvalidation-rmse:0.01655\n",
      "[1240]\tvalidation-rmse:0.01655\n",
      "[1241]\tvalidation-rmse:0.01655\n",
      "[1242]\tvalidation-rmse:0.01655\n",
      "[1243]\tvalidation-rmse:0.01655\n",
      "[1244]\tvalidation-rmse:0.01655\n",
      "[1245]\tvalidation-rmse:0.01655\n",
      "[1246]\tvalidation-rmse:0.01655\n",
      "[1247]\tvalidation-rmse:0.01655\n",
      "[1248]\tvalidation-rmse:0.01655\n",
      "[1249]\tvalidation-rmse:0.01655\n",
      "[1250]\tvalidation-rmse:0.01654\n",
      "[1251]\tvalidation-rmse:0.01654\n",
      "[1252]\tvalidation-rmse:0.01654\n",
      "[1253]\tvalidation-rmse:0.01655\n",
      "[1254]\tvalidation-rmse:0.01655\n",
      "[1255]\tvalidation-rmse:0.01655\n",
      "[1256]\tvalidation-rmse:0.01655\n",
      "[1257]\tvalidation-rmse:0.01655\n",
      "[1258]\tvalidation-rmse:0.01655\n",
      "[1259]\tvalidation-rmse:0.01655\n",
      "[1260]\tvalidation-rmse:0.01655\n",
      "[1261]\tvalidation-rmse:0.01655\n",
      "[1262]\tvalidation-rmse:0.01655\n",
      "[1263]\tvalidation-rmse:0.01655\n",
      "[1264]\tvalidation-rmse:0.01655\n",
      "[1265]\tvalidation-rmse:0.01655\n",
      "[1266]\tvalidation-rmse:0.01654\n",
      "[1267]\tvalidation-rmse:0.01654\n",
      "[1268]\tvalidation-rmse:0.01654\n",
      "[1269]\tvalidation-rmse:0.01655\n",
      "[1270]\tvalidation-rmse:0.01655\n",
      "[1271]\tvalidation-rmse:0.01655\n",
      "[1272]\tvalidation-rmse:0.01655\n",
      "[1273]\tvalidation-rmse:0.01654\n",
      "[1274]\tvalidation-rmse:0.01654\n",
      "[1275]\tvalidation-rmse:0.01654\n",
      "[1276]\tvalidation-rmse:0.01654\n",
      "[1277]\tvalidation-rmse:0.01654\n",
      "[1278]\tvalidation-rmse:0.01654\n",
      "[1279]\tvalidation-rmse:0.01654\n",
      "[1280]\tvalidation-rmse:0.01654\n",
      "[1281]\tvalidation-rmse:0.01654\n",
      "[1282]\tvalidation-rmse:0.01654\n",
      "[1283]\tvalidation-rmse:0.01654\n",
      "[1284]\tvalidation-rmse:0.01654\n",
      "[1285]\tvalidation-rmse:0.01654\n",
      "[1286]\tvalidation-rmse:0.01654\n",
      "[1287]\tvalidation-rmse:0.01654\n",
      "[1288]\tvalidation-rmse:0.01654\n",
      "[1289]\tvalidation-rmse:0.01654\n",
      "[1290]\tvalidation-rmse:0.01654\n",
      "[1291]\tvalidation-rmse:0.01654\n",
      "[1292]\tvalidation-rmse:0.01654\n",
      "[1293]\tvalidation-rmse:0.01654\n",
      "[1294]\tvalidation-rmse:0.01653\n",
      "[1295]\tvalidation-rmse:0.01653\n",
      "[1296]\tvalidation-rmse:0.01653\n",
      "[1297]\tvalidation-rmse:0.01653\n",
      "[1298]\tvalidation-rmse:0.01653\n",
      "[1299]\tvalidation-rmse:0.01653\n",
      "[1300]\tvalidation-rmse:0.01653\n",
      "[1301]\tvalidation-rmse:0.01653\n",
      "[1302]\tvalidation-rmse:0.01653\n",
      "[1303]\tvalidation-rmse:0.01653\n",
      "[1304]\tvalidation-rmse:0.01653\n",
      "[1305]\tvalidation-rmse:0.01653\n",
      "[1306]\tvalidation-rmse:0.01654\n",
      "[1307]\tvalidation-rmse:0.01653\n",
      "[1308]\tvalidation-rmse:0.01653\n",
      "[1309]\tvalidation-rmse:0.01653\n",
      "[1310]\tvalidation-rmse:0.01653\n",
      "[1311]\tvalidation-rmse:0.01653\n",
      "[1312]\tvalidation-rmse:0.01653\n",
      "[1313]\tvalidation-rmse:0.01653\n",
      "[1314]\tvalidation-rmse:0.01653\n",
      "[1315]\tvalidation-rmse:0.01653\n",
      "[1316]\tvalidation-rmse:0.01653\n",
      "[1317]\tvalidation-rmse:0.01653\n",
      "[1318]\tvalidation-rmse:0.01653\n",
      "[1319]\tvalidation-rmse:0.01653\n",
      "[1320]\tvalidation-rmse:0.01653\n",
      "[1321]\tvalidation-rmse:0.01653\n",
      "[1322]\tvalidation-rmse:0.01653\n",
      "[1323]\tvalidation-rmse:0.01653\n",
      "[1324]\tvalidation-rmse:0.01653\n",
      "[1325]\tvalidation-rmse:0.01653\n",
      "[1326]\tvalidation-rmse:0.01653\n",
      "[1327]\tvalidation-rmse:0.01653\n",
      "[1328]\tvalidation-rmse:0.01653\n",
      "[1329]\tvalidation-rmse:0.01653\n",
      "[1330]\tvalidation-rmse:0.01653\n",
      "[1331]\tvalidation-rmse:0.01653\n",
      "[1332]\tvalidation-rmse:0.01652\n",
      "[1333]\tvalidation-rmse:0.01652\n",
      "[1334]\tvalidation-rmse:0.01652\n",
      "[1335]\tvalidation-rmse:0.01652\n",
      "[1336]\tvalidation-rmse:0.01652\n",
      "[1337]\tvalidation-rmse:0.01652\n",
      "[1338]\tvalidation-rmse:0.01652\n",
      "[1339]\tvalidation-rmse:0.01652\n",
      "[1340]\tvalidation-rmse:0.01652\n",
      "[1341]\tvalidation-rmse:0.01652\n",
      "[1342]\tvalidation-rmse:0.01652\n",
      "[1343]\tvalidation-rmse:0.01652\n",
      "[1344]\tvalidation-rmse:0.01652\n",
      "[1345]\tvalidation-rmse:0.01652\n",
      "[1346]\tvalidation-rmse:0.01652\n",
      "[1347]\tvalidation-rmse:0.01652\n",
      "[1348]\tvalidation-rmse:0.01652\n",
      "[1349]\tvalidation-rmse:0.01652\n",
      "[1350]\tvalidation-rmse:0.01652\n",
      "[1351]\tvalidation-rmse:0.01652\n",
      "[1352]\tvalidation-rmse:0.01652\n",
      "[1353]\tvalidation-rmse:0.01652\n",
      "[1354]\tvalidation-rmse:0.01652\n",
      "[1355]\tvalidation-rmse:0.01652\n",
      "[1356]\tvalidation-rmse:0.01652\n",
      "[1357]\tvalidation-rmse:0.01652\n",
      "[1358]\tvalidation-rmse:0.01652\n",
      "[1359]\tvalidation-rmse:0.01652\n",
      "[1360]\tvalidation-rmse:0.01652\n",
      "[1361]\tvalidation-rmse:0.01652\n",
      "[1362]\tvalidation-rmse:0.01652\n",
      "[1363]\tvalidation-rmse:0.01652\n",
      "[1364]\tvalidation-rmse:0.01652\n",
      "[1365]\tvalidation-rmse:0.01651\n",
      "[1366]\tvalidation-rmse:0.01651\n",
      "[1367]\tvalidation-rmse:0.01652\n",
      "[1368]\tvalidation-rmse:0.01652\n",
      "[1369]\tvalidation-rmse:0.01652\n",
      "[1370]\tvalidation-rmse:0.01652\n",
      "[1371]\tvalidation-rmse:0.01652\n",
      "[1372]\tvalidation-rmse:0.01652\n",
      "[1373]\tvalidation-rmse:0.01652\n",
      "[1374]\tvalidation-rmse:0.01652\n",
      "[1375]\tvalidation-rmse:0.01652\n",
      "[1376]\tvalidation-rmse:0.01652\n",
      "[1377]\tvalidation-rmse:0.01652\n",
      "[1378]\tvalidation-rmse:0.01651\n",
      "[1379]\tvalidation-rmse:0.01651\n",
      "[1380]\tvalidation-rmse:0.01652\n",
      "[1381]\tvalidation-rmse:0.01652\n",
      "[1382]\tvalidation-rmse:0.01652\n",
      "[1383]\tvalidation-rmse:0.01651\n",
      "[1384]\tvalidation-rmse:0.01651\n",
      "[1385]\tvalidation-rmse:0.01651\n",
      "[1386]\tvalidation-rmse:0.01651\n",
      "[1387]\tvalidation-rmse:0.01651\n",
      "[1388]\tvalidation-rmse:0.01652\n",
      "[1389]\tvalidation-rmse:0.01651\n",
      "[1390]\tvalidation-rmse:0.01651\n",
      "[1391]\tvalidation-rmse:0.01651\n",
      "[1392]\tvalidation-rmse:0.01651\n",
      "[1393]\tvalidation-rmse:0.01651\n",
      "[1394]\tvalidation-rmse:0.01651\n",
      "[1395]\tvalidation-rmse:0.01651\n",
      "[1396]\tvalidation-rmse:0.01651\n",
      "[1397]\tvalidation-rmse:0.01651\n",
      "[1398]\tvalidation-rmse:0.01651\n",
      "[1399]\tvalidation-rmse:0.01651\n",
      "[1400]\tvalidation-rmse:0.01651\n",
      "[1401]\tvalidation-rmse:0.01651\n",
      "[1402]\tvalidation-rmse:0.01651\n",
      "[1403]\tvalidation-rmse:0.01651\n",
      "[1404]\tvalidation-rmse:0.01651\n",
      "[1405]\tvalidation-rmse:0.01651\n",
      "[1406]\tvalidation-rmse:0.01651\n",
      "[1407]\tvalidation-rmse:0.01651\n",
      "[1408]\tvalidation-rmse:0.01651\n",
      "[1409]\tvalidation-rmse:0.01651\n",
      "[1410]\tvalidation-rmse:0.01651\n",
      "[1411]\tvalidation-rmse:0.01652\n",
      "[1412]\tvalidation-rmse:0.01652\n",
      "[1413]\tvalidation-rmse:0.01652\n",
      "[1414]\tvalidation-rmse:0.01652\n",
      "[1415]\tvalidation-rmse:0.01651\n",
      "[1416]\tvalidation-rmse:0.01651\n",
      "[1417]\tvalidation-rmse:0.01651\n",
      "[1418]\tvalidation-rmse:0.01651\n",
      "[1419]\tvalidation-rmse:0.01651\n",
      "[1420]\tvalidation-rmse:0.01651\n",
      "[1421]\tvalidation-rmse:0.01651\n",
      "[1422]\tvalidation-rmse:0.01651\n",
      "[1423]\tvalidation-rmse:0.01651\n",
      "[1424]\tvalidation-rmse:0.01651\n",
      "[1425]\tvalidation-rmse:0.01651\n",
      "[1426]\tvalidation-rmse:0.01651\n",
      "[1427]\tvalidation-rmse:0.01651\n",
      "[1428]\tvalidation-rmse:0.01651\n",
      "[1429]\tvalidation-rmse:0.01651\n",
      "[1430]\tvalidation-rmse:0.01651\n",
      "[1431]\tvalidation-rmse:0.01651\n",
      "[1432]\tvalidation-rmse:0.01651\n",
      "[1433]\tvalidation-rmse:0.01651\n",
      "[1434]\tvalidation-rmse:0.01651\n",
      "[1435]\tvalidation-rmse:0.01651\n",
      "[1436]\tvalidation-rmse:0.01651\n",
      "[1437]\tvalidation-rmse:0.01651\n",
      "[1438]\tvalidation-rmse:0.01651\n",
      "[1439]\tvalidation-rmse:0.01651\n",
      "[1440]\tvalidation-rmse:0.01651\n",
      "[1441]\tvalidation-rmse:0.01651\n",
      "[1442]\tvalidation-rmse:0.01651\n",
      "[1443]\tvalidation-rmse:0.01651\n",
      "[1444]\tvalidation-rmse:0.01651\n",
      "[1445]\tvalidation-rmse:0.01651\n",
      "[1446]\tvalidation-rmse:0.01651\n",
      "[1447]\tvalidation-rmse:0.01651\n",
      "[1448]\tvalidation-rmse:0.01651\n",
      "[1449]\tvalidation-rmse:0.01651\n",
      "[1450]\tvalidation-rmse:0.01651\n",
      "[1451]\tvalidation-rmse:0.01651\n",
      "[1452]\tvalidation-rmse:0.01651\n",
      "[1453]\tvalidation-rmse:0.01651\n",
      "[1454]\tvalidation-rmse:0.01650\n",
      "[1455]\tvalidation-rmse:0.01650\n",
      "[1456]\tvalidation-rmse:0.01650\n",
      "[1457]\tvalidation-rmse:0.01650\n",
      "[1458]\tvalidation-rmse:0.01651\n",
      "[1459]\tvalidation-rmse:0.01651\n",
      "[1460]\tvalidation-rmse:0.01651\n",
      "[1461]\tvalidation-rmse:0.01651\n",
      "[1462]\tvalidation-rmse:0.01651\n",
      "[1463]\tvalidation-rmse:0.01651\n",
      "[1464]\tvalidation-rmse:0.01651\n",
      "[1465]\tvalidation-rmse:0.01651\n",
      "[1466]\tvalidation-rmse:0.01651\n",
      "[1467]\tvalidation-rmse:0.01651\n",
      "[1468]\tvalidation-rmse:0.01650\n",
      "[1469]\tvalidation-rmse:0.01650\n",
      "[1470]\tvalidation-rmse:0.01650\n",
      "[1471]\tvalidation-rmse:0.01650\n",
      "[1472]\tvalidation-rmse:0.01650\n",
      "[1473]\tvalidation-rmse:0.01650\n",
      "[1474]\tvalidation-rmse:0.01650\n",
      "[1475]\tvalidation-rmse:0.01650\n",
      "[1476]\tvalidation-rmse:0.01650\n",
      "[1477]\tvalidation-rmse:0.01650\n",
      "[1478]\tvalidation-rmse:0.01650\n",
      "[1479]\tvalidation-rmse:0.01650\n",
      "[1480]\tvalidation-rmse:0.01650\n",
      "[1481]\tvalidation-rmse:0.01650\n",
      "[1482]\tvalidation-rmse:0.01650\n",
      "[1483]\tvalidation-rmse:0.01650\n",
      "[1484]\tvalidation-rmse:0.01650\n",
      "[1485]\tvalidation-rmse:0.01650\n",
      "[1486]\tvalidation-rmse:0.01650\n",
      "[1487]\tvalidation-rmse:0.01650\n",
      "[1488]\tvalidation-rmse:0.01650\n",
      "[1489]\tvalidation-rmse:0.01650\n",
      "[1490]\tvalidation-rmse:0.01650\n",
      "[1491]\tvalidation-rmse:0.01650\n",
      "[1492]\tvalidation-rmse:0.01650\n",
      "[1493]\tvalidation-rmse:0.01650\n",
      "[1494]\tvalidation-rmse:0.01650\n",
      "[1495]\tvalidation-rmse:0.01650\n",
      "[1496]\tvalidation-rmse:0.01649\n",
      "[1497]\tvalidation-rmse:0.01650\n",
      "[1498]\tvalidation-rmse:0.01650\n",
      "[1499]\tvalidation-rmse:0.01650\n",
      "[1500]\tvalidation-rmse:0.01650\n",
      "[1501]\tvalidation-rmse:0.01650\n",
      "[1502]\tvalidation-rmse:0.01650\n",
      "[1503]\tvalidation-rmse:0.01650\n",
      "[1504]\tvalidation-rmse:0.01650\n",
      "[1505]\tvalidation-rmse:0.01650\n",
      "[1506]\tvalidation-rmse:0.01650\n",
      "[1507]\tvalidation-rmse:0.01650\n",
      "[1508]\tvalidation-rmse:0.01650\n",
      "[1509]\tvalidation-rmse:0.01650\n",
      "[1510]\tvalidation-rmse:0.01650\n",
      "[1511]\tvalidation-rmse:0.01650\n",
      "[1512]\tvalidation-rmse:0.01650\n",
      "[1513]\tvalidation-rmse:0.01650\n",
      "[1514]\tvalidation-rmse:0.01650\n",
      "[1515]\tvalidation-rmse:0.01650\n",
      "[1516]\tvalidation-rmse:0.01650\n",
      "[1517]\tvalidation-rmse:0.01650\n",
      "[1518]\tvalidation-rmse:0.01650\n",
      "[1519]\tvalidation-rmse:0.01650\n",
      "[1520]\tvalidation-rmse:0.01650\n",
      "[1521]\tvalidation-rmse:0.01650\n",
      "[1522]\tvalidation-rmse:0.01650\n",
      "[1523]\tvalidation-rmse:0.01650\n",
      "[1524]\tvalidation-rmse:0.01650\n",
      "[1525]\tvalidation-rmse:0.01650\n",
      "[1526]\tvalidation-rmse:0.01650\n",
      "[1527]\tvalidation-rmse:0.01650\n",
      "[1528]\tvalidation-rmse:0.01650\n",
      "[1529]\tvalidation-rmse:0.01650\n",
      "[1530]\tvalidation-rmse:0.01649\n",
      "[1531]\tvalidation-rmse:0.01649\n",
      "[1532]\tvalidation-rmse:0.01649\n",
      "[1533]\tvalidation-rmse:0.01649\n",
      "[1534]\tvalidation-rmse:0.01649\n",
      "[1535]\tvalidation-rmse:0.01649\n",
      "[1536]\tvalidation-rmse:0.01649\n",
      "[1537]\tvalidation-rmse:0.01649\n",
      "[1538]\tvalidation-rmse:0.01649\n",
      "[1539]\tvalidation-rmse:0.01649\n",
      "[1540]\tvalidation-rmse:0.01649\n",
      "[1541]\tvalidation-rmse:0.01649\n",
      "[1542]\tvalidation-rmse:0.01649\n",
      "[1543]\tvalidation-rmse:0.01649\n",
      "[1544]\tvalidation-rmse:0.01649\n",
      "[1545]\tvalidation-rmse:0.01649\n",
      "[1546]\tvalidation-rmse:0.01649\n",
      "[1547]\tvalidation-rmse:0.01649\n",
      "[1548]\tvalidation-rmse:0.01649\n",
      "[1549]\tvalidation-rmse:0.01649\n",
      "[1550]\tvalidation-rmse:0.01649\n",
      "[1551]\tvalidation-rmse:0.01649\n",
      "[1552]\tvalidation-rmse:0.01649\n",
      "[1553]\tvalidation-rmse:0.01649\n",
      "[1554]\tvalidation-rmse:0.01649\n",
      "[1555]\tvalidation-rmse:0.01649\n",
      "[1556]\tvalidation-rmse:0.01649\n",
      "[1557]\tvalidation-rmse:0.01649\n",
      "[1558]\tvalidation-rmse:0.01649\n",
      "[1559]\tvalidation-rmse:0.01649\n",
      "[1560]\tvalidation-rmse:0.01649\n",
      "[1561]\tvalidation-rmse:0.01649\n",
      "[1562]\tvalidation-rmse:0.01649\n",
      "[1563]\tvalidation-rmse:0.01649\n",
      "[1564]\tvalidation-rmse:0.01649\n",
      "[1565]\tvalidation-rmse:0.01649\n",
      "[1566]\tvalidation-rmse:0.01649\n",
      "[1567]\tvalidation-rmse:0.01648\n",
      "[1568]\tvalidation-rmse:0.01648\n",
      "[1569]\tvalidation-rmse:0.01648\n",
      "[1570]\tvalidation-rmse:0.01648\n",
      "[1571]\tvalidation-rmse:0.01648\n",
      "[1572]\tvalidation-rmse:0.01648\n",
      "[1573]\tvalidation-rmse:0.01649\n",
      "[1574]\tvalidation-rmse:0.01648\n",
      "[1575]\tvalidation-rmse:0.01649\n",
      "[1576]\tvalidation-rmse:0.01649\n",
      "[1577]\tvalidation-rmse:0.01649\n",
      "[1578]\tvalidation-rmse:0.01648\n",
      "[1579]\tvalidation-rmse:0.01649\n",
      "[1580]\tvalidation-rmse:0.01649\n",
      "[1581]\tvalidation-rmse:0.01648\n",
      "[1582]\tvalidation-rmse:0.01648\n",
      "[1583]\tvalidation-rmse:0.01648\n",
      "[1584]\tvalidation-rmse:0.01648\n",
      "[1585]\tvalidation-rmse:0.01648\n",
      "[1586]\tvalidation-rmse:0.01648\n",
      "[1587]\tvalidation-rmse:0.01648\n",
      "[1588]\tvalidation-rmse:0.01648\n",
      "[1589]\tvalidation-rmse:0.01648\n",
      "[1590]\tvalidation-rmse:0.01648\n",
      "[1591]\tvalidation-rmse:0.01648\n",
      "[1592]\tvalidation-rmse:0.01648\n",
      "[1593]\tvalidation-rmse:0.01648\n",
      "[1594]\tvalidation-rmse:0.01648\n",
      "[1595]\tvalidation-rmse:0.01648\n",
      "[1596]\tvalidation-rmse:0.01648\n",
      "[1597]\tvalidation-rmse:0.01648\n",
      "[1598]\tvalidation-rmse:0.01648\n",
      "[1599]\tvalidation-rmse:0.01648\n",
      "[1600]\tvalidation-rmse:0.01648\n",
      "[1601]\tvalidation-rmse:0.01648\n",
      "[1602]\tvalidation-rmse:0.01648\n",
      "[1603]\tvalidation-rmse:0.01648\n",
      "[1604]\tvalidation-rmse:0.01648\n",
      "[1605]\tvalidation-rmse:0.01648\n",
      "[1606]\tvalidation-rmse:0.01648\n",
      "[1607]\tvalidation-rmse:0.01648\n",
      "[1608]\tvalidation-rmse:0.01648\n",
      "[1609]\tvalidation-rmse:0.01648\n",
      "[1610]\tvalidation-rmse:0.01648\n",
      "[1611]\tvalidation-rmse:0.01648\n",
      "[1612]\tvalidation-rmse:0.01648\n",
      "[1613]\tvalidation-rmse:0.01648\n",
      "[1614]\tvalidation-rmse:0.01648\n",
      "[1615]\tvalidation-rmse:0.01647\n",
      "[1616]\tvalidation-rmse:0.01647\n",
      "[1617]\tvalidation-rmse:0.01648\n",
      "[1618]\tvalidation-rmse:0.01648\n",
      "[1619]\tvalidation-rmse:0.01648\n",
      "[1620]\tvalidation-rmse:0.01648\n",
      "[1621]\tvalidation-rmse:0.01648\n",
      "[1622]\tvalidation-rmse:0.01647\n",
      "[1623]\tvalidation-rmse:0.01647\n",
      "[1624]\tvalidation-rmse:0.01648\n",
      "[1625]\tvalidation-rmse:0.01648\n",
      "[1626]\tvalidation-rmse:0.01648\n",
      "[1627]\tvalidation-rmse:0.01648\n",
      "[1628]\tvalidation-rmse:0.01648\n",
      "[1629]\tvalidation-rmse:0.01648\n",
      "[1630]\tvalidation-rmse:0.01648\n",
      "[1631]\tvalidation-rmse:0.01648\n",
      "[1632]\tvalidation-rmse:0.01648\n",
      "[1633]\tvalidation-rmse:0.01648\n",
      "[1634]\tvalidation-rmse:0.01648\n",
      "[1635]\tvalidation-rmse:0.01648\n",
      "[1636]\tvalidation-rmse:0.01648\n",
      "[1637]\tvalidation-rmse:0.01647\n",
      "[1638]\tvalidation-rmse:0.01647\n",
      "[1639]\tvalidation-rmse:0.01647\n",
      "[1640]\tvalidation-rmse:0.01647\n",
      "[1641]\tvalidation-rmse:0.01647\n",
      "[1642]\tvalidation-rmse:0.01647\n",
      "[1643]\tvalidation-rmse:0.01647\n",
      "[1644]\tvalidation-rmse:0.01647\n",
      "[1645]\tvalidation-rmse:0.01647\n",
      "[1646]\tvalidation-rmse:0.01647\n",
      "[1647]\tvalidation-rmse:0.01647\n",
      "[1648]\tvalidation-rmse:0.01647\n",
      "[1649]\tvalidation-rmse:0.01647\n",
      "[1650]\tvalidation-rmse:0.01647\n",
      "[1651]\tvalidation-rmse:0.01647\n",
      "[1652]\tvalidation-rmse:0.01647\n",
      "[1653]\tvalidation-rmse:0.01647\n",
      "[1654]\tvalidation-rmse:0.01647\n",
      "[1655]\tvalidation-rmse:0.01647\n",
      "[1656]\tvalidation-rmse:0.01647\n",
      "[1657]\tvalidation-rmse:0.01647\n",
      "[1658]\tvalidation-rmse:0.01647\n",
      "[1659]\tvalidation-rmse:0.01647\n",
      "[1660]\tvalidation-rmse:0.01648\n",
      "[1661]\tvalidation-rmse:0.01648\n",
      "[1662]\tvalidation-rmse:0.01648\n",
      "[1663]\tvalidation-rmse:0.01648\n",
      "[1664]\tvalidation-rmse:0.01648\n",
      "[1665]\tvalidation-rmse:0.01648\n",
      "[1666]\tvalidation-rmse:0.01648\n",
      "[1667]\tvalidation-rmse:0.01648\n",
      "[1668]\tvalidation-rmse:0.01648\n",
      "[1669]\tvalidation-rmse:0.01648\n",
      "[1670]\tvalidation-rmse:0.01648\n",
      "[1671]\tvalidation-rmse:0.01648\n",
      "[1672]\tvalidation-rmse:0.01648\n",
      "[1673]\tvalidation-rmse:0.01648\n",
      "[1674]\tvalidation-rmse:0.01648\n",
      "[1675]\tvalidation-rmse:0.01648\n",
      "[1676]\tvalidation-rmse:0.01648\n",
      "[1677]\tvalidation-rmse:0.01648\n",
      "[1678]\tvalidation-rmse:0.01648\n",
      "[1679]\tvalidation-rmse:0.01648\n",
      "[1680]\tvalidation-rmse:0.01648\n",
      "[1681]\tvalidation-rmse:0.01648\n",
      "[1682]\tvalidation-rmse:0.01647\n",
      "[1683]\tvalidation-rmse:0.01647\n",
      "[1684]\tvalidation-rmse:0.01647\n",
      "[1685]\tvalidation-rmse:0.01647\n",
      "[1686]\tvalidation-rmse:0.01648\n",
      "[1687]\tvalidation-rmse:0.01648\n",
      "[1688]\tvalidation-rmse:0.01648\n",
      "[1689]\tvalidation-rmse:0.01647\n",
      "[1690]\tvalidation-rmse:0.01647\n",
      "[1691]\tvalidation-rmse:0.01647\n",
      "[1692]\tvalidation-rmse:0.01647\n",
      "[1693]\tvalidation-rmse:0.01647\n",
      "[1694]\tvalidation-rmse:0.01647\n",
      "[1695]\tvalidation-rmse:0.01647\n",
      "[1696]\tvalidation-rmse:0.01647\n",
      "[1697]\tvalidation-rmse:0.01648\n",
      "[1698]\tvalidation-rmse:0.01648\n",
      "[1699]\tvalidation-rmse:0.01648\n",
      "[1700]\tvalidation-rmse:0.01648\n",
      "[1701]\tvalidation-rmse:0.01648\n",
      "[1702]\tvalidation-rmse:0.01648\n",
      "[1703]\tvalidation-rmse:0.01648\n",
      "[1704]\tvalidation-rmse:0.01648\n",
      "[1705]\tvalidation-rmse:0.01648\n",
      "[1706]\tvalidation-rmse:0.01648\n",
      "[1707]\tvalidation-rmse:0.01648\n",
      "[1708]\tvalidation-rmse:0.01648\n",
      "[1709]\tvalidation-rmse:0.01648\n",
      "[1710]\tvalidation-rmse:0.01647\n",
      "[1711]\tvalidation-rmse:0.01647\n",
      "[1712]\tvalidation-rmse:0.01647\n",
      "[1713]\tvalidation-rmse:0.01647\n",
      "[1714]\tvalidation-rmse:0.01647\n",
      "[1715]\tvalidation-rmse:0.01647\n",
      "[1716]\tvalidation-rmse:0.01647\n",
      "[1717]\tvalidation-rmse:0.01647\n",
      "[1718]\tvalidation-rmse:0.01647\n",
      "[1719]\tvalidation-rmse:0.01647\n",
      "[1720]\tvalidation-rmse:0.01647\n",
      "[1721]\tvalidation-rmse:0.01647\n",
      "[1722]\tvalidation-rmse:0.01647\n",
      "[1723]\tvalidation-rmse:0.01647\n",
      "[1724]\tvalidation-rmse:0.01647\n",
      "[1725]\tvalidation-rmse:0.01647\n",
      "[1726]\tvalidation-rmse:0.01647\n",
      "[1727]\tvalidation-rmse:0.01647\n",
      "[1728]\tvalidation-rmse:0.01647\n",
      "[1729]\tvalidation-rmse:0.01647\n",
      "[1730]\tvalidation-rmse:0.01647\n",
      "[1731]\tvalidation-rmse:0.01647\n",
      "[1732]\tvalidation-rmse:0.01647\n",
      "[1733]\tvalidation-rmse:0.01647\n",
      "[1734]\tvalidation-rmse:0.01647\n",
      "[1735]\tvalidation-rmse:0.01647\n",
      "[1736]\tvalidation-rmse:0.01647\n",
      "[1737]\tvalidation-rmse:0.01647\n",
      "[1738]\tvalidation-rmse:0.01647\n",
      "[1739]\tvalidation-rmse:0.01647\n",
      "[1740]\tvalidation-rmse:0.01647\n",
      "[1741]\tvalidation-rmse:0.01647\n",
      "[1742]\tvalidation-rmse:0.01647\n",
      "[1743]\tvalidation-rmse:0.01647\n",
      "[1744]\tvalidation-rmse:0.01647\n",
      "[1745]\tvalidation-rmse:0.01647\n",
      "[1746]\tvalidation-rmse:0.01647\n",
      "[1747]\tvalidation-rmse:0.01647\n",
      "[1748]\tvalidation-rmse:0.01647\n",
      "[1749]\tvalidation-rmse:0.01647\n",
      "[1750]\tvalidation-rmse:0.01647\n",
      "[1751]\tvalidation-rmse:0.01647\n",
      "[1752]\tvalidation-rmse:0.01647\n",
      "[1753]\tvalidation-rmse:0.01647\n",
      "[1754]\tvalidation-rmse:0.01647\n",
      "[1755]\tvalidation-rmse:0.01647\n",
      "[1756]\tvalidation-rmse:0.01647\n",
      "[1757]\tvalidation-rmse:0.01647\n",
      "[1758]\tvalidation-rmse:0.01647\n",
      "[1759]\tvalidation-rmse:0.01647\n",
      "[1760]\tvalidation-rmse:0.01647\n",
      "[1761]\tvalidation-rmse:0.01647\n",
      "[1762]\tvalidation-rmse:0.01647\n",
      "[1763]\tvalidation-rmse:0.01647\n",
      "[1764]\tvalidation-rmse:0.01647\n",
      "[1765]\tvalidation-rmse:0.01647\n",
      "[1766]\tvalidation-rmse:0.01647\n",
      "[1767]\tvalidation-rmse:0.01647\n",
      "[1768]\tvalidation-rmse:0.01647\n",
      "[1769]\tvalidation-rmse:0.01647\n",
      "[1770]\tvalidation-rmse:0.01647\n",
      "[1771]\tvalidation-rmse:0.01647\n",
      "[1772]\tvalidation-rmse:0.01647\n",
      "[1773]\tvalidation-rmse:0.01647\n",
      "[1774]\tvalidation-rmse:0.01647\n",
      "[1775]\tvalidation-rmse:0.01647\n",
      "[1776]\tvalidation-rmse:0.01647\n",
      "[1777]\tvalidation-rmse:0.01647\n",
      "[1778]\tvalidation-rmse:0.01647\n",
      "[1779]\tvalidation-rmse:0.01647\n",
      "[1780]\tvalidation-rmse:0.01647\n",
      "[1781]\tvalidation-rmse:0.01647\n",
      "[1782]\tvalidation-rmse:0.01647\n",
      "[1783]\tvalidation-rmse:0.01647\n",
      "[1784]\tvalidation-rmse:0.01647\n",
      "[1785]\tvalidation-rmse:0.01646\n",
      "[1786]\tvalidation-rmse:0.01647\n",
      "[1787]\tvalidation-rmse:0.01647\n",
      "[1788]\tvalidation-rmse:0.01647\n",
      "[1789]\tvalidation-rmse:0.01647\n",
      "[1790]\tvalidation-rmse:0.01647\n",
      "[1791]\tvalidation-rmse:0.01647\n",
      "[1792]\tvalidation-rmse:0.01647\n",
      "[1793]\tvalidation-rmse:0.01647\n",
      "[1794]\tvalidation-rmse:0.01647\n",
      "[1795]\tvalidation-rmse:0.01647\n",
      "[1796]\tvalidation-rmse:0.01647\n",
      "[1797]\tvalidation-rmse:0.01647\n",
      "[1798]\tvalidation-rmse:0.01647\n",
      "[1799]\tvalidation-rmse:0.01647\n",
      "[1800]\tvalidation-rmse:0.01647\n",
      "[1801]\tvalidation-rmse:0.01647\n",
      "[1802]\tvalidation-rmse:0.01647\n",
      "[1803]\tvalidation-rmse:0.01647\n",
      "[1804]\tvalidation-rmse:0.01647\n",
      "[1805]\tvalidation-rmse:0.01647\n",
      "[1806]\tvalidation-rmse:0.01647\n",
      "[1807]\tvalidation-rmse:0.01646\n",
      "[1808]\tvalidation-rmse:0.01646\n",
      "[1809]\tvalidation-rmse:0.01646\n",
      "[1810]\tvalidation-rmse:0.01646\n",
      "[1811]\tvalidation-rmse:0.01646\n",
      "[1812]\tvalidation-rmse:0.01646\n",
      "[1813]\tvalidation-rmse:0.01646\n",
      "[1814]\tvalidation-rmse:0.01646\n",
      "[1815]\tvalidation-rmse:0.01646\n",
      "[1816]\tvalidation-rmse:0.01646\n",
      "[1817]\tvalidation-rmse:0.01646\n",
      "[1818]\tvalidation-rmse:0.01646\n",
      "[1819]\tvalidation-rmse:0.01646\n",
      "[1820]\tvalidation-rmse:0.01646\n",
      "[1821]\tvalidation-rmse:0.01646\n",
      "[1822]\tvalidation-rmse:0.01646\n",
      "[1823]\tvalidation-rmse:0.01646\n",
      "[1824]\tvalidation-rmse:0.01646\n",
      "[1825]\tvalidation-rmse:0.01646\n",
      "[1826]\tvalidation-rmse:0.01646\n",
      "[1827]\tvalidation-rmse:0.01646\n",
      "[1828]\tvalidation-rmse:0.01646\n",
      "[1829]\tvalidation-rmse:0.01646\n",
      "[1830]\tvalidation-rmse:0.01646\n",
      "[1831]\tvalidation-rmse:0.01646\n",
      "[1832]\tvalidation-rmse:0.01646\n",
      "[1833]\tvalidation-rmse:0.01646\n",
      "[1834]\tvalidation-rmse:0.01646\n",
      "[1835]\tvalidation-rmse:0.01646\n",
      "[1836]\tvalidation-rmse:0.01646\n",
      "[1837]\tvalidation-rmse:0.01646\n",
      "[1838]\tvalidation-rmse:0.01646\n",
      "[1839]\tvalidation-rmse:0.01646\n",
      "[1840]\tvalidation-rmse:0.01646\n",
      "[1841]\tvalidation-rmse:0.01646\n",
      "[1842]\tvalidation-rmse:0.01646\n",
      "[1843]\tvalidation-rmse:0.01646\n",
      "[1844]\tvalidation-rmse:0.01646\n",
      "[1845]\tvalidation-rmse:0.01646\n",
      "[1846]\tvalidation-rmse:0.01646\n",
      "[1847]\tvalidation-rmse:0.01646\n",
      "[1848]\tvalidation-rmse:0.01646\n",
      "[1849]\tvalidation-rmse:0.01646\n",
      "[1850]\tvalidation-rmse:0.01645\n",
      "[1851]\tvalidation-rmse:0.01645\n",
      "[1852]\tvalidation-rmse:0.01645\n",
      "[1853]\tvalidation-rmse:0.01645\n",
      "[1854]\tvalidation-rmse:0.01645\n",
      "[1855]\tvalidation-rmse:0.01645\n",
      "[1856]\tvalidation-rmse:0.01645\n",
      "[1857]\tvalidation-rmse:0.01645\n",
      "[1858]\tvalidation-rmse:0.01645\n",
      "[1859]\tvalidation-rmse:0.01645\n",
      "[1860]\tvalidation-rmse:0.01645\n",
      "[1861]\tvalidation-rmse:0.01645\n",
      "[1862]\tvalidation-rmse:0.01645\n",
      "[1863]\tvalidation-rmse:0.01645\n",
      "[1864]\tvalidation-rmse:0.01645\n",
      "[1865]\tvalidation-rmse:0.01645\n",
      "[1866]\tvalidation-rmse:0.01645\n",
      "[1867]\tvalidation-rmse:0.01645\n",
      "[1868]\tvalidation-rmse:0.01645\n",
      "[1869]\tvalidation-rmse:0.01645\n",
      "[1870]\tvalidation-rmse:0.01645\n",
      "[1871]\tvalidation-rmse:0.01645\n",
      "[1872]\tvalidation-rmse:0.01645\n",
      "[1873]\tvalidation-rmse:0.01645\n",
      "[1874]\tvalidation-rmse:0.01645\n",
      "[1875]\tvalidation-rmse:0.01645\n",
      "[1876]\tvalidation-rmse:0.01645\n",
      "[1877]\tvalidation-rmse:0.01645\n",
      "[1878]\tvalidation-rmse:0.01645\n",
      "[1879]\tvalidation-rmse:0.01645\n",
      "[1880]\tvalidation-rmse:0.01645\n",
      "[1881]\tvalidation-rmse:0.01645\n",
      "[1882]\tvalidation-rmse:0.01645\n",
      "[1883]\tvalidation-rmse:0.01645\n",
      "[1884]\tvalidation-rmse:0.01645\n",
      "[1885]\tvalidation-rmse:0.01645\n",
      "[1886]\tvalidation-rmse:0.01645\n",
      "[1887]\tvalidation-rmse:0.01645\n",
      "[1888]\tvalidation-rmse:0.01645\n",
      "[1889]\tvalidation-rmse:0.01645\n",
      "[1890]\tvalidation-rmse:0.01645\n",
      "[1891]\tvalidation-rmse:0.01645\n",
      "[1892]\tvalidation-rmse:0.01645\n",
      "[1893]\tvalidation-rmse:0.01645\n",
      "[1894]\tvalidation-rmse:0.01645\n",
      "[1895]\tvalidation-rmse:0.01645\n",
      "[1896]\tvalidation-rmse:0.01645\n",
      "[1897]\tvalidation-rmse:0.01645\n",
      "[1898]\tvalidation-rmse:0.01645\n",
      "[1899]\tvalidation-rmse:0.01645\n",
      "[1900]\tvalidation-rmse:0.01645\n",
      "[1901]\tvalidation-rmse:0.01645\n",
      "[1902]\tvalidation-rmse:0.01645\n",
      "[1903]\tvalidation-rmse:0.01645\n",
      "[1904]\tvalidation-rmse:0.01645\n",
      "[1905]\tvalidation-rmse:0.01645\n",
      "[1906]\tvalidation-rmse:0.01645\n",
      "[1907]\tvalidation-rmse:0.01645\n",
      "[1908]\tvalidation-rmse:0.01645\n",
      "[1909]\tvalidation-rmse:0.01645\n",
      "[1910]\tvalidation-rmse:0.01645\n",
      "[1911]\tvalidation-rmse:0.01645\n",
      "[1912]\tvalidation-rmse:0.01645\n",
      "[1913]\tvalidation-rmse:0.01645\n",
      "[1914]\tvalidation-rmse:0.01645\n",
      "[1915]\tvalidation-rmse:0.01645\n",
      "[1916]\tvalidation-rmse:0.01644\n",
      "[1917]\tvalidation-rmse:0.01644\n",
      "[1918]\tvalidation-rmse:0.01645\n",
      "[1919]\tvalidation-rmse:0.01644\n",
      "[1920]\tvalidation-rmse:0.01644\n",
      "[1921]\tvalidation-rmse:0.01644\n",
      "[1922]\tvalidation-rmse:0.01644\n",
      "[1923]\tvalidation-rmse:0.01644\n",
      "[1924]\tvalidation-rmse:0.01644\n",
      "[1925]\tvalidation-rmse:0.01644\n",
      "[1926]\tvalidation-rmse:0.01644\n",
      "[1927]\tvalidation-rmse:0.01644\n",
      "[1928]\tvalidation-rmse:0.01644\n",
      "[1929]\tvalidation-rmse:0.01644\n",
      "[1930]\tvalidation-rmse:0.01644\n",
      "[1931]\tvalidation-rmse:0.01644\n",
      "[1932]\tvalidation-rmse:0.01644\n",
      "[1933]\tvalidation-rmse:0.01644\n",
      "[1934]\tvalidation-rmse:0.01644\n",
      "[1935]\tvalidation-rmse:0.01644\n",
      "[1936]\tvalidation-rmse:0.01644\n",
      "[1937]\tvalidation-rmse:0.01644\n",
      "[1938]\tvalidation-rmse:0.01644\n",
      "[1939]\tvalidation-rmse:0.01644\n",
      "[1940]\tvalidation-rmse:0.01644\n",
      "[1941]\tvalidation-rmse:0.01644\n",
      "[1942]\tvalidation-rmse:0.01644\n",
      "[1943]\tvalidation-rmse:0.01644\n",
      "[1944]\tvalidation-rmse:0.01644\n",
      "[1945]\tvalidation-rmse:0.01644\n",
      "[1946]\tvalidation-rmse:0.01644\n",
      "[1947]\tvalidation-rmse:0.01644\n",
      "[1948]\tvalidation-rmse:0.01644\n",
      "[1949]\tvalidation-rmse:0.01644\n",
      "[1950]\tvalidation-rmse:0.01644\n",
      "[1951]\tvalidation-rmse:0.01644\n",
      "[1952]\tvalidation-rmse:0.01644\n",
      "[1953]\tvalidation-rmse:0.01644\n",
      "[1954]\tvalidation-rmse:0.01644\n",
      "[1955]\tvalidation-rmse:0.01644\n",
      "[1956]\tvalidation-rmse:0.01644\n",
      "[1957]\tvalidation-rmse:0.01644\n",
      "[1958]\tvalidation-rmse:0.01644\n",
      "[1959]\tvalidation-rmse:0.01644\n",
      "[1960]\tvalidation-rmse:0.01644\n",
      "[1961]\tvalidation-rmse:0.01644\n",
      "[1962]\tvalidation-rmse:0.01644\n",
      "[1963]\tvalidation-rmse:0.01644\n",
      "[1964]\tvalidation-rmse:0.01644\n",
      "[1965]\tvalidation-rmse:0.01644\n",
      "[1966]\tvalidation-rmse:0.01643\n",
      "[1967]\tvalidation-rmse:0.01643\n",
      "[1968]\tvalidation-rmse:0.01643\n",
      "[1969]\tvalidation-rmse:0.01643\n",
      "[1970]\tvalidation-rmse:0.01643\n",
      "[1971]\tvalidation-rmse:0.01643\n",
      "[1972]\tvalidation-rmse:0.01643\n",
      "[1973]\tvalidation-rmse:0.01643\n",
      "[1974]\tvalidation-rmse:0.01643\n",
      "[1975]\tvalidation-rmse:0.01643\n",
      "[1976]\tvalidation-rmse:0.01643\n",
      "[1977]\tvalidation-rmse:0.01643\n",
      "[1978]\tvalidation-rmse:0.01643\n",
      "[1979]\tvalidation-rmse:0.01643\n",
      "[1980]\tvalidation-rmse:0.01643\n",
      "[1981]\tvalidation-rmse:0.01643\n",
      "[1982]\tvalidation-rmse:0.01643\n",
      "[1983]\tvalidation-rmse:0.01643\n",
      "[1984]\tvalidation-rmse:0.01643\n",
      "[1985]\tvalidation-rmse:0.01643\n",
      "[1986]\tvalidation-rmse:0.01643\n",
      "[1987]\tvalidation-rmse:0.01643\n",
      "[1988]\tvalidation-rmse:0.01643\n",
      "[1989]\tvalidation-rmse:0.01643\n",
      "[1990]\tvalidation-rmse:0.01643\n",
      "[1991]\tvalidation-rmse:0.01643\n",
      "[1992]\tvalidation-rmse:0.01643\n",
      "[1993]\tvalidation-rmse:0.01643\n",
      "[1994]\tvalidation-rmse:0.01643\n",
      "[1995]\tvalidation-rmse:0.01643\n",
      "[1996]\tvalidation-rmse:0.01643\n",
      "[1997]\tvalidation-rmse:0.01643\n",
      "[1998]\tvalidation-rmse:0.01643\n",
      "[1999]\tvalidation-rmse:0.01643\n",
      "[2000]\tvalidation-rmse:0.01643\n",
      "[2001]\tvalidation-rmse:0.01643\n",
      "[2002]\tvalidation-rmse:0.01643\n",
      "[2003]\tvalidation-rmse:0.01643\n",
      "[2004]\tvalidation-rmse:0.01643\n",
      "[2005]\tvalidation-rmse:0.01643\n",
      "[2006]\tvalidation-rmse:0.01643\n",
      "[2007]\tvalidation-rmse:0.01643\n",
      "[2008]\tvalidation-rmse:0.01643\n",
      "[2009]\tvalidation-rmse:0.01643\n",
      "[2010]\tvalidation-rmse:0.01643\n",
      "[2011]\tvalidation-rmse:0.01643\n",
      "[2012]\tvalidation-rmse:0.01643\n",
      "[2013]\tvalidation-rmse:0.01643\n",
      "[2014]\tvalidation-rmse:0.01643\n",
      "[2015]\tvalidation-rmse:0.01643\n",
      "[2016]\tvalidation-rmse:0.01643\n",
      "[2017]\tvalidation-rmse:0.01643\n",
      "[2018]\tvalidation-rmse:0.01643\n",
      "[2019]\tvalidation-rmse:0.01643\n",
      "[2020]\tvalidation-rmse:0.01643\n",
      "[2021]\tvalidation-rmse:0.01643\n",
      "[2022]\tvalidation-rmse:0.01643\n",
      "[2023]\tvalidation-rmse:0.01643\n",
      "[2024]\tvalidation-rmse:0.01643\n",
      "[2025]\tvalidation-rmse:0.01643\n",
      "[2026]\tvalidation-rmse:0.01643\n",
      "[2027]\tvalidation-rmse:0.01643\n",
      "[2028]\tvalidation-rmse:0.01643\n",
      "[2029]\tvalidation-rmse:0.01643\n",
      "[2030]\tvalidation-rmse:0.01643\n",
      "[2031]\tvalidation-rmse:0.01643\n",
      "[2032]\tvalidation-rmse:0.01643\n",
      "[2033]\tvalidation-rmse:0.01643\n",
      "[2034]\tvalidation-rmse:0.01643\n",
      "[2035]\tvalidation-rmse:0.01643\n",
      "[2036]\tvalidation-rmse:0.01643\n",
      "[2037]\tvalidation-rmse:0.01643\n",
      "[2038]\tvalidation-rmse:0.01643\n",
      "[2039]\tvalidation-rmse:0.01643\n",
      "[2040]\tvalidation-rmse:0.01643\n",
      "[2041]\tvalidation-rmse:0.01643\n",
      "[2042]\tvalidation-rmse:0.01643\n",
      "[2043]\tvalidation-rmse:0.01643\n",
      "[2044]\tvalidation-rmse:0.01643\n",
      "[2045]\tvalidation-rmse:0.01643\n",
      "[2046]\tvalidation-rmse:0.01643\n",
      "[2047]\tvalidation-rmse:0.01643\n",
      "[2048]\tvalidation-rmse:0.01643\n",
      "[2049]\tvalidation-rmse:0.01643\n",
      "[2050]\tvalidation-rmse:0.01642\n",
      "[2051]\tvalidation-rmse:0.01642\n",
      "[2052]\tvalidation-rmse:0.01642\n",
      "[2053]\tvalidation-rmse:0.01642\n",
      "[2054]\tvalidation-rmse:0.01642\n",
      "[2055]\tvalidation-rmse:0.01642\n",
      "[2056]\tvalidation-rmse:0.01642\n",
      "[2057]\tvalidation-rmse:0.01642\n",
      "[2058]\tvalidation-rmse:0.01642\n",
      "[2059]\tvalidation-rmse:0.01643\n",
      "[2060]\tvalidation-rmse:0.01643\n",
      "[2061]\tvalidation-rmse:0.01643\n",
      "[2062]\tvalidation-rmse:0.01643\n",
      "[2063]\tvalidation-rmse:0.01643\n",
      "[2064]\tvalidation-rmse:0.01643\n",
      "[2065]\tvalidation-rmse:0.01643\n",
      "[2066]\tvalidation-rmse:0.01642\n",
      "[2067]\tvalidation-rmse:0.01642\n",
      "[2068]\tvalidation-rmse:0.01642\n",
      "[2069]\tvalidation-rmse:0.01643\n",
      "[2070]\tvalidation-rmse:0.01643\n",
      "[2071]\tvalidation-rmse:0.01642\n",
      "[2072]\tvalidation-rmse:0.01642\n",
      "[2073]\tvalidation-rmse:0.01642\n",
      "[2074]\tvalidation-rmse:0.01642\n",
      "[2075]\tvalidation-rmse:0.01642\n",
      "[2076]\tvalidation-rmse:0.01642\n",
      "[2077]\tvalidation-rmse:0.01642\n",
      "[2078]\tvalidation-rmse:0.01642\n",
      "[2079]\tvalidation-rmse:0.01642\n",
      "[2080]\tvalidation-rmse:0.01642\n",
      "[2081]\tvalidation-rmse:0.01642\n",
      "[2082]\tvalidation-rmse:0.01642\n",
      "[2083]\tvalidation-rmse:0.01642\n",
      "[2084]\tvalidation-rmse:0.01642\n",
      "[2085]\tvalidation-rmse:0.01642\n",
      "[2086]\tvalidation-rmse:0.01642\n",
      "[2087]\tvalidation-rmse:0.01642\n",
      "[2088]\tvalidation-rmse:0.01642\n",
      "[2089]\tvalidation-rmse:0.01642\n",
      "[2090]\tvalidation-rmse:0.01642\n",
      "[2091]\tvalidation-rmse:0.01642\n",
      "[2092]\tvalidation-rmse:0.01642\n",
      "[2093]\tvalidation-rmse:0.01642\n",
      "[2094]\tvalidation-rmse:0.01642\n",
      "[2095]\tvalidation-rmse:0.01642\n",
      "[2096]\tvalidation-rmse:0.01642\n",
      "[2097]\tvalidation-rmse:0.01642\n",
      "[2098]\tvalidation-rmse:0.01642\n",
      "[2099]\tvalidation-rmse:0.01642\n",
      "[2100]\tvalidation-rmse:0.01642\n",
      "[2101]\tvalidation-rmse:0.01642\n",
      "[2102]\tvalidation-rmse:0.01642\n",
      "[2103]\tvalidation-rmse:0.01642\n",
      "[2104]\tvalidation-rmse:0.01642\n",
      "[2105]\tvalidation-rmse:0.01642\n",
      "[2106]\tvalidation-rmse:0.01642\n",
      "[2107]\tvalidation-rmse:0.01642\n",
      "[2108]\tvalidation-rmse:0.01642\n",
      "[2109]\tvalidation-rmse:0.01642\n",
      "[2110]\tvalidation-rmse:0.01642\n",
      "[2111]\tvalidation-rmse:0.01642\n",
      "[2112]\tvalidation-rmse:0.01641\n",
      "[2113]\tvalidation-rmse:0.01641\n",
      "[2114]\tvalidation-rmse:0.01641\n",
      "[2115]\tvalidation-rmse:0.01641\n",
      "[2116]\tvalidation-rmse:0.01641\n",
      "[2117]\tvalidation-rmse:0.01641\n",
      "[2118]\tvalidation-rmse:0.01641\n",
      "[2119]\tvalidation-rmse:0.01641\n",
      "[2120]\tvalidation-rmse:0.01641\n",
      "[2121]\tvalidation-rmse:0.01641\n",
      "[2122]\tvalidation-rmse:0.01641\n",
      "[2123]\tvalidation-rmse:0.01641\n",
      "[2124]\tvalidation-rmse:0.01641\n",
      "[2125]\tvalidation-rmse:0.01641\n",
      "[2126]\tvalidation-rmse:0.01641\n",
      "[2127]\tvalidation-rmse:0.01641\n",
      "[2128]\tvalidation-rmse:0.01641\n",
      "[2129]\tvalidation-rmse:0.01641\n",
      "[2130]\tvalidation-rmse:0.01641\n",
      "[2131]\tvalidation-rmse:0.01641\n",
      "[2132]\tvalidation-rmse:0.01641\n",
      "[2133]\tvalidation-rmse:0.01641\n",
      "[2134]\tvalidation-rmse:0.01641\n",
      "[2135]\tvalidation-rmse:0.01641\n",
      "[2136]\tvalidation-rmse:0.01641\n",
      "[2137]\tvalidation-rmse:0.01641\n",
      "[2138]\tvalidation-rmse:0.01641\n",
      "[2139]\tvalidation-rmse:0.01641\n",
      "[2140]\tvalidation-rmse:0.01641\n",
      "[2141]\tvalidation-rmse:0.01641\n",
      "[2142]\tvalidation-rmse:0.01641\n",
      "[2143]\tvalidation-rmse:0.01641\n",
      "[2144]\tvalidation-rmse:0.01641\n",
      "[2145]\tvalidation-rmse:0.01641\n",
      "[2146]\tvalidation-rmse:0.01641\n",
      "[2147]\tvalidation-rmse:0.01641\n",
      "[2148]\tvalidation-rmse:0.01641\n",
      "[2149]\tvalidation-rmse:0.01641\n",
      "[2150]\tvalidation-rmse:0.01641\n",
      "[2151]\tvalidation-rmse:0.01641\n",
      "[2152]\tvalidation-rmse:0.01641\n",
      "[2153]\tvalidation-rmse:0.01641\n",
      "[2154]\tvalidation-rmse:0.01641\n",
      "[2155]\tvalidation-rmse:0.01641\n",
      "[2156]\tvalidation-rmse:0.01641\n",
      "[2157]\tvalidation-rmse:0.01641\n",
      "[2158]\tvalidation-rmse:0.01641\n",
      "[2159]\tvalidation-rmse:0.01641\n",
      "[2160]\tvalidation-rmse:0.01641\n",
      "[2161]\tvalidation-rmse:0.01641\n",
      "[2162]\tvalidation-rmse:0.01641\n",
      "[2163]\tvalidation-rmse:0.01641\n",
      "[2164]\tvalidation-rmse:0.01641\n",
      "[2165]\tvalidation-rmse:0.01641\n",
      "[2166]\tvalidation-rmse:0.01641\n",
      "[2167]\tvalidation-rmse:0.01641\n",
      "[2168]\tvalidation-rmse:0.01641\n",
      "[2169]\tvalidation-rmse:0.01641\n",
      "[2170]\tvalidation-rmse:0.01641\n",
      "[2171]\tvalidation-rmse:0.01641\n",
      "[2172]\tvalidation-rmse:0.01641\n",
      "[2173]\tvalidation-rmse:0.01641\n",
      "[2174]\tvalidation-rmse:0.01641\n",
      "[2175]\tvalidation-rmse:0.01641\n",
      "[2176]\tvalidation-rmse:0.01641\n",
      "[2177]\tvalidation-rmse:0.01641\n",
      "[2178]\tvalidation-rmse:0.01641\n",
      "[2179]\tvalidation-rmse:0.01641\n",
      "[2180]\tvalidation-rmse:0.01641\n",
      "[2181]\tvalidation-rmse:0.01641\n",
      "[2182]\tvalidation-rmse:0.01641\n",
      "[2183]\tvalidation-rmse:0.01641\n",
      "[2184]\tvalidation-rmse:0.01641\n",
      "[2185]\tvalidation-rmse:0.01641\n",
      "[2186]\tvalidation-rmse:0.01641\n",
      "[2187]\tvalidation-rmse:0.01641\n",
      "[2188]\tvalidation-rmse:0.01641\n",
      "[2189]\tvalidation-rmse:0.01641\n",
      "[2190]\tvalidation-rmse:0.01641\n",
      "[2191]\tvalidation-rmse:0.01641\n",
      "[2192]\tvalidation-rmse:0.01641\n",
      "[2193]\tvalidation-rmse:0.01641\n",
      "[2194]\tvalidation-rmse:0.01641\n",
      "[2195]\tvalidation-rmse:0.01641\n",
      "[2196]\tvalidation-rmse:0.01641\n",
      "[2197]\tvalidation-rmse:0.01641\n",
      "[2198]\tvalidation-rmse:0.01641\n",
      "[2199]\tvalidation-rmse:0.01641\n",
      "[2200]\tvalidation-rmse:0.01641\n",
      "[2201]\tvalidation-rmse:0.01641\n",
      "[2202]\tvalidation-rmse:0.01641\n",
      "[2203]\tvalidation-rmse:0.01641\n",
      "[2204]\tvalidation-rmse:0.01641\n",
      "[2205]\tvalidation-rmse:0.01641\n",
      "[2206]\tvalidation-rmse:0.01641\n",
      "[2207]\tvalidation-rmse:0.01641\n",
      "[2208]\tvalidation-rmse:0.01641\n",
      "[2209]\tvalidation-rmse:0.01641\n",
      "[2210]\tvalidation-rmse:0.01641\n",
      "[2211]\tvalidation-rmse:0.01641\n",
      "[2212]\tvalidation-rmse:0.01641\n",
      "[2213]\tvalidation-rmse:0.01641\n",
      "[2214]\tvalidation-rmse:0.01641\n",
      "[2215]\tvalidation-rmse:0.01641\n",
      "[2216]\tvalidation-rmse:0.01641\n",
      "[2217]\tvalidation-rmse:0.01641\n",
      "[2218]\tvalidation-rmse:0.01641\n",
      "[2219]\tvalidation-rmse:0.01641\n",
      "[2220]\tvalidation-rmse:0.01641\n",
      "[2221]\tvalidation-rmse:0.01641\n",
      "[2222]\tvalidation-rmse:0.01641\n",
      "[2223]\tvalidation-rmse:0.01641\n",
      "[2224]\tvalidation-rmse:0.01641\n",
      "[2225]\tvalidation-rmse:0.01641\n",
      "[2226]\tvalidation-rmse:0.01641\n",
      "[2227]\tvalidation-rmse:0.01641\n",
      "[2228]\tvalidation-rmse:0.01641\n",
      "[2229]\tvalidation-rmse:0.01641\n",
      "[2230]\tvalidation-rmse:0.01641\n",
      "[2231]\tvalidation-rmse:0.01641\n",
      "[2232]\tvalidation-rmse:0.01641\n",
      "[2233]\tvalidation-rmse:0.01641\n",
      "[2234]\tvalidation-rmse:0.01641\n",
      "[2235]\tvalidation-rmse:0.01641\n",
      "[2236]\tvalidation-rmse:0.01641\n",
      "[2237]\tvalidation-rmse:0.01641\n",
      "[2238]\tvalidation-rmse:0.01641\n",
      "[2239]\tvalidation-rmse:0.01641\n",
      "[2240]\tvalidation-rmse:0.01641\n",
      "[2241]\tvalidation-rmse:0.01641\n",
      "[2242]\tvalidation-rmse:0.01641\n",
      "[2243]\tvalidation-rmse:0.01641\n",
      "[2244]\tvalidation-rmse:0.01641\n",
      "[2245]\tvalidation-rmse:0.01641\n",
      "[2246]\tvalidation-rmse:0.01641\n",
      "[2247]\tvalidation-rmse:0.01641\n",
      "[2248]\tvalidation-rmse:0.01641\n",
      "[2249]\tvalidation-rmse:0.01641\n",
      "[2250]\tvalidation-rmse:0.01641\n",
      "[2251]\tvalidation-rmse:0.01641\n",
      "[2252]\tvalidation-rmse:0.01641\n",
      "[2253]\tvalidation-rmse:0.01641\n",
      "[2254]\tvalidation-rmse:0.01641\n",
      "[2255]\tvalidation-rmse:0.01641\n",
      "[2256]\tvalidation-rmse:0.01641\n",
      "[2257]\tvalidation-rmse:0.01641\n",
      "[2258]\tvalidation-rmse:0.01641\n",
      "[2259]\tvalidation-rmse:0.01641\n",
      "[2260]\tvalidation-rmse:0.01641\n",
      "[2261]\tvalidation-rmse:0.01641\n",
      "[2262]\tvalidation-rmse:0.01641\n",
      "[2263]\tvalidation-rmse:0.01641\n",
      "[2264]\tvalidation-rmse:0.01641\n",
      "[2265]\tvalidation-rmse:0.01641\n",
      "[2266]\tvalidation-rmse:0.01641\n",
      "[2267]\tvalidation-rmse:0.01641\n",
      "[2268]\tvalidation-rmse:0.01641\n",
      "[2269]\tvalidation-rmse:0.01641\n",
      "[2270]\tvalidation-rmse:0.01641\n",
      "[2271]\tvalidation-rmse:0.01641\n",
      "[2272]\tvalidation-rmse:0.01641\n",
      "[2273]\tvalidation-rmse:0.01641\n",
      "[2274]\tvalidation-rmse:0.01641\n",
      "[2275]\tvalidation-rmse:0.01641\n",
      "[2276]\tvalidation-rmse:0.01641\n",
      "[2277]\tvalidation-rmse:0.01641\n",
      "[2278]\tvalidation-rmse:0.01641\n",
      "[2279]\tvalidation-rmse:0.01641\n",
      "[2280]\tvalidation-rmse:0.01641\n",
      "[2281]\tvalidation-rmse:0.01641\n",
      "[2282]\tvalidation-rmse:0.01641\n",
      "[2283]\tvalidation-rmse:0.01641\n",
      "[2284]\tvalidation-rmse:0.01641\n",
      "[2285]\tvalidation-rmse:0.01641\n",
      "[2286]\tvalidation-rmse:0.01641\n",
      "[2287]\tvalidation-rmse:0.01641\n",
      "[2288]\tvalidation-rmse:0.01641\n",
      "[2289]\tvalidation-rmse:0.01641\n",
      "[2290]\tvalidation-rmse:0.01641\n",
      "[2291]\tvalidation-rmse:0.01641\n",
      "[2292]\tvalidation-rmse:0.01641\n",
      "[2293]\tvalidation-rmse:0.01641\n",
      "[2294]\tvalidation-rmse:0.01641\n",
      "[2295]\tvalidation-rmse:0.01641\n",
      "[2296]\tvalidation-rmse:0.01641\n",
      "[2297]\tvalidation-rmse:0.01641\n",
      "[2298]\tvalidation-rmse:0.01640\n",
      "[2299]\tvalidation-rmse:0.01640\n",
      "[2300]\tvalidation-rmse:0.01640\n",
      "[2301]\tvalidation-rmse:0.01640\n",
      "[2302]\tvalidation-rmse:0.01641\n",
      "[2303]\tvalidation-rmse:0.01641\n",
      "[2304]\tvalidation-rmse:0.01641\n",
      "[2305]\tvalidation-rmse:0.01641\n",
      "[2306]\tvalidation-rmse:0.01640\n",
      "[2307]\tvalidation-rmse:0.01640\n",
      "[2308]\tvalidation-rmse:0.01640\n",
      "[2309]\tvalidation-rmse:0.01640\n",
      "[2310]\tvalidation-rmse:0.01640\n",
      "[2311]\tvalidation-rmse:0.01640\n",
      "[2312]\tvalidation-rmse:0.01640\n",
      "[2313]\tvalidation-rmse:0.01640\n",
      "[2314]\tvalidation-rmse:0.01640\n",
      "[2315]\tvalidation-rmse:0.01640\n",
      "[2316]\tvalidation-rmse:0.01640\n",
      "[2317]\tvalidation-rmse:0.01640\n",
      "[2318]\tvalidation-rmse:0.01640\n",
      "[2319]\tvalidation-rmse:0.01640\n",
      "[2320]\tvalidation-rmse:0.01640\n",
      "[2321]\tvalidation-rmse:0.01640\n",
      "[2322]\tvalidation-rmse:0.01640\n",
      "[2323]\tvalidation-rmse:0.01640\n",
      "[2324]\tvalidation-rmse:0.01640\n",
      "[2325]\tvalidation-rmse:0.01640\n",
      "[2326]\tvalidation-rmse:0.01640\n",
      "[2327]\tvalidation-rmse:0.01640\n",
      "[2328]\tvalidation-rmse:0.01640\n",
      "[2329]\tvalidation-rmse:0.01640\n",
      "[2330]\tvalidation-rmse:0.01640\n",
      "[2331]\tvalidation-rmse:0.01640\n",
      "[2332]\tvalidation-rmse:0.01640\n",
      "[2333]\tvalidation-rmse:0.01640\n",
      "[2334]\tvalidation-rmse:0.01640\n",
      "[2335]\tvalidation-rmse:0.01640\n",
      "[2336]\tvalidation-rmse:0.01640\n",
      "[2337]\tvalidation-rmse:0.01640\n",
      "[2338]\tvalidation-rmse:0.01640\n",
      "[2339]\tvalidation-rmse:0.01640\n",
      "[2340]\tvalidation-rmse:0.01640\n",
      "[2341]\tvalidation-rmse:0.01640\n",
      "[2342]\tvalidation-rmse:0.01640\n",
      "[2343]\tvalidation-rmse:0.01640\n",
      "[2344]\tvalidation-rmse:0.01640\n",
      "[2345]\tvalidation-rmse:0.01640\n",
      "[2346]\tvalidation-rmse:0.01640\n",
      "[2347]\tvalidation-rmse:0.01640\n",
      "[2348]\tvalidation-rmse:0.01640\n",
      "[2349]\tvalidation-rmse:0.01640\n",
      "[2350]\tvalidation-rmse:0.01640\n",
      "[2351]\tvalidation-rmse:0.01640\n",
      "[2352]\tvalidation-rmse:0.01640\n",
      "[2353]\tvalidation-rmse:0.01640\n",
      "[2354]\tvalidation-rmse:0.01640\n",
      "[2355]\tvalidation-rmse:0.01640\n",
      "[2356]\tvalidation-rmse:0.01639\n",
      "[2357]\tvalidation-rmse:0.01639\n",
      "[2358]\tvalidation-rmse:0.01639\n",
      "[2359]\tvalidation-rmse:0.01639\n",
      "[2360]\tvalidation-rmse:0.01639\n",
      "[2361]\tvalidation-rmse:0.01639\n",
      "[2362]\tvalidation-rmse:0.01639\n",
      "[2363]\tvalidation-rmse:0.01639\n",
      "[2364]\tvalidation-rmse:0.01639\n",
      "[2365]\tvalidation-rmse:0.01639\n",
      "[2366]\tvalidation-rmse:0.01640\n",
      "[2367]\tvalidation-rmse:0.01639\n",
      "[2368]\tvalidation-rmse:0.01640\n",
      "[2369]\tvalidation-rmse:0.01640\n",
      "[2370]\tvalidation-rmse:0.01640\n",
      "[2371]\tvalidation-rmse:0.01640\n",
      "[2372]\tvalidation-rmse:0.01640\n",
      "[2373]\tvalidation-rmse:0.01640\n",
      "[2374]\tvalidation-rmse:0.01640\n",
      "[2375]\tvalidation-rmse:0.01640\n",
      "[2376]\tvalidation-rmse:0.01640\n",
      "[2377]\tvalidation-rmse:0.01640\n",
      "[2378]\tvalidation-rmse:0.01639\n",
      "[2379]\tvalidation-rmse:0.01640\n",
      "[2380]\tvalidation-rmse:0.01640\n",
      "[2381]\tvalidation-rmse:0.01640\n",
      "[2382]\tvalidation-rmse:0.01640\n",
      "[2383]\tvalidation-rmse:0.01639\n",
      "[2384]\tvalidation-rmse:0.01639\n",
      "[2385]\tvalidation-rmse:0.01639\n",
      "[2386]\tvalidation-rmse:0.01639\n",
      "[2387]\tvalidation-rmse:0.01639\n",
      "[2388]\tvalidation-rmse:0.01639\n",
      "[2389]\tvalidation-rmse:0.01639\n",
      "[2390]\tvalidation-rmse:0.01639\n",
      "[2391]\tvalidation-rmse:0.01639\n",
      "[2392]\tvalidation-rmse:0.01639\n",
      "[2393]\tvalidation-rmse:0.01639\n",
      "[2394]\tvalidation-rmse:0.01639\n",
      "[2395]\tvalidation-rmse:0.01639\n",
      "[2396]\tvalidation-rmse:0.01639\n",
      "[2397]\tvalidation-rmse:0.01639\n",
      "[2398]\tvalidation-rmse:0.01639\n",
      "[2399]\tvalidation-rmse:0.01639\n",
      "[2400]\tvalidation-rmse:0.01639\n",
      "[2401]\tvalidation-rmse:0.01639\n",
      "[2402]\tvalidation-rmse:0.01639\n",
      "[2403]\tvalidation-rmse:0.01639\n",
      "[2404]\tvalidation-rmse:0.01639\n",
      "[2405]\tvalidation-rmse:0.01639\n",
      "[2406]\tvalidation-rmse:0.01639\n",
      "[2407]\tvalidation-rmse:0.01639\n",
      "[2408]\tvalidation-rmse:0.01639\n",
      "[2409]\tvalidation-rmse:0.01639\n",
      "[2410]\tvalidation-rmse:0.01639\n",
      "[2411]\tvalidation-rmse:0.01639\n",
      "[2412]\tvalidation-rmse:0.01639\n",
      "[2413]\tvalidation-rmse:0.01639\n",
      "[2414]\tvalidation-rmse:0.01639\n",
      "[2415]\tvalidation-rmse:0.01639\n",
      "[2416]\tvalidation-rmse:0.01639\n",
      "[2417]\tvalidation-rmse:0.01639\n",
      "[2418]\tvalidation-rmse:0.01639\n",
      "[2419]\tvalidation-rmse:0.01639\n",
      "[2420]\tvalidation-rmse:0.01639\n",
      "[2421]\tvalidation-rmse:0.01639\n",
      "[2422]\tvalidation-rmse:0.01639\n",
      "[2423]\tvalidation-rmse:0.01639\n",
      "[2424]\tvalidation-rmse:0.01639\n",
      "[2425]\tvalidation-rmse:0.01639\n",
      "[2426]\tvalidation-rmse:0.01639\n",
      "[2427]\tvalidation-rmse:0.01639\n",
      "[2428]\tvalidation-rmse:0.01639\n",
      "[2429]\tvalidation-rmse:0.01639\n",
      "[2430]\tvalidation-rmse:0.01639\n",
      "[2431]\tvalidation-rmse:0.01639\n",
      "[2432]\tvalidation-rmse:0.01639\n",
      "[2433]\tvalidation-rmse:0.01639\n",
      "[2434]\tvalidation-rmse:0.01639\n",
      "[2435]\tvalidation-rmse:0.01639\n",
      "[2436]\tvalidation-rmse:0.01639\n",
      "[2437]\tvalidation-rmse:0.01639\n",
      "[2438]\tvalidation-rmse:0.01639\n",
      "[2439]\tvalidation-rmse:0.01639\n",
      "[2440]\tvalidation-rmse:0.01639\n",
      "[2441]\tvalidation-rmse:0.01639\n",
      "[2442]\tvalidation-rmse:0.01639\n",
      "[2443]\tvalidation-rmse:0.01639\n",
      "[2444]\tvalidation-rmse:0.01639\n",
      "[2445]\tvalidation-rmse:0.01639\n",
      "[2446]\tvalidation-rmse:0.01639\n",
      "[2447]\tvalidation-rmse:0.01639\n",
      "[2448]\tvalidation-rmse:0.01639\n",
      "[2449]\tvalidation-rmse:0.01639\n",
      "[2450]\tvalidation-rmse:0.01639\n",
      "[2451]\tvalidation-rmse:0.01639\n",
      "[2452]\tvalidation-rmse:0.01639\n",
      "[2453]\tvalidation-rmse:0.01639\n",
      "[2454]\tvalidation-rmse:0.01639\n",
      "[2455]\tvalidation-rmse:0.01639\n",
      "[2456]\tvalidation-rmse:0.01639\n",
      "[2457]\tvalidation-rmse:0.01639\n",
      "[2458]\tvalidation-rmse:0.01639\n",
      "[2459]\tvalidation-rmse:0.01639\n",
      "[2460]\tvalidation-rmse:0.01639\n",
      "[2461]\tvalidation-rmse:0.01639\n",
      "[2462]\tvalidation-rmse:0.01639\n",
      "[2463]\tvalidation-rmse:0.01639\n",
      "[2464]\tvalidation-rmse:0.01639\n",
      "[2465]\tvalidation-rmse:0.01639\n",
      "[2466]\tvalidation-rmse:0.01639\n",
      "[2467]\tvalidation-rmse:0.01639\n",
      "[2468]\tvalidation-rmse:0.01639\n",
      "[2469]\tvalidation-rmse:0.01639\n",
      "[2470]\tvalidation-rmse:0.01639\n",
      "[2471]\tvalidation-rmse:0.01639\n",
      "[2472]\tvalidation-rmse:0.01639\n",
      "[2473]\tvalidation-rmse:0.01639\n",
      "[2474]\tvalidation-rmse:0.01639\n",
      "[2475]\tvalidation-rmse:0.01639\n",
      "[2476]\tvalidation-rmse:0.01639\n",
      "[2477]\tvalidation-rmse:0.01639\n",
      "[2478]\tvalidation-rmse:0.01639\n",
      "[2479]\tvalidation-rmse:0.01639\n",
      "[2480]\tvalidation-rmse:0.01639\n",
      "[2481]\tvalidation-rmse:0.01639\n",
      "[2482]\tvalidation-rmse:0.01639\n",
      "[2483]\tvalidation-rmse:0.01639\n",
      "[2484]\tvalidation-rmse:0.01639\n",
      "[2485]\tvalidation-rmse:0.01639\n",
      "[2486]\tvalidation-rmse:0.01639\n",
      "[2487]\tvalidation-rmse:0.01639\n",
      "[2488]\tvalidation-rmse:0.01639\n",
      "[2489]\tvalidation-rmse:0.01639\n",
      "[2490]\tvalidation-rmse:0.01639\n",
      "[2491]\tvalidation-rmse:0.01638\n",
      "[2492]\tvalidation-rmse:0.01638\n",
      "[2493]\tvalidation-rmse:0.01638\n",
      "[2494]\tvalidation-rmse:0.01638\n",
      "[2495]\tvalidation-rmse:0.01638\n",
      "[2496]\tvalidation-rmse:0.01638\n",
      "[2497]\tvalidation-rmse:0.01638\n",
      "[2498]\tvalidation-rmse:0.01638\n",
      "[2499]\tvalidation-rmse:0.01638\n",
      "[2500]\tvalidation-rmse:0.01639\n",
      "[2501]\tvalidation-rmse:0.01639\n",
      "[2502]\tvalidation-rmse:0.01639\n",
      "[2503]\tvalidation-rmse:0.01639\n",
      "[2504]\tvalidation-rmse:0.01639\n",
      "[2505]\tvalidation-rmse:0.01638\n",
      "[2506]\tvalidation-rmse:0.01638\n",
      "[2507]\tvalidation-rmse:0.01638\n",
      "[2508]\tvalidation-rmse:0.01638\n",
      "[2509]\tvalidation-rmse:0.01638\n",
      "[2510]\tvalidation-rmse:0.01638\n",
      "[2511]\tvalidation-rmse:0.01638\n",
      "[2512]\tvalidation-rmse:0.01638\n",
      "[2513]\tvalidation-rmse:0.01638\n",
      "[2514]\tvalidation-rmse:0.01638\n",
      "[2515]\tvalidation-rmse:0.01638\n",
      "[2516]\tvalidation-rmse:0.01638\n",
      "[2517]\tvalidation-rmse:0.01638\n",
      "[2518]\tvalidation-rmse:0.01638\n",
      "[2519]\tvalidation-rmse:0.01638\n",
      "[2520]\tvalidation-rmse:0.01638\n",
      "[2521]\tvalidation-rmse:0.01638\n",
      "[2522]\tvalidation-rmse:0.01638\n",
      "[2523]\tvalidation-rmse:0.01638\n",
      "[2524]\tvalidation-rmse:0.01638\n",
      "[2525]\tvalidation-rmse:0.01638\n",
      "[2526]\tvalidation-rmse:0.01638\n",
      "[2527]\tvalidation-rmse:0.01638\n",
      "[2528]\tvalidation-rmse:0.01638\n",
      "[2529]\tvalidation-rmse:0.01638\n",
      "[2530]\tvalidation-rmse:0.01638\n",
      "[2531]\tvalidation-rmse:0.01638\n",
      "[2532]\tvalidation-rmse:0.01638\n",
      "[2533]\tvalidation-rmse:0.01638\n",
      "[2534]\tvalidation-rmse:0.01638\n",
      "[2535]\tvalidation-rmse:0.01638\n",
      "[2536]\tvalidation-rmse:0.01638\n",
      "[2537]\tvalidation-rmse:0.01638\n",
      "[2538]\tvalidation-rmse:0.01638\n",
      "[2539]\tvalidation-rmse:0.01638\n",
      "[2540]\tvalidation-rmse:0.01638\n",
      "[2541]\tvalidation-rmse:0.01638\n",
      "[2542]\tvalidation-rmse:0.01638\n",
      "[2543]\tvalidation-rmse:0.01638\n",
      "[2544]\tvalidation-rmse:0.01638\n",
      "[2545]\tvalidation-rmse:0.01638\n",
      "[2546]\tvalidation-rmse:0.01638\n",
      "[2547]\tvalidation-rmse:0.01638\n",
      "[2548]\tvalidation-rmse:0.01638\n",
      "[2549]\tvalidation-rmse:0.01638\n",
      "[2550]\tvalidation-rmse:0.01638\n",
      "[2551]\tvalidation-rmse:0.01638\n",
      "[2552]\tvalidation-rmse:0.01638\n",
      "[2553]\tvalidation-rmse:0.01638\n",
      "[2554]\tvalidation-rmse:0.01638\n",
      "[2555]\tvalidation-rmse:0.01638\n",
      "[2556]\tvalidation-rmse:0.01638\n",
      "[2557]\tvalidation-rmse:0.01638\n",
      "[2558]\tvalidation-rmse:0.01638\n",
      "[2559]\tvalidation-rmse:0.01638\n",
      "[2560]\tvalidation-rmse:0.01638\n",
      "[2561]\tvalidation-rmse:0.01638\n",
      "[2562]\tvalidation-rmse:0.01638\n",
      "[2563]\tvalidation-rmse:0.01638\n",
      "[2564]\tvalidation-rmse:0.01638\n",
      "[2565]\tvalidation-rmse:0.01638\n",
      "[2566]\tvalidation-rmse:0.01638\n",
      "[2567]\tvalidation-rmse:0.01638\n",
      "[2568]\tvalidation-rmse:0.01638\n",
      "[2569]\tvalidation-rmse:0.01638\n",
      "[2570]\tvalidation-rmse:0.01638\n",
      "[2571]\tvalidation-rmse:0.01638\n",
      "[2572]\tvalidation-rmse:0.01638\n",
      "[2573]\tvalidation-rmse:0.01638\n",
      "[2574]\tvalidation-rmse:0.01638\n",
      "[2575]\tvalidation-rmse:0.01638\n",
      "[2576]\tvalidation-rmse:0.01638\n",
      "[2577]\tvalidation-rmse:0.01638\n",
      "[2578]\tvalidation-rmse:0.01638\n",
      "[2579]\tvalidation-rmse:0.01638\n",
      "[2580]\tvalidation-rmse:0.01638\n",
      "[2581]\tvalidation-rmse:0.01638\n",
      "[2582]\tvalidation-rmse:0.01638\n",
      "[2583]\tvalidation-rmse:0.01638\n",
      "[2584]\tvalidation-rmse:0.01638\n",
      "[2585]\tvalidation-rmse:0.01638\n",
      "[2586]\tvalidation-rmse:0.01638\n",
      "[2587]\tvalidation-rmse:0.01638\n",
      "[2588]\tvalidation-rmse:0.01638\n",
      "[2589]\tvalidation-rmse:0.01638\n",
      "[2590]\tvalidation-rmse:0.01638\n",
      "[2591]\tvalidation-rmse:0.01638\n",
      "[2592]\tvalidation-rmse:0.01638\n",
      "[2593]\tvalidation-rmse:0.01638\n",
      "[2594]\tvalidation-rmse:0.01638\n",
      "[2595]\tvalidation-rmse:0.01638\n",
      "[2596]\tvalidation-rmse:0.01638\n",
      "[2597]\tvalidation-rmse:0.01638\n",
      "[2598]\tvalidation-rmse:0.01638\n",
      "[2599]\tvalidation-rmse:0.01638\n",
      "[2600]\tvalidation-rmse:0.01638\n",
      "[2601]\tvalidation-rmse:0.01638\n",
      "[2602]\tvalidation-rmse:0.01638\n",
      "[2603]\tvalidation-rmse:0.01638\n",
      "[2604]\tvalidation-rmse:0.01638\n",
      "[2605]\tvalidation-rmse:0.01638\n",
      "[2606]\tvalidation-rmse:0.01638\n",
      "[2607]\tvalidation-rmse:0.01638\n",
      "[2608]\tvalidation-rmse:0.01638\n",
      "[2609]\tvalidation-rmse:0.01638\n",
      "[2610]\tvalidation-rmse:0.01638\n",
      "[2611]\tvalidation-rmse:0.01638\n",
      "[2612]\tvalidation-rmse:0.01638\n",
      "[2613]\tvalidation-rmse:0.01638\n",
      "[2614]\tvalidation-rmse:0.01638\n",
      "[2615]\tvalidation-rmse:0.01638\n",
      "[2616]\tvalidation-rmse:0.01638\n",
      "[2617]\tvalidation-rmse:0.01638\n",
      "[2618]\tvalidation-rmse:0.01638\n",
      "[2619]\tvalidation-rmse:0.01638\n",
      "[2620]\tvalidation-rmse:0.01638\n",
      "[2621]\tvalidation-rmse:0.01638\n",
      "[2622]\tvalidation-rmse:0.01638\n",
      "[2623]\tvalidation-rmse:0.01638\n",
      "[2624]\tvalidation-rmse:0.01638\n",
      "[2625]\tvalidation-rmse:0.01638\n",
      "[2626]\tvalidation-rmse:0.01638\n",
      "[2627]\tvalidation-rmse:0.01638\n",
      "[2628]\tvalidation-rmse:0.01638\n",
      "[2629]\tvalidation-rmse:0.01638\n",
      "[2630]\tvalidation-rmse:0.01638\n",
      "[2631]\tvalidation-rmse:0.01638\n",
      "[2632]\tvalidation-rmse:0.01638\n",
      "[2633]\tvalidation-rmse:0.01638\n",
      "[2634]\tvalidation-rmse:0.01638\n",
      "[2635]\tvalidation-rmse:0.01638\n",
      "[2636]\tvalidation-rmse:0.01638\n",
      "[2637]\tvalidation-rmse:0.01638\n",
      "[2638]\tvalidation-rmse:0.01638\n",
      "[2639]\tvalidation-rmse:0.01638\n",
      "[2640]\tvalidation-rmse:0.01638\n",
      "[2641]\tvalidation-rmse:0.01638\n",
      "[2642]\tvalidation-rmse:0.01638\n",
      "[2643]\tvalidation-rmse:0.01638\n",
      "[2644]\tvalidation-rmse:0.01638\n",
      "[2645]\tvalidation-rmse:0.01638\n",
      "[2646]\tvalidation-rmse:0.01638\n",
      "[2647]\tvalidation-rmse:0.01638\n",
      "[2648]\tvalidation-rmse:0.01638\n",
      "[2649]\tvalidation-rmse:0.01638\n",
      "[2650]\tvalidation-rmse:0.01638\n",
      "[2651]\tvalidation-rmse:0.01638\n",
      "[2652]\tvalidation-rmse:0.01638\n",
      "[2653]\tvalidation-rmse:0.01638\n",
      "[2654]\tvalidation-rmse:0.01638\n",
      "[2655]\tvalidation-rmse:0.01638\n",
      "[2656]\tvalidation-rmse:0.01638\n",
      "[2657]\tvalidation-rmse:0.01638\n",
      "[2658]\tvalidation-rmse:0.01638\n",
      "[2659]\tvalidation-rmse:0.01638\n",
      "[2660]\tvalidation-rmse:0.01638\n",
      "[2661]\tvalidation-rmse:0.01638\n",
      "[2662]\tvalidation-rmse:0.01638\n",
      "[2663]\tvalidation-rmse:0.01638\n",
      "[2664]\tvalidation-rmse:0.01638\n",
      "[2665]\tvalidation-rmse:0.01638\n",
      "[2666]\tvalidation-rmse:0.01638\n",
      "[2667]\tvalidation-rmse:0.01638\n",
      "[2668]\tvalidation-rmse:0.01638\n",
      "[2669]\tvalidation-rmse:0.01638\n",
      "[2670]\tvalidation-rmse:0.01638\n",
      "[2671]\tvalidation-rmse:0.01638\n",
      "[2672]\tvalidation-rmse:0.01638\n",
      "[2673]\tvalidation-rmse:0.01638\n",
      "[2674]\tvalidation-rmse:0.01638\n",
      "[2675]\tvalidation-rmse:0.01638\n",
      "[2676]\tvalidation-rmse:0.01638\n",
      "[2677]\tvalidation-rmse:0.01638\n",
      "[2678]\tvalidation-rmse:0.01638\n",
      "[2679]\tvalidation-rmse:0.01638\n",
      "[2680]\tvalidation-rmse:0.01638\n",
      "[2681]\tvalidation-rmse:0.01638\n",
      "[2682]\tvalidation-rmse:0.01638\n",
      "[2683]\tvalidation-rmse:0.01638\n",
      "[2684]\tvalidation-rmse:0.01638\n",
      "[2685]\tvalidation-rmse:0.01638\n",
      "[2686]\tvalidation-rmse:0.01638\n",
      "[2687]\tvalidation-rmse:0.01638\n",
      "[2688]\tvalidation-rmse:0.01638\n",
      "[2689]\tvalidation-rmse:0.01638\n",
      "[2690]\tvalidation-rmse:0.01638\n",
      "[2691]\tvalidation-rmse:0.01638\n",
      "[2692]\tvalidation-rmse:0.01638\n",
      "[2693]\tvalidation-rmse:0.01638\n",
      "[2694]\tvalidation-rmse:0.01638\n",
      "[2695]\tvalidation-rmse:0.01638\n",
      "[2696]\tvalidation-rmse:0.01638\n",
      "[2697]\tvalidation-rmse:0.01638\n",
      "[2698]\tvalidation-rmse:0.01638\n",
      "[2699]\tvalidation-rmse:0.01638\n",
      "[2700]\tvalidation-rmse:0.01638\n",
      "[2701]\tvalidation-rmse:0.01638\n",
      "[2702]\tvalidation-rmse:0.01638\n",
      "[2703]\tvalidation-rmse:0.01638\n",
      "[2704]\tvalidation-rmse:0.01638\n",
      "[2705]\tvalidation-rmse:0.01638\n",
      "[2706]\tvalidation-rmse:0.01638\n",
      "[2707]\tvalidation-rmse:0.01638\n",
      "[2708]\tvalidation-rmse:0.01638\n",
      "[2709]\tvalidation-rmse:0.01638\n",
      "[2710]\tvalidation-rmse:0.01638\n",
      "[2711]\tvalidation-rmse:0.01638\n",
      "[2712]\tvalidation-rmse:0.01638\n",
      "[2713]\tvalidation-rmse:0.01638\n",
      "[2714]\tvalidation-rmse:0.01638\n",
      "[2715]\tvalidation-rmse:0.01638\n",
      "[2716]\tvalidation-rmse:0.01638\n",
      "[2717]\tvalidation-rmse:0.01638\n",
      "[2718]\tvalidation-rmse:0.01638\n",
      "[2719]\tvalidation-rmse:0.01638\n",
      "[2720]\tvalidation-rmse:0.01638\n",
      "[2721]\tvalidation-rmse:0.01638\n",
      "[2722]\tvalidation-rmse:0.01638\n",
      "[2723]\tvalidation-rmse:0.01638\n",
      "[2724]\tvalidation-rmse:0.01638\n",
      "[2725]\tvalidation-rmse:0.01638\n",
      "[2726]\tvalidation-rmse:0.01638\n",
      "[2727]\tvalidation-rmse:0.01638\n",
      "[2728]\tvalidation-rmse:0.01638\n",
      "[2729]\tvalidation-rmse:0.01638\n",
      "Model for Z stopped at best iteration: 2629\n",
      "Prediction output shape: (4018, 3)\n",
      "Expected shape: (4018, 3)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_base = {\n",
    "    'n_estimators': 5000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "num_boost_round = 5000 # Max number of trees\n",
    "\n",
    "y_train_cols = [y_train.values[:, i] for i in range(3)] # [y_train_X, y_train_Y, y_train_Z]\n",
    "y_test_cols = [y_test.values[:, i] for i in range(3)]   # [y_test_X, y_test_Y, y_test_Z]\n",
    "\n",
    "dtrain_cols = [xgb.DMatrix(X_xgb_train_scaled, label=y) for y in y_train_cols]\n",
    "dtest_cols = [xgb.DMatrix(X_xgb_test_scaled, label=y) for y in y_test_cols]\n",
    "\n",
    "# Initialize prediction array\n",
    "N_test = X_xgb_test_scaled.shape[0]\n",
    "y_pred_xgb_tuned_au = np.zeros((N_test, 3))\n",
    "\n",
    "coord_names = ['X', 'Y', 'Z']\n",
    "\n",
    "print(\"Starting manual, stable XGBoost training (one model per coordinate)...\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Training {coord_names[i]}-coordinate model ---\")\n",
    "    \n",
    "    # Fit the single-output model with EARLY STOPPING\n",
    "    model = xgb.train(\n",
    "        params=xgb_base,\n",
    "        dtrain=dtrain_cols[i],\n",
    "        num_boost_round=num_boost_round,\n",
    "        \n",
    "        # This is the correct native way to pass the eval set\n",
    "        evals=[(dtest_cols[i], 'validation')], \n",
    "        \n",
    "        # This is the correct native way to pass early stopping\n",
    "        early_stopping_rounds=100, \n",
    "        \n",
    "        # verbose=False,\n",
    "    )\n",
    "    \n",
    "    # Store predictions\n",
    "    y_pred_xgb_tuned_au[:, i] = model.predict(dtest_cols[i], iteration_range=(0, model.best_iteration))\n",
    "    \n",
    "    print(f\"Model for {coord_names[i]} stopped at best iteration: {model.best_iteration}\")\n",
    "\n",
    "\n",
    "print(f\"Prediction output shape: {y_pred_xgb_tuned_au.shape}\")\n",
    "print(f\"Expected shape: ({len(X_xgb_test_scaled)}, 3)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation (XGBoost) ---\n",
      "Overall Averaged RMSE: 0.022144 AU\n",
      "AUX-coordinate RMSE: 0.027297 AU\n",
      "AUY-coordinate RMSE: 0.022760 AU\n",
      "AUZ-coordinate RMSE: 0.016376 AU\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE for each coordinate individually\n",
    "rmse_x_xgb = np.sqrt(mean_squared_error(y_test_np[:, 0], y_pred_xgb_tuned_au[:, 0]))\n",
    "rmse_y_xgb = np.sqrt(mean_squared_error(y_test_np[:, 1], y_pred_xgb_tuned_au[:, 1]))\n",
    "rmse_z_xgb = np.sqrt(mean_squared_error(y_test_np[:, 2], y_pred_xgb_tuned_au[:, 2]))\n",
    "\n",
    "# Calculate the overall averaged RMSE\n",
    "rmse_overall_ens = np.mean([rmse_x_xgb, rmse_y_xgb, rmse_z_xgb])\n",
    "\n",
    "print(\"\\n--- Model Evaluation (XGBoost) ---\")\n",
    "print(f\"Overall Averaged RMSE: {rmse_overall_ens:.6f} AU\")\n",
    "print(f\"AUX-coordinate RMSE: {rmse_x_xgb:.6f} AU\")\n",
    "print(f\"AUY-coordinate RMSE: {rmse_y_xgb:.6f} AU\")\n",
    "print(f\"AUZ-coordinate RMSE: {rmse_z_xgb:.6f} AU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting manual, stable XGBoost training (one model per coordinate)...\n",
      "\n",
      "--- Training X-coordinate model ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'xgboost.callback' has no attribute 'early_stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 30\u001b[39m\n",
      "\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Initialize a fresh XGBRegressor for each target\u001b[39;00m\n",
      "\u001b[32m     28\u001b[39m model = xgb.XGBRegressor(**xgb_base)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m early_stop_callback = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stop\u001b[49m(\n",
      "\u001b[32m     31\u001b[39m     rounds=\u001b[32m100\u001b[39m, \u001b[38;5;66;03m# Use 'rounds' instead of 'stopping_rounds' for callback object\u001b[39;00m\n",
      "\u001b[32m     32\u001b[39m     metric_name=\u001b[33m'\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m'\u001b[39m,\n",
      "\u001b[32m     33\u001b[39m     data_name=\u001b[33m'\u001b[39m\u001b[33mvalidation_0\u001b[39m\u001b[33m'\u001b[39m,\n",
      "\u001b[32m     34\u001b[39m     save_best=\u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m     35\u001b[39m )\n",
      "\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Fit the single-output model with EARLY STOPPING\u001b[39;00m\n",
      "\u001b[32m     38\u001b[39m model.fit(\n",
      "\u001b[32m     39\u001b[39m     X_train_scaled, \n",
      "\u001b[32m     40\u001b[39m     y_train_cols[i], \u001b[38;5;66;03m# Single target column\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[32m     48\u001b[39m )\n",
      "\n",
      "\u001b[31mAttributeError\u001b[39m: module 'xgboost.callback' has no attribute 'early_stop'"
     ]
    }
   ],
   "source": [
    "# import xgboost as xgb # type: ignore\n",
    "\n",
    "# xgb_base = {\n",
    "#     'n_estimators': 5000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 6,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'random_state': 42,\n",
    "#     'n_jobs': -1,\n",
    "#     'objective': 'reg:squarederror'\n",
    "# }\n",
    "\n",
    "# y_train_cols = [y_train.values[:, i] for i in range(3)] # [y_train_X, y_train_Y, y_train_Z]\n",
    "# y_test_cols = [y_test.values[:, i] for i in range(3)]   # [y_test_X, y_test_Y, y_test_Z]\n",
    "\n",
    "# # Initialize prediction array\n",
    "# N_test = X_test_scaled.shape[0]\n",
    "# y_pred_xgb_tuned_au = np.zeros((N_test, 3))\n",
    "\n",
    "# coord_names = ['X', 'Y', 'Z']\n",
    "\n",
    "# print(\"Starting manual, stable XGBoost training (one model per coordinate)...\")\n",
    "# for i in range(3):\n",
    "#     print(f\"\\n--- Training {coord_names[i]}-coordinate model ---\")\n",
    "    \n",
    "#     # Initialize a fresh XGBRegressor for each target\n",
    "#     model = xgb.XGBRegressor(**xgb_base)\n",
    "    \n",
    "#     early_stop_callback = xgb.callback.early_stop(\n",
    "#         rounds=100, # Use 'rounds' instead of 'stopping_rounds' for callback object\n",
    "#         metric_name='rmse',\n",
    "#         data_name='validation_0',\n",
    "#         save_best=True\n",
    "#     )\n",
    "    \n",
    "#     # Fit the single-output model with EARLY STOPPING\n",
    "#     model.fit(\n",
    "#         X_train_scaled, \n",
    "#         y_train_cols[i], # Single target column\n",
    "        \n",
    "#         # Pass the validation set with a named tuple (validation_0)\n",
    "#         eval_set=[(X_test_scaled, y_test_cols[i], 'validation_0')], \n",
    "        \n",
    "#         # Pass the callback list\n",
    "#         callbacks=[early_stop_callback],\n",
    "#         verbose=False,\n",
    "#     )\n",
    "    \n",
    "#     # Store predictions\n",
    "#     y_pred_xgb_tuned_au[:, i] = model.predict(X_test_scaled)\n",
    "    \n",
    "#     print(f\"Model for {coord_names[i]} stopped at best iteration: {model.best_iteration}\")\n",
    "\n",
    "\n",
    "# print(f\"Prediction output shape: {y_pred_xgb_tuned_au.shape}\")\n",
    "# print(f\"Expected shape: ({len(X_test_scaled)}, 3)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation (XGBoost + MLP Ensemble) ---\n",
      "Overall Averaged RMSE: 0.006965 AU\n",
      "AUX-coordinate RMSE: 0.008629 AU\n",
      "AUY-coordinate RMSE: 0.007312 AU\n",
      "AUZ-coordinate RMSE: 0.004954 AU\n"
     ]
    }
   ],
   "source": [
    "# Running ensamble\n",
    "\n",
    "# Define Weights\n",
    "W_MLP = 0.7\n",
    "W_XGB = 0.3\n",
    "\n",
    "# Create the Ensemble Prediction (weighted average of the two models)\n",
    "y_pred_ensemble_au = (W_MLP * y_pred_mlp_au) + (W_XGB * y_pred_xgb_tuned_au)\n",
    "\n",
    "# Evaluate the Ensemble\n",
    "# You need your original unscaled test targets (y_test) as a NumPy array for comparison.\n",
    "# Assuming 'y_test' is your DataFrame of unscaled targets (X_au, Y_au, Z_au)\n",
    "y_test_np = y_test.values \n",
    "\n",
    "# Calculate RMSE for each coordinate individually\n",
    "rmse_x_ens = np.sqrt(mean_squared_error(y_test_np[:, 0], y_pred_ensemble_au[:, 0]))\n",
    "rmse_y_ens = np.sqrt(mean_squared_error(y_test_np[:, 1], y_pred_ensemble_au[:, 1]))\n",
    "rmse_z_ens = np.sqrt(mean_squared_error(y_test_np[:, 2], y_pred_ensemble_au[:, 2]))\n",
    "\n",
    "# Calculate the overall averaged RMSE\n",
    "rmse_overall_ens = np.mean([rmse_x_ens, rmse_y_ens, rmse_z_ens])\n",
    "\n",
    "print(\"\\n--- Model Evaluation (XGBoost + MLP Ensemble) ---\")\n",
    "print(f\"Overall Averaged RMSE: {rmse_overall_ens:.6f} AU\")\n",
    "print(f\"AUX-coordinate RMSE: {rmse_x_ens:.6f} AU\")\n",
    "print(f\"AUY-coordinate RMSE: {rmse_y_ens:.6f} AU\")\n",
    "print(f\"AUZ-coordinate RMSE: {rmse_z_ens:.6f} AU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training 3 diverse MLP models...\n",
      "\n",
      "--- Training MLP Model 1 ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - loss: 0.0556 - val_loss: 0.0392 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0226 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.0144 - val_loss: 0.0168 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.0093 - val_loss: 0.0109 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0063 - val_loss: 0.0084 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.0047 - val_loss: 0.0067 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0036 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0031 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 0.0024 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0023 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0020 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0020 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.0016 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0016 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.0016 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.0014 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.0014 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.0016 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0013 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0012 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0011 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0010 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0010 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 9.5101e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0010 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 0.0010 - val_loss: 8.9948e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0011 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 8.7592e-04 - val_loss: 8.6309e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 9.1155e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 9.2967e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 9.2375e-04 - val_loss: 8.4130e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 8.7091e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 9.2739e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 8.6216e-04 - val_loss: 9.2754e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 9.2258e-04 - val_loss: 9.8139e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 9.6546e-04 - val_loss: 9.3370e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 9.8100e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 8.1583e-04 - val_loss: 7.9075e-04 - learning_rate: 0.0010\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.0010 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 8.1817e-04 - val_loss: 8.7728e-04 - learning_rate: 0.0010\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 8.4040e-04 - val_loss: 8.5532e-04 - learning_rate: 0.0010\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 8.6962e-04 - val_loss: 8.8010e-04 - learning_rate: 0.0010\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 8.7326e-04 - val_loss: 9.2959e-04 - learning_rate: 0.0010\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 8.7380e-04 - val_loss: 8.8793e-04 - learning_rate: 0.0010\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 8.0692e-04 - val_loss: 9.3748e-04 - learning_rate: 0.0010\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 8.4274e-04 - val_loss: 9.9855e-04 - learning_rate: 0.0010\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 8.3315e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 8.5083e-04 - val_loss: 9.4423e-04 - learning_rate: 0.0010\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 8.0213e-04 - val_loss: 8.9512e-04 - learning_rate: 0.0010\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 7.9718e-04 - val_loss: 7.9676e-04 - learning_rate: 0.0010\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 9.5663e-04 - val_loss: 8.9741e-04 - learning_rate: 0.0010\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 7.3947e-04 - val_loss: 8.0197e-04 - learning_rate: 0.0010\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.8391e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 7.9729e-04 - val_loss: 8.6009e-04 - learning_rate: 0.0010\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 8.3189e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 7.6606e-04 - val_loss: 8.4141e-04 - learning_rate: 0.0010\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 7.8527e-04 - val_loss: 8.0482e-04 - learning_rate: 0.0010\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 7.9349e-04 - val_loss: 7.3647e-04 - learning_rate: 0.0010\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.7965e-04 - val_loss: 7.9329e-04 - learning_rate: 0.0010\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 7.7304e-04 - val_loss: 8.5592e-04 - learning_rate: 0.0010\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 8.5108e-04 - val_loss: 7.2578e-04 - learning_rate: 0.0010\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.9418e-04 - val_loss: 7.8033e-04 - learning_rate: 0.0010\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 7.5892e-04 - val_loss: 9.6052e-04 - learning_rate: 0.0010\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 7.3936e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 7.6865e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 7.3627e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 7.7335e-04 - val_loss: 7.8715e-04 - learning_rate: 0.0010\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 6.9232e-04 - val_loss: 9.7798e-04 - learning_rate: 0.0010\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.5453e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 7.1622e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 7.5605e-04 - val_loss: 8.1900e-04 - learning_rate: 0.0010\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 7.0710e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 7.4665e-04 - val_loss: 8.8270e-04 - learning_rate: 0.0010\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 6.9461e-04 - val_loss: 9.5543e-04 - learning_rate: 0.0010\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 8.2684e-04 - val_loss: 8.6676e-04 - learning_rate: 0.0010\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.6805e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 7.0052e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.0771e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 7.2648e-04 - val_loss: 8.3963e-04 - learning_rate: 0.0010\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.7627e-04 - val_loss: 7.3533e-04 - learning_rate: 0.0010\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 7.1990e-04 - val_loss: 9.7393e-04 - learning_rate: 0.0010\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 7.0266e-04 - val_loss: 9.6539e-04 - learning_rate: 0.0010\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 6.9792e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 6.7007e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 7.0947e-04 - val_loss: 7.6992e-04 - learning_rate: 0.0010\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 6.5961e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 7.0210e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.9769e-04 - val_loss: 8.2037e-04 - learning_rate: 0.0010\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 5.9810e-04 - val_loss: 5.9384e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 5.9340e-04 - val_loss: 6.3048e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.9471e-04 - val_loss: 6.0390e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 6.0421e-04 - val_loss: 6.1515e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 6.0055e-04 - val_loss: 6.6183e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 6.0262e-04 - val_loss: 7.3817e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 5.9900e-04 - val_loss: 7.2230e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.0481e-04 - val_loss: 6.3428e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.9417e-04 - val_loss: 6.5115e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.9445e-04 - val_loss: 6.5439e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.9135e-04 - val_loss: 6.5722e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.8972e-04 - val_loss: 6.5955e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.8810e-04 - val_loss: 6.8193e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.8557e-04 - val_loss: 6.6526e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.8310e-04 - val_loss: 6.5272e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.8180e-04 - val_loss: 6.1561e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 5.8101e-04 - val_loss: 6.7270e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.8759e-04 - val_loss: 6.3477e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.7961e-04 - val_loss: 5.8940e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.7167e-04 - val_loss: 5.9892e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.7839e-04 - val_loss: 5.9022e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.7553e-04 - val_loss: 5.7483e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.7227e-04 - val_loss: 5.8220e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 5.7097e-04 - val_loss: 5.8573e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.6702e-04 - val_loss: 5.7714e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.6983e-04 - val_loss: 5.8734e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.6614e-04 - val_loss: 5.7982e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.6489e-04 - val_loss: 5.7558e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 5.6591e-04 - val_loss: 6.1262e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.6668e-04 - val_loss: 6.1938e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.6239e-04 - val_loss: 6.1732e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.6256e-04 - val_loss: 6.2310e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 5.6614e-04 - val_loss: 6.9588e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 5.5863e-04 - val_loss: 6.8722e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 5.5708e-04 - val_loss: 6.5156e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.5762e-04 - val_loss: 6.7351e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 5.5425e-04 - val_loss: 6.2631e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 5.5322e-04 - val_loss: 6.0427e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.5102e-04 - val_loss: 5.7197e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.5290e-04 - val_loss: 6.1860e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.4775e-04 - val_loss: 5.7658e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 5.4832e-04 - val_loss: 5.6545e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.4712e-04 - val_loss: 5.6338e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.4734e-04 - val_loss: 5.9385e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 5.4520e-04 - val_loss: 5.9716e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.4364e-04 - val_loss: 5.7939e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 5.4303e-04 - val_loss: 5.6977e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.4199e-04 - val_loss: 5.6939e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 151/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.4137e-04 - val_loss: 5.7297e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 152/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 5.4022e-04 - val_loss: 5.6495e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 153/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.3873e-04 - val_loss: 5.7619e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 154/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 5.0712e-04 - val_loss: 5.1371e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 155/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.0733e-04 - val_loss: 5.3772e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 156/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 5.0687e-04 - val_loss: 5.3042e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 157/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.0938e-04 - val_loss: 5.3584e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 158/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.0955e-04 - val_loss: 5.3620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 159/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.0865e-04 - val_loss: 5.3589e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 160/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.0757e-04 - val_loss: 5.3554e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 161/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 5.0650e-04 - val_loss: 5.3479e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 162/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 5.0546e-04 - val_loss: 5.3324e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 163/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 5.0457e-04 - val_loss: 5.3127e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 164/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 5.0365e-04 - val_loss: 5.2798e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 165/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 5.0276e-04 - val_loss: 5.2625e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 166/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.0179e-04 - val_loss: 5.2367e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 167/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 5.0090e-04 - val_loss: 5.2180e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 168/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 5.0005e-04 - val_loss: 5.2093e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 169/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.9915e-04 - val_loss: 5.1938e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 170/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.9834e-04 - val_loss: 5.1906e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 171/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.9740e-04 - val_loss: 5.1945e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 172/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.9637e-04 - val_loss: 5.2065e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 173/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 4.9542e-04 - val_loss: 5.2152e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 174/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.9456e-04 - val_loss: 5.2617e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 175/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 4.9374e-04 - val_loss: 5.2808e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 176/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.9290e-04 - val_loss: 5.2899e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 177/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.9215e-04 - val_loss: 5.2765e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 178/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.9129e-04 - val_loss: 5.2833e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 179/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 4.9060e-04 - val_loss: 5.2584e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 180/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.8995e-04 - val_loss: 5.2265e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 181/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.8923e-04 - val_loss: 5.2435e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 182/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.8842e-04 - val_loss: 5.2453e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 183/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.8785e-04 - val_loss: 5.2097e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 184/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.8710e-04 - val_loss: 5.1950e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 185/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.8652e-04 - val_loss: 5.1754e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 186/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.8575e-04 - val_loss: 5.1807e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 187/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.8523e-04 - val_loss: 5.1486e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 188/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.8456e-04 - val_loss: 5.1361e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 189/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.8403e-04 - val_loss: 5.1324e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 190/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.8326e-04 - val_loss: 5.1183e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 191/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.8265e-04 - val_loss: 5.0947e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 192/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.8197e-04 - val_loss: 5.0717e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 193/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.8130e-04 - val_loss: 5.0520e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 194/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.8082e-04 - val_loss: 5.0104e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 195/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.7999e-04 - val_loss: 5.0148e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 196/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 4.7943e-04 - val_loss: 4.9963e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 197/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.7874e-04 - val_loss: 5.0005e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 198/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.7805e-04 - val_loss: 5.0160e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 199/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.7744e-04 - val_loss: 5.0046e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 200/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.7692e-04 - val_loss: 4.9835e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 201/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 4.7629e-04 - val_loss: 4.9630e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 202/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.7563e-04 - val_loss: 4.9932e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 203/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.7487e-04 - val_loss: 5.0621e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 204/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 4.6542e-04 - val_loss: 4.7709e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 205/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.6543e-04 - val_loss: 4.7923e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 206/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.6500e-04 - val_loss: 4.7630e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 207/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.6554e-04 - val_loss: 4.7636e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 208/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.6532e-04 - val_loss: 4.7550e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 209/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.6490e-04 - val_loss: 4.7492e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 210/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.6447e-04 - val_loss: 4.7468e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 211/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.6400e-04 - val_loss: 4.7458e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 212/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.6354e-04 - val_loss: 4.7394e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 213/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.6310e-04 - val_loss: 4.7357e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 214/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.6264e-04 - val_loss: 4.7337e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 215/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.6218e-04 - val_loss: 4.7306e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 216/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.6176e-04 - val_loss: 4.7262e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 217/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.6134e-04 - val_loss: 4.7249e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 218/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.6093e-04 - val_loss: 4.7210e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 219/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.6051e-04 - val_loss: 4.7236e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 220/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.6009e-04 - val_loss: 4.7231e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 221/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.5967e-04 - val_loss: 4.7208e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 222/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.5926e-04 - val_loss: 4.7167e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 223/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.5884e-04 - val_loss: 4.7110e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 224/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.5845e-04 - val_loss: 4.7075e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 225/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.5804e-04 - val_loss: 4.7029e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 226/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.5763e-04 - val_loss: 4.6997e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 227/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 4.5721e-04 - val_loss: 4.6870e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 228/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.5681e-04 - val_loss: 4.6831e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 229/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.5641e-04 - val_loss: 4.6800e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 230/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 4.5601e-04 - val_loss: 4.6714e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 231/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.5560e-04 - val_loss: 4.6666e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 232/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.5522e-04 - val_loss: 4.6598e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 233/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.5483e-04 - val_loss: 4.6562e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 234/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.5445e-04 - val_loss: 4.6507e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 235/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.5407e-04 - val_loss: 4.6463e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 236/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.5368e-04 - val_loss: 4.6393e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 237/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.5330e-04 - val_loss: 4.6370e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 238/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.5293e-04 - val_loss: 4.6330e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 239/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.5255e-04 - val_loss: 4.6291e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 240/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.5218e-04 - val_loss: 4.6201e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 241/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.5182e-04 - val_loss: 4.6124e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 242/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.5144e-04 - val_loss: 4.6092e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 243/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.5109e-04 - val_loss: 4.6072e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 244/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.5071e-04 - val_loss: 4.6007e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 245/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.5036e-04 - val_loss: 4.5944e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 246/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.4998e-04 - val_loss: 4.5880e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 247/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.4963e-04 - val_loss: 4.5836e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 248/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.4931e-04 - val_loss: 4.5797e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 249/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.4895e-04 - val_loss: 4.5762e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 250/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.4862e-04 - val_loss: 4.5743e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 251/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.4828e-04 - val_loss: 4.5712e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 252/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.4797e-04 - val_loss: 4.5703e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 253/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 4.4765e-04 - val_loss: 4.5693e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 254/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.4731e-04 - val_loss: 4.5671e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 255/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.4453e-04 - val_loss: 4.4988e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 256/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.4428e-04 - val_loss: 4.4993e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 257/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.4416e-04 - val_loss: 4.5050e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 258/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.4405e-04 - val_loss: 4.5128e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 259/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.4388e-04 - val_loss: 4.5166e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 260/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.4367e-04 - val_loss: 4.5173e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 261/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.4344e-04 - val_loss: 4.5177e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 262/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.4322e-04 - val_loss: 4.5178e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 263/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.4299e-04 - val_loss: 4.5186e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 264/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.4276e-04 - val_loss: 4.5183e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 265/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.4253e-04 - val_loss: 4.5175e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 266/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.4230e-04 - val_loss: 4.5168e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 267/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.4207e-04 - val_loss: 4.5150e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 268/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.4184e-04 - val_loss: 4.5143e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 269/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.4162e-04 - val_loss: 4.5151e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 270/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.4139e-04 - val_loss: 4.5126e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 271/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.4116e-04 - val_loss: 4.5121e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 272/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.4094e-04 - val_loss: 4.5116e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 273/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.4071e-04 - val_loss: 4.5116e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 274/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.4049e-04 - val_loss: 4.5102e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 275/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.4026e-04 - val_loss: 4.5098e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 276/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.4005e-04 - val_loss: 4.5083e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 277/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3983e-04 - val_loss: 4.5059e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 278/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.3960e-04 - val_loss: 4.5046e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 279/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.3939e-04 - val_loss: 4.5030e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 280/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.3916e-04 - val_loss: 4.5014e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 281/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.3895e-04 - val_loss: 4.5009e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 282/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.3872e-04 - val_loss: 4.4999e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 283/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.3851e-04 - val_loss: 4.5014e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 284/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.3829e-04 - val_loss: 4.5003e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 285/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3808e-04 - val_loss: 4.5005e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 286/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.3786e-04 - val_loss: 4.4997e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 287/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 4.3764e-04 - val_loss: 4.4991e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 288/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.3742e-04 - val_loss: 4.4988e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 289/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.3720e-04 - val_loss: 4.4971e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 290/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.3698e-04 - val_loss: 4.4975e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 291/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.3676e-04 - val_loss: 4.4956e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 292/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.3655e-04 - val_loss: 4.4936e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 293/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3633e-04 - val_loss: 4.4927e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 294/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.3612e-04 - val_loss: 4.4914e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 295/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.3591e-04 - val_loss: 4.4896e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 296/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.3569e-04 - val_loss: 4.4879e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 297/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.3548e-04 - val_loss: 4.4870e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 298/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.3526e-04 - val_loss: 4.4838e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 299/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3504e-04 - val_loss: 4.4814e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 300/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.3483e-04 - val_loss: 4.4779e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 301/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.3461e-04 - val_loss: 4.4714e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 302/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.3439e-04 - val_loss: 4.4656e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 303/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.3417e-04 - val_loss: 4.4610e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 304/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.3395e-04 - val_loss: 4.4552e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 305/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.3256e-04 - val_loss: 4.4054e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 306/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.3253e-04 - val_loss: 4.3964e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 307/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.3249e-04 - val_loss: 4.3912e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 308/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.3241e-04 - val_loss: 4.3889e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 309/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.3230e-04 - val_loss: 4.3877e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 310/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.3218e-04 - val_loss: 4.3867e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 311/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.3206e-04 - val_loss: 4.3860e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 312/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.3193e-04 - val_loss: 4.3852e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 313/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.3180e-04 - val_loss: 4.3843e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 314/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.3167e-04 - val_loss: 4.3837e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 315/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.3155e-04 - val_loss: 4.3827e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 316/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.3142e-04 - val_loss: 4.3820e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 317/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.3129e-04 - val_loss: 4.3811e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 318/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.3117e-04 - val_loss: 4.3804e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 319/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.3104e-04 - val_loss: 4.3797e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 320/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.3091e-04 - val_loss: 4.3789e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 321/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.3079e-04 - val_loss: 4.3777e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 322/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.3066e-04 - val_loss: 4.3771e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 323/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.3054e-04 - val_loss: 4.3766e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 324/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.3041e-04 - val_loss: 4.3758e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 325/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.3029e-04 - val_loss: 4.3749e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 326/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.3016e-04 - val_loss: 4.3741e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 327/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 4.3003e-04 - val_loss: 4.3736e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 328/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.2991e-04 - val_loss: 4.3730e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 329/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.2978e-04 - val_loss: 4.3718e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 330/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2966e-04 - val_loss: 4.3708e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 331/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.2953e-04 - val_loss: 4.3700e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 332/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2941e-04 - val_loss: 4.3693e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 333/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2928e-04 - val_loss: 4.3686e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 334/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.2916e-04 - val_loss: 4.3676e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 335/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.2903e-04 - val_loss: 4.3666e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 336/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.2891e-04 - val_loss: 4.3654e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 337/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2878e-04 - val_loss: 4.3647e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 338/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 4.2866e-04 - val_loss: 4.3633e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 339/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.2853e-04 - val_loss: 4.3623e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 340/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.2840e-04 - val_loss: 4.3607e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 341/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.2827e-04 - val_loss: 4.3589e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 342/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2815e-04 - val_loss: 4.3577e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 343/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 4.2802e-04 - val_loss: 4.3564e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 344/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2789e-04 - val_loss: 4.3549e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 345/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.2777e-04 - val_loss: 4.3528e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 346/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.2764e-04 - val_loss: 4.3506e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 347/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.2752e-04 - val_loss: 4.3481e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 348/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2739e-04 - val_loss: 4.3458e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 349/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.2727e-04 - val_loss: 4.3433e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 350/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.2714e-04 - val_loss: 4.3410e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 351/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2702e-04 - val_loss: 4.3386e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 352/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.2690e-04 - val_loss: 4.3367e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 353/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.2677e-04 - val_loss: 4.3350e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 354/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.2665e-04 - val_loss: 4.3334e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 355/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.2580e-04 - val_loss: 4.3504e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 356/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2577e-04 - val_loss: 4.3457e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 357/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.2575e-04 - val_loss: 4.3426e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 358/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.2570e-04 - val_loss: 4.3408e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 359/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.2563e-04 - val_loss: 4.3395e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 360/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.2555e-04 - val_loss: 4.3378e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 361/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.2547e-04 - val_loss: 4.3365e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 362/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.2539e-04 - val_loss: 4.3354e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 363/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.2531e-04 - val_loss: 4.3341e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 364/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.2523e-04 - val_loss: 4.3329e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 365/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.2515e-04 - val_loss: 4.3312e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 366/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.2507e-04 - val_loss: 4.3298e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 367/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.2499e-04 - val_loss: 4.3286e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 368/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.2490e-04 - val_loss: 4.3277e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 369/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.2482e-04 - val_loss: 4.3266e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 370/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.2474e-04 - val_loss: 4.3253e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 371/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.2466e-04 - val_loss: 4.3241e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 372/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2458e-04 - val_loss: 4.3231e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 373/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.2450e-04 - val_loss: 4.3221e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 374/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.2442e-04 - val_loss: 4.3209e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 375/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 4.2434e-04 - val_loss: 4.3196e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 376/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.2426e-04 - val_loss: 4.3182e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 377/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.2418e-04 - val_loss: 4.3168e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 378/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 4.2410e-04 - val_loss: 4.3155e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 379/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.2402e-04 - val_loss: 4.3138e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 380/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 4.2394e-04 - val_loss: 4.3124e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 381/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.2385e-04 - val_loss: 4.3111e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 382/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.2377e-04 - val_loss: 4.3100e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 383/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.2369e-04 - val_loss: 4.3088e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 384/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2361e-04 - val_loss: 4.3077e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 385/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.2353e-04 - val_loss: 4.3066e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 386/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.2345e-04 - val_loss: 4.3055e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 387/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.2337e-04 - val_loss: 4.3043e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 388/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.2329e-04 - val_loss: 4.3031e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 389/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.2321e-04 - val_loss: 4.3022e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 390/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2314e-04 - val_loss: 4.3011e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 391/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2306e-04 - val_loss: 4.2999e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 392/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.2298e-04 - val_loss: 4.2990e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 393/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 4.2290e-04 - val_loss: 4.2980e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 394/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.2283e-04 - val_loss: 4.2968e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 395/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.2275e-04 - val_loss: 4.2958e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 396/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.2267e-04 - val_loss: 4.2948e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 397/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2260e-04 - val_loss: 4.2936e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 398/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2252e-04 - val_loss: 4.2924e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 399/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2244e-04 - val_loss: 4.2915e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 400/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.2237e-04 - val_loss: 4.2908e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 401/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.2229e-04 - val_loss: 4.2898e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 402/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.2221e-04 - val_loss: 4.2888e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 403/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 4.2214e-04 - val_loss: 4.2880e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 404/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2206e-04 - val_loss: 4.2872e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 405/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.2169e-04 - val_loss: 4.2809e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 406/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 4.2167e-04 - val_loss: 4.2807e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 407/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2164e-04 - val_loss: 4.2805e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 408/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.2160e-04 - val_loss: 4.2803e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 409/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.2156e-04 - val_loss: 4.2800e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 410/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.2152e-04 - val_loss: 4.2797e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 411/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.2147e-04 - val_loss: 4.2795e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 412/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.2143e-04 - val_loss: 4.2793e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 413/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.2138e-04 - val_loss: 4.2790e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 414/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2134e-04 - val_loss: 4.2786e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 415/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 4.2129e-04 - val_loss: 4.2784e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 416/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.2125e-04 - val_loss: 4.2780e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 417/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.2120e-04 - val_loss: 4.2776e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 418/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.2116e-04 - val_loss: 4.2772e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 419/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.2111e-04 - val_loss: 4.2769e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 420/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.2107e-04 - val_loss: 4.2766e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 421/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.2103e-04 - val_loss: 4.2762e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 422/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2098e-04 - val_loss: 4.2758e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 423/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.2094e-04 - val_loss: 4.2754e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 424/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 4.2089e-04 - val_loss: 4.2750e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 425/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.2085e-04 - val_loss: 4.2746e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 426/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.2080e-04 - val_loss: 4.2742e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 427/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.2076e-04 - val_loss: 4.2738e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 428/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.2071e-04 - val_loss: 4.2734e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 429/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.2067e-04 - val_loss: 4.2729e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 430/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.2062e-04 - val_loss: 4.2724e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 431/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.2058e-04 - val_loss: 4.2719e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 432/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.2053e-04 - val_loss: 4.2716e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 433/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.2049e-04 - val_loss: 4.2712e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 434/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.2045e-04 - val_loss: 4.2707e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 435/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2040e-04 - val_loss: 4.2703e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 436/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2036e-04 - val_loss: 4.2698e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 437/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.2031e-04 - val_loss: 4.2693e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 438/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2027e-04 - val_loss: 4.2689e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 439/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2022e-04 - val_loss: 4.2685e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 440/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2018e-04 - val_loss: 4.2680e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 441/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2014e-04 - val_loss: 4.2676e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 442/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2009e-04 - val_loss: 4.2672e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 443/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.2005e-04 - val_loss: 4.2667e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 444/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.2000e-04 - val_loss: 4.2664e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 445/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1996e-04 - val_loss: 4.2660e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 446/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1992e-04 - val_loss: 4.2656e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 447/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1987e-04 - val_loss: 4.2652e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 448/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1983e-04 - val_loss: 4.2648e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 449/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1978e-04 - val_loss: 4.2644e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 450/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1974e-04 - val_loss: 4.2639e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 451/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1970e-04 - val_loss: 4.2635e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 452/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1965e-04 - val_loss: 4.2630e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 453/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1961e-04 - val_loss: 4.2626e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 454/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1957e-04 - val_loss: 4.2621e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 455/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.1938e-04 - val_loss: 4.2531e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 456/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1936e-04 - val_loss: 4.2534e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 457/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1934e-04 - val_loss: 4.2534e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 458/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1932e-04 - val_loss: 4.2532e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 459/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1929e-04 - val_loss: 4.2530e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 460/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1927e-04 - val_loss: 4.2528e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 461/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1925e-04 - val_loss: 4.2526e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 462/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1922e-04 - val_loss: 4.2523e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 463/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1920e-04 - val_loss: 4.2521e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 464/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 4.1917e-04 - val_loss: 4.2519e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 465/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1915e-04 - val_loss: 4.2517e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 466/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1912e-04 - val_loss: 4.2515e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 467/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1910e-04 - val_loss: 4.2512e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 468/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1907e-04 - val_loss: 4.2509e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 469/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1905e-04 - val_loss: 4.2506e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 470/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 4.1902e-04 - val_loss: 4.2504e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 471/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1900e-04 - val_loss: 4.2501e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 472/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1897e-04 - val_loss: 4.2499e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 473/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1895e-04 - val_loss: 4.2497e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 474/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1892e-04 - val_loss: 4.2494e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 475/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 4.1890e-04 - val_loss: 4.2492e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 476/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1887e-04 - val_loss: 4.2489e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 477/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1885e-04 - val_loss: 4.2487e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 478/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1882e-04 - val_loss: 4.2484e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 479/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1880e-04 - val_loss: 4.2481e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 480/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1877e-04 - val_loss: 4.2479e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 481/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1875e-04 - val_loss: 4.2476e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 482/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.1872e-04 - val_loss: 4.2474e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 483/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1870e-04 - val_loss: 4.2471e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 484/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 4.1867e-04 - val_loss: 4.2468e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 485/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1865e-04 - val_loss: 4.2466e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 486/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1862e-04 - val_loss: 4.2463e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 487/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1860e-04 - val_loss: 4.2460e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 488/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1857e-04 - val_loss: 4.2458e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 489/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1855e-04 - val_loss: 4.2455e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 490/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1853e-04 - val_loss: 4.2453e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 491/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1850e-04 - val_loss: 4.2450e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 492/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1848e-04 - val_loss: 4.2448e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 493/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1845e-04 - val_loss: 4.2445e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 494/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1843e-04 - val_loss: 4.2443e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 495/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1840e-04 - val_loss: 4.2440e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 496/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 4.1838e-04 - val_loss: 4.2438e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 497/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1835e-04 - val_loss: 4.2436e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 498/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1833e-04 - val_loss: 4.2434e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 499/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1830e-04 - val_loss: 4.2432e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 500/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1828e-04 - val_loss: 4.2430e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 501/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1825e-04 - val_loss: 4.2427e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 502/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1823e-04 - val_loss: 4.2425e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 503/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1821e-04 - val_loss: 4.2422e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 504/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1818e-04 - val_loss: 4.2420e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 505/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1806e-04 - val_loss: 4.2359e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 506/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 4.1805e-04 - val_loss: 4.2357e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 507/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1804e-04 - val_loss: 4.2356e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 508/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1803e-04 - val_loss: 4.2355e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 509/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1801e-04 - val_loss: 4.2354e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 510/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.1800e-04 - val_loss: 4.2353e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 511/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 4.1799e-04 - val_loss: 4.2351e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 512/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1797e-04 - val_loss: 4.2350e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 513/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1796e-04 - val_loss: 4.2349e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 514/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1794e-04 - val_loss: 4.2347e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 515/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1793e-04 - val_loss: 4.2346e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 516/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1792e-04 - val_loss: 4.2345e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 517/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1790e-04 - val_loss: 4.2344e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 518/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1789e-04 - val_loss: 4.2342e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 519/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1787e-04 - val_loss: 4.2341e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 520/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1786e-04 - val_loss: 4.2340e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 521/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1785e-04 - val_loss: 4.2339e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 522/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1783e-04 - val_loss: 4.2337e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 523/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 4.1782e-04 - val_loss: 4.2336e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 524/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.1780e-04 - val_loss: 4.2335e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 525/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1779e-04 - val_loss: 4.2333e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 526/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1778e-04 - val_loss: 4.2332e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 527/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1776e-04 - val_loss: 4.2331e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 528/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1775e-04 - val_loss: 4.2330e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 529/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1774e-04 - val_loss: 4.2328e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 530/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1772e-04 - val_loss: 4.2327e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 531/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1771e-04 - val_loss: 4.2326e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 532/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1769e-04 - val_loss: 4.2324e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 533/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.1768e-04 - val_loss: 4.2323e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 534/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1767e-04 - val_loss: 4.2322e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 535/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1765e-04 - val_loss: 4.2321e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 536/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1764e-04 - val_loss: 4.2319e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 537/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1762e-04 - val_loss: 4.2318e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 538/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1761e-04 - val_loss: 4.2317e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 539/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1760e-04 - val_loss: 4.2316e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 540/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1758e-04 - val_loss: 4.2314e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 541/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1757e-04 - val_loss: 4.2313e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 542/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1755e-04 - val_loss: 4.2312e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 543/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1754e-04 - val_loss: 4.2310e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 544/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.1753e-04 - val_loss: 4.2309e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 545/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1751e-04 - val_loss: 4.2307e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 546/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1750e-04 - val_loss: 4.2306e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 547/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1749e-04 - val_loss: 4.2305e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 548/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1747e-04 - val_loss: 4.2303e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 549/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1746e-04 - val_loss: 4.2302e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 550/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1744e-04 - val_loss: 4.2301e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 551/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1743e-04 - val_loss: 4.2299e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 552/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1742e-04 - val_loss: 4.2298e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 553/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1740e-04 - val_loss: 4.2296e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 554/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1739e-04 - val_loss: 4.2295e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 555/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1733e-04 - val_loss: 4.2294e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 556/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.1732e-04 - val_loss: 4.2293e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 557/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1732e-04 - val_loss: 4.2292e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 558/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1731e-04 - val_loss: 4.2291e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 559/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1730e-04 - val_loss: 4.2290e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 560/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1729e-04 - val_loss: 4.2289e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 561/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1729e-04 - val_loss: 4.2289e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 562/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1728e-04 - val_loss: 4.2288e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 563/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.1727e-04 - val_loss: 4.2287e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 564/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1726e-04 - val_loss: 4.2286e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 565/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1726e-04 - val_loss: 4.2285e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 566/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1725e-04 - val_loss: 4.2285e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 567/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.1724e-04 - val_loss: 4.2284e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 568/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1723e-04 - val_loss: 4.2283e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 569/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.1723e-04 - val_loss: 4.2282e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 570/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1722e-04 - val_loss: 4.2281e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 571/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1721e-04 - val_loss: 4.2281e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 572/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1720e-04 - val_loss: 4.2280e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 573/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1720e-04 - val_loss: 4.2279e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 574/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1719e-04 - val_loss: 4.2278e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 575/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1718e-04 - val_loss: 4.2278e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 576/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1717e-04 - val_loss: 4.2277e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 577/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1717e-04 - val_loss: 4.2276e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 578/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1716e-04 - val_loss: 4.2275e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 579/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1715e-04 - val_loss: 4.2275e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 580/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1714e-04 - val_loss: 4.2274e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 581/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1714e-04 - val_loss: 4.2273e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 582/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1713e-04 - val_loss: 4.2272e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 583/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1712e-04 - val_loss: 4.2271e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 584/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1711e-04 - val_loss: 4.2271e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 585/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1711e-04 - val_loss: 4.2270e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 586/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1710e-04 - val_loss: 4.2269e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 587/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1709e-04 - val_loss: 4.2268e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 588/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.1708e-04 - val_loss: 4.2267e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 589/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1708e-04 - val_loss: 4.2267e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 590/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1707e-04 - val_loss: 4.2266e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 591/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 4.1706e-04 - val_loss: 4.2265e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 592/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1705e-04 - val_loss: 4.2265e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 593/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1705e-04 - val_loss: 4.2264e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 594/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1704e-04 - val_loss: 4.2263e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 595/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1703e-04 - val_loss: 4.2262e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 596/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.1702e-04 - val_loss: 4.2261e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 597/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1702e-04 - val_loss: 4.2261e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 598/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1701e-04 - val_loss: 4.2260e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 599/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1700e-04 - val_loss: 4.2259e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 600/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1699e-04 - val_loss: 4.2258e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 601/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1699e-04 - val_loss: 4.2258e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 602/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1698e-04 - val_loss: 4.2257e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 603/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1697e-04 - val_loss: 4.2256e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 604/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1696e-04 - val_loss: 4.2255e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 605/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 4.1694e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 606/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1693e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 607/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1693e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 608/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1693e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 609/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.1692e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 610/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.1692e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 611/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1691e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 612/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1691e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 613/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1691e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 614/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.1690e-04 - val_loss: 4.2226e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 615/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1690e-04 - val_loss: 4.2226e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 616/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1689e-04 - val_loss: 4.2226e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 617/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1689e-04 - val_loss: 4.2225e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 618/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.1689e-04 - val_loss: 4.2225e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 619/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.1688e-04 - val_loss: 4.2225e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 620/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1688e-04 - val_loss: 4.2224e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 621/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1687e-04 - val_loss: 4.2224e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 622/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1687e-04 - val_loss: 4.2223e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 623/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1687e-04 - val_loss: 4.2223e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 624/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1686e-04 - val_loss: 4.2223e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 625/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1686e-04 - val_loss: 4.2222e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 626/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.1686e-04 - val_loss: 4.2222e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 627/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.1685e-04 - val_loss: 4.2222e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 628/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1685e-04 - val_loss: 4.2221e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 629/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1684e-04 - val_loss: 4.2221e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 630/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1684e-04 - val_loss: 4.2220e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 631/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.1684e-04 - val_loss: 4.2220e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 632/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1683e-04 - val_loss: 4.2219e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 633/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1683e-04 - val_loss: 4.2219e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 634/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 4.1682e-04 - val_loss: 4.2218e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 635/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.1682e-04 - val_loss: 4.2218e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 636/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 4.1682e-04 - val_loss: 4.2218e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 637/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1681e-04 - val_loss: 4.2217e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 638/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.1681e-04 - val_loss: 4.2217e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 639/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1680e-04 - val_loss: 4.2216e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 640/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1680e-04 - val_loss: 4.2216e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 641/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1680e-04 - val_loss: 4.2215e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 642/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1679e-04 - val_loss: 4.2215e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 643/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1679e-04 - val_loss: 4.2215e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 644/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1679e-04 - val_loss: 4.2214e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 645/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1678e-04 - val_loss: 4.2214e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 646/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1678e-04 - val_loss: 4.2213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 647/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.1677e-04 - val_loss: 4.2213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 648/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1677e-04 - val_loss: 4.2213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 649/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1677e-04 - val_loss: 4.2212e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 650/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1676e-04 - val_loss: 4.2212e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 651/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1676e-04 - val_loss: 4.2212e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 652/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1675e-04 - val_loss: 4.2211e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 653/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1675e-04 - val_loss: 4.2211e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 654/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1675e-04 - val_loss: 4.2210e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 655/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1673e-04 - val_loss: 4.2187e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 656/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 657/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 658/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 659/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 660/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 661/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 662/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 663/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 664/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 665/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 666/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 667/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 668/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 669/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 670/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 671/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 672/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 673/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 674/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 675/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 676/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 677/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 678/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 679/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 680/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 681/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1668e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 682/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 683/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 684/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 685/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 686/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 687/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 688/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 689/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 690/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 691/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1666e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 692/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1665e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 693/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1665e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 694/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1665e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 695/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 4.1665e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 696/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1665e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 697/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1664e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 698/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1664e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 699/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1664e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 700/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1664e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 701/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1664e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 702/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1663e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 703/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1663e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 704/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1663e-04 - val_loss: 4.2176e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 705/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 706/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 707/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 708/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 709/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 710/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 711/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 712/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1662e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 713/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1662e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 714/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1662e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 715/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 716/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 717/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 718/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 719/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 720/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 721/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 722/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 723/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 724/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 725/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 726/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 727/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 728/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 729/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 730/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 731/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 732/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 733/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 734/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 735/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 736/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 737/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 738/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 739/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 740/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 741/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 742/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 743/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 744/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 745/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 746/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 747/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 748/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 749/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 750/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 751/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 752/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 753/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 754/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 755/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 756/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 757/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 758/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 759/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 760/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 761/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 762/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 763/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 764/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 765/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 766/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 767/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 768/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 769/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 770/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 771/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 772/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 773/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 774/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 775/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 776/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 777/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 778/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 779/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 780/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 781/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 782/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 783/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 784/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 785/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 786/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 787/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 788/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 789/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 790/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 791/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 792/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 793/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 794/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 795/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 796/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1657e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 797/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 798/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 799/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 800/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 801/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 802/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 803/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 804/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 805/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 806/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 807/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 808/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 809/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 810/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 811/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 812/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 813/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 814/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 815/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 816/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 817/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 818/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 819/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 820/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 821/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 822/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1657e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 823/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 4.1657e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 824/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1657e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 825/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 826/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 827/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 828/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 829/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 830/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 831/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 832/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 833/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 834/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 835/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 836/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 837/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 838/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 839/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 840/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 841/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 842/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 843/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 844/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 845/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 846/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 847/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 848/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 849/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 850/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 851/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 852/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 853/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step\n",
      "Model 1 Test Set RMSE: 0.002859 AU\n",
      "\n",
      "--- Training MLP Model 2 ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 0.0592 - val_loss: 0.0410 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0218 - val_loss: 0.0235 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0138 - val_loss: 0.0158 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0092 - val_loss: 0.0117 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0063 - val_loss: 0.0095 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0046 - val_loss: 0.0091 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0039 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.0031 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0027 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0024 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0020 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0020 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0020 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0017 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.0016 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0016 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.0016 - val_loss: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0014 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.0012 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.0013 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 0.0013 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0013 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0011 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0011 - val_loss: 9.6725e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0010 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 9.9506e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0011 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.0010 - val_loss: 9.6028e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0010 - val_loss: 9.2748e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 9.5384e-04 - val_loss: 9.0202e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 9.4500e-04 - val_loss: 9.2950e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0010 - val_loss: 9.1055e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 9.3765e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.0010 - val_loss: 9.0060e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 9.8892e-04 - val_loss: 8.8312e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 8.8856e-04 - val_loss: 9.2904e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0010 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 7.9893e-04 - val_loss: 8.0624e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.9332e-04 - val_loss: 7.9899e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 7.9372e-04 - val_loss: 8.5157e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 8.0808e-04 - val_loss: 8.9539e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 7.9338e-04 - val_loss: 8.4283e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 8.0979e-04 - val_loss: 7.9306e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.9577e-04 - val_loss: 8.8208e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.7623e-04 - val_loss: 7.9572e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 8.0173e-04 - val_loss: 8.7250e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 7.8750e-04 - val_loss: 8.2414e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 7.6900e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 7.7222e-04 - val_loss: 9.8606e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.6998e-04 - val_loss: 8.3467e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.6248e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.6343e-04 - val_loss: 7.7579e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.5951e-04 - val_loss: 7.9209e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 7.5479e-04 - val_loss: 7.8208e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 7.5080e-04 - val_loss: 8.1270e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.4514e-04 - val_loss: 8.5590e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.4205e-04 - val_loss: 8.6702e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 7.3874e-04 - val_loss: 8.5155e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 7.3490e-04 - val_loss: 8.6692e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 7.3119e-04 - val_loss: 8.8783e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.2800e-04 - val_loss: 8.6425e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.2456e-04 - val_loss: 8.4528e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.2092e-04 - val_loss: 8.6000e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.1825e-04 - val_loss: 8.5758e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.1398e-04 - val_loss: 8.8894e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 7.1119e-04 - val_loss: 8.6452e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.0844e-04 - val_loss: 8.0501e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 7.0560e-04 - val_loss: 7.8399e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.0134e-04 - val_loss: 8.1077e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.9395e-04 - val_loss: 8.0754e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.0757e-04 - val_loss: 7.0068e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.9248e-04 - val_loss: 7.0828e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 6.9064e-04 - val_loss: 7.0356e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.8868e-04 - val_loss: 6.9449e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.8698e-04 - val_loss: 7.0830e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 6.8285e-04 - val_loss: 7.0560e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.7996e-04 - val_loss: 7.0333e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.7860e-04 - val_loss: 7.2070e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 6.7319e-04 - val_loss: 7.0366e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 6.7646e-04 - val_loss: 6.8608e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.7062e-04 - val_loss: 7.1688e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.6318e-04 - val_loss: 6.6294e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.6224e-04 - val_loss: 7.0750e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.7923e-04 - val_loss: 6.5467e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 6.5443e-04 - val_loss: 6.7436e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 6.6949e-04 - val_loss: 6.5065e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.5350e-04 - val_loss: 6.5782e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.1554e-04 - val_loss: 6.1467e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 6.1204e-04 - val_loss: 6.1795e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 6.1296e-04 - val_loss: 6.1821e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 6.1554e-04 - val_loss: 6.2273e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.1546e-04 - val_loss: 6.2132e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 6.1171e-04 - val_loss: 6.2031e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.1323e-04 - val_loss: 6.1190e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.0789e-04 - val_loss: 6.1585e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.0939e-04 - val_loss: 6.0441e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.0509e-04 - val_loss: 6.0680e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.0464e-04 - val_loss: 6.0078e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.0252e-04 - val_loss: 6.0048e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.0071e-04 - val_loss: 5.9671e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.9839e-04 - val_loss: 5.9910e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.9849e-04 - val_loss: 5.9476e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 5.9481e-04 - val_loss: 5.9659e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.9572e-04 - val_loss: 5.9089e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.9199e-04 - val_loss: 5.9297e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.9218e-04 - val_loss: 5.8849e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.8888e-04 - val_loss: 5.9075e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 5.8872e-04 - val_loss: 5.8620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.8517e-04 - val_loss: 5.8642e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.8703e-04 - val_loss: 5.8176e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.8248e-04 - val_loss: 5.8265e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.8230e-04 - val_loss: 5.8057e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.8159e-04 - val_loss: 5.7919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 5.7897e-04 - val_loss: 5.7564e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.7540e-04 - val_loss: 5.7697e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.7732e-04 - val_loss: 5.7917e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.7264e-04 - val_loss: 5.7707e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 5.7330e-04 - val_loss: 5.7349e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.6862e-04 - val_loss: 5.7271e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.6998e-04 - val_loss: 5.7002e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 5.6759e-04 - val_loss: 5.7781e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 5.6640e-04 - val_loss: 5.8024e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 5.6468e-04 - val_loss: 5.8119e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.6361e-04 - val_loss: 5.8007e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.6225e-04 - val_loss: 5.7647e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.6022e-04 - val_loss: 5.6803e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 5.6057e-04 - val_loss: 5.7004e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.5843e-04 - val_loss: 5.6842e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.5602e-04 - val_loss: 5.6115e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.5541e-04 - val_loss: 5.5972e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.5409e-04 - val_loss: 5.5876e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.5275e-04 - val_loss: 5.5905e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 5.5127e-04 - val_loss: 5.5794e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 5.5036e-04 - val_loss: 5.5697e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 5.4948e-04 - val_loss: 5.5605e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 5.4891e-04 - val_loss: 5.5406e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.4792e-04 - val_loss: 5.5164e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430us/step\n",
      "Model 2 Test Set RMSE: 0.115300 AU\n",
      "\n",
      "--- Training MLP Model 3 ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step - loss: 0.0525 - val_loss: 0.0503 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0224 - val_loss: 0.0318 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0141 - val_loss: 0.0203 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0091 - val_loss: 0.0187 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0063 - val_loss: 0.0191 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0047 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0036 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.0028 - val_loss: 0.0087 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0024 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0022 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0019 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0022 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0016 - val_loss: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0016 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.0015 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.0014 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0017 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0012 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.0016 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0011 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.0011 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0011 - val_loss: 9.6447e-04 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 0.0010 - val_loss: 9.3234e-04 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 9.9642e-04 - val_loss: 9.5364e-04 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0010 - val_loss: 9.4387e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 9.1934e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 9.8708e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 0.0010 - val_loss: 9.6737e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 9.5339e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 9.5424e-04 - val_loss: 8.4752e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 9.6358e-04 - val_loss: 8.8428e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 9.5975e-04 - val_loss: 8.6761e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 9.4230e-04 - val_loss: 9.1710e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 9.2020e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 9.3411e-04 - val_loss: 8.9830e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 9.2636e-04 - val_loss: 8.8293e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 9.1449e-04 - val_loss: 9.1616e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 8.4992e-04 - val_loss: 9.2206e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 9.5978e-04 - val_loss: 9.6026e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 8.8596e-04 - val_loss: 9.8447e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 8.8123e-04 - val_loss: 7.9363e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.5280e-04 - val_loss: 7.5902e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 7.5166e-04 - val_loss: 7.5731e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 7.5998e-04 - val_loss: 7.6959e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 7.6668e-04 - val_loss: 7.6116e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.7181e-04 - val_loss: 8.2877e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.4822e-04 - val_loss: 7.3660e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 7.5455e-04 - val_loss: 7.3206e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 7.4333e-04 - val_loss: 7.2230e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 7.4921e-04 - val_loss: 7.2086e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.3751e-04 - val_loss: 7.2580e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 7.3144e-04 - val_loss: 7.1344e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 7.3486e-04 - val_loss: 7.2369e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.1480e-04 - val_loss: 7.2408e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 7.2922e-04 - val_loss: 7.1549e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 7.1399e-04 - val_loss: 7.0961e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.0400e-04 - val_loss: 7.2067e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 7.0797e-04 - val_loss: 9.1501e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 7.0423e-04 - val_loss: 8.6561e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 7.0041e-04 - val_loss: 6.9555e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 6.9259e-04 - val_loss: 7.7157e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.9279e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 6.9155e-04 - val_loss: 6.7980e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 6.8411e-04 - val_loss: 7.7542e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 6.8061e-04 - val_loss: 8.6355e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.8657e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 6.8163e-04 - val_loss: 7.0531e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 6.6760e-04 - val_loss: 7.5591e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 6.7592e-04 - val_loss: 9.9951e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 6.7330e-04 - val_loss: 6.8514e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 6.6330e-04 - val_loss: 7.0573e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 6.5667e-04 - val_loss: 7.3311e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 6.6101e-04 - val_loss: 7.4637e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.5450e-04 - val_loss: 7.1730e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 6.5056e-04 - val_loss: 7.2811e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 6.4935e-04 - val_loss: 7.1370e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 6.4875e-04 - val_loss: 6.9825e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 6.5010e-04 - val_loss: 6.7995e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 6.3979e-04 - val_loss: 8.0499e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 6.4367e-04 - val_loss: 6.9911e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.4409e-04 - val_loss: 6.4734e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 6.4001e-04 - val_loss: 6.4637e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.2439e-04 - val_loss: 6.9735e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 6.3919e-04 - val_loss: 6.5078e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 6.2344e-04 - val_loss: 6.9424e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 6.2757e-04 - val_loss: 6.8582e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.2290e-04 - val_loss: 7.0039e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 6.1995e-04 - val_loss: 6.6088e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.2128e-04 - val_loss: 7.6363e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.1675e-04 - val_loss: 6.7022e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 6.1668e-04 - val_loss: 7.7172e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.7606e-04 - val_loss: 6.2191e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.7274e-04 - val_loss: 6.1104e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.7351e-04 - val_loss: 5.9417e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.7554e-04 - val_loss: 6.0389e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.7484e-04 - val_loss: 6.1142e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 5.7499e-04 - val_loss: 6.0856e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.7097e-04 - val_loss: 6.1209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.7246e-04 - val_loss: 5.9409e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.6645e-04 - val_loss: 6.0480e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.6977e-04 - val_loss: 5.8881e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.6357e-04 - val_loss: 6.0170e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 5.6516e-04 - val_loss: 5.9054e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.6234e-04 - val_loss: 6.0092e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.6136e-04 - val_loss: 6.0102e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.5946e-04 - val_loss: 6.0436e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.5842e-04 - val_loss: 6.0098e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.5688e-04 - val_loss: 6.0215e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.5588e-04 - val_loss: 5.9515e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.5448e-04 - val_loss: 5.9706e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.5306e-04 - val_loss: 5.9492e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.5171e-04 - val_loss: 5.9841e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.5085e-04 - val_loss: 5.9402e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 5.4940e-04 - val_loss: 5.9925e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.4829e-04 - val_loss: 5.9478e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 5.4707e-04 - val_loss: 5.9529e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.4612e-04 - val_loss: 5.9044e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.4435e-04 - val_loss: 5.9431e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 5.4410e-04 - val_loss: 5.8876e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.4259e-04 - val_loss: 5.9079e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 5.4162e-04 - val_loss: 5.8632e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.4022e-04 - val_loss: 5.8595e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 5.3923e-04 - val_loss: 5.7632e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 5.3858e-04 - val_loss: 5.7505e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.3733e-04 - val_loss: 5.7088e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.3648e-04 - val_loss: 5.6809e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.3523e-04 - val_loss: 5.6326e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.3444e-04 - val_loss: 5.6183e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 5.3315e-04 - val_loss: 5.5566e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 5.3209e-04 - val_loss: 5.5313e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 5.3145e-04 - val_loss: 5.4845e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 5.3033e-04 - val_loss: 5.4687e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.2981e-04 - val_loss: 5.4457e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 5.2861e-04 - val_loss: 5.4807e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.2773e-04 - val_loss: 5.4749e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.2693e-04 - val_loss: 5.4706e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 5.2615e-04 - val_loss: 5.4299e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.2506e-04 - val_loss: 5.4115e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.2469e-04 - val_loss: 5.3795e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.2347e-04 - val_loss: 5.3586e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.2291e-04 - val_loss: 5.3325e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438us/step\n",
      "Model 3 Test Set RMSE: 0.137518 AU\n",
      "\n",
      "--- Final Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.091990 AU\n",
      "Ensemble Y-coordinate RMSE: 0.079537 AU\n",
      "Ensemble Z-coordinate RMSE: 0.040822 AU\n",
      "Overall Ensemble RMSE: 0.070783 AU\n",
      "\n",
      "--- Final Comparison ---\n",
      "Best Single MLP Overall RMSE: 0.003100 AU\n",
      "Multi-MLP Ensemble Overall RMSE: 0.070783 AU\n",
      "\n",
      "RESULT: The single MLP model remains the most accurate.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# --- MODEL CONFIGURATION ---\n",
    "NUM_MODELS = 3 # Number of MLP models in the ensemble\n",
    "# Set a high number of epochs since Early Stopping will handle when to stop\n",
    "EPOCHS = 5000   \n",
    "INPUT_SHAPE = X_train_scaled.shape[1] # Number of input features\n",
    "models = []\n",
    "y_pred_list = []\n",
    "\n",
    "# --- Define Callbacks for Training ---\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', # Monitors validation loss\n",
    "    patience=150,        # Stops training after 150 epochs of no improvement\n",
    "    restore_best_weights=True,\n",
    "    min_delta = 1e-7 \n",
    "    )\n",
    "\n",
    "lr_on_plateau_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss', # Monitors validation loss\n",
    "    factor=0.5,         # Reduces learning rate by 50%\n",
    "    patience=50,        # If no improvement for 50 epochs, reduce LR\n",
    "    min_lr=1e-7         # The minimum learning rate to allow\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, lr_on_plateau_callback]\n",
    "\n",
    "# --- 1. BUILD AND TRAIN THE INDIVIDUAL MLP MODELS ---\n",
    "print(f\"Building and training {NUM_MODELS} diverse MLP models...\")\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    # Use a different random seed for each model to ensure diverse weight initializations\n",
    "    tf.keras.utils.set_random_seed(i + 1)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.0001), \n",
    "              input_shape=(INPUT_SHAPE,)), \n",
    "        \n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.0001)), \n",
    "        \n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "        Dense(3, activation='linear') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"\\n--- Training MLP Model {i+1} ---\")\n",
    "    model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train.values, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(X_test_scaled, y_test.values),\n",
    "        callbacks=callbacks, # Pass the list of callbacks here\n",
    "        verbose=1 # Changed to 1 to show the callback messages\n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    # Generate predictions for the current model and store them\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    # Calculate and print the individual model's RMSE for comparison\n",
    "    rmse_x = np.sqrt(mean_squared_error(y_test.values[:, 0], y_pred[:, 0]))\n",
    "    rmse_y = np.sqrt(mean_squared_error(y_test.values[:, 1], y_pred[:, 1]))\n",
    "    rmse_z = np.sqrt(mean_squared_error(y_test.values[:, 2], y_pred[:, 2]))\n",
    "    overall_rmse = np.mean([rmse_x, rmse_y, rmse_z])\n",
    "    \n",
    "    print(f\"Model {i+1} Test Set RMSE: {overall_rmse:.6f} AU\")\n",
    "\n",
    "# --- 2. CREATE THE ENSEMBLE PREDICTION ---\n",
    "# The ensemble prediction is the simple average of all individual model predictions.\n",
    "y_ensemble_pred_au = np.mean(y_pred_list, axis=0)\n",
    "\n",
    "# --- 3. EVALUATE THE ENSEMBLE PERFORMANCE ---\n",
    "print(\"\\n--- Final Ensemble Evaluation ---\")\n",
    "rmse_x_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 0], y_ensemble_pred_au[:, 0]))\n",
    "rmse_y_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 1], y_ensemble_pred_au[:, 1]))\n",
    "rmse_z_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 2], y_ensemble_pred_au[:, 2]))\n",
    "overall_ensemble_rmse = np.mean([rmse_x_ensemble, rmse_y_ensemble, rmse_z_ensemble])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {rmse_z_ensemble:.6f} AU\")\n",
    "print(f\"Overall Ensemble RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# --- 4. Compare with your best single MLP result ---\n",
    "# Replace the value below with your actual best MLP RMSE (approx. 0.0031)\n",
    "BEST_SINGLE_MLP_RMSE = 0.003100\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "print(f\"Best Single MLP Overall RMSE: {BEST_SINGLE_MLP_RMSE:.6f} AU\")\n",
    "print(f\"Multi-MLP Ensemble Overall RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# Determine if the ensemble is better\n",
    "if overall_ensemble_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(\"\\nSUCCESS: The multi-model MLP ensemble is more accurate!\")\n",
    "else:\n",
    "    print(\"\\nRESULT: The single MLP model remains the most accurate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to models/mars_position_predictor_mm1.keras\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'models'\n",
    "MODEL_FILENAME = 'mars_position_predictor_mm1.keras' \n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# os.makedirs(MODEL_DIR, exist_ok=True) \n",
    "\n",
    "# Save the model\n",
    "models[0].save(os.path.join(MODEL_DIR, MODEL_FILENAME))\n",
    "\n",
    "print(f\"Model saved successfully to {os.path.join(MODEL_DIR, MODEL_FILENAME)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MM1 Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.003033 AU\n",
      "Ensemble Y-coordinate RMSE: 0.003884 AU\n",
      "Ensemble Z-coordinate RMSE: 0.001660 AU\n",
      "Overall Ensemble RMSE: 0.002859 AU\n"
     ]
    }
   ],
   "source": [
    "y_pred_mm1 = y_pred_list[0]\n",
    "print(\"\\n--- MM1 Ensemble Evaluation ---\")\n",
    "rmse_x_mm1 = np.sqrt(mean_squared_error(y_test.values[:, 0], y_pred_mm1[:, 0]))\n",
    "rmse_y_mm1 = np.sqrt(mean_squared_error(y_test.values[:, 1], y_pred_mm1[:, 1]))\n",
    "rmse_z_mm1 = np.sqrt(mean_squared_error(y_test.values[:, 2], y_pred_mm1[:, 2]))\n",
    "overall_mm1_rmse = np.mean([rmse_x_mm1, rmse_y_mm1, rmse_z_mm1])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_mm1:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_mm1:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {rmse_z_mm1:.6f} AU\")\n",
    "print(f\"Overall Ensemble RMSE: {overall_mm1_rmse:.6f} AU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.002818 AU\n",
      "Ensemble Y-coordinate RMSE: 0.003294 AU\n",
      "Ensemble Z-coordinate RMSE: 0.001294 AU\n",
      "Overall Ensemble RMSE: 0.002468 AU\n",
      "\n",
      "--- Final Comparison ---\n",
      "Best Single MLP Overall RMSE: 0.003100 AU\n",
      "Multi-MLP Ensemble Overall RMSE: 0.002468 AU\n",
      "\n",
      "SUCCESS: The multi-model MLP ensemble is more accurate!\n"
     ]
    }
   ],
   "source": [
    "y_pred_mm1 = y_pred_list[0]\n",
    "y_pred_list_ens = [y_pred_mlp_au, y_pred_mm1]\n",
    "y_ensemble_pred_au = np.mean(y_pred_list_ens, axis=0)\n",
    "\n",
    "# --- 3. EVALUATE THE ENSEMBLE PERFORMANCE ---\n",
    "print(\"\\n--- Final Ensemble Evaluation ---\")\n",
    "rmse_x_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 0], y_ensemble_pred_au[:, 0]))\n",
    "rmse_y_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 1], y_ensemble_pred_au[:, 1]))\n",
    "rmse_z_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 2], y_ensemble_pred_au[:, 2]))\n",
    "overall_ensemble_rmse = np.mean([rmse_x_ensemble, rmse_y_ensemble, rmse_z_ensemble])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {rmse_z_ensemble:.6f} AU\")\n",
    "print(f\"Overall Ensemble RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# --- 4. Compare with your best single MLP result ---\n",
    "# Replace the value below with your actual best MLP RMSE (approx. 0.0031)\n",
    "BEST_SINGLE_MLP_RMSE = 0.003100\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "print(f\"Best Single MLP Overall RMSE: {BEST_SINGLE_MLP_RMSE:.6f} AU\")\n",
    "print(f\"Multi-MLP Ensemble Overall RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# Determine if the ensemble is better\n",
    "if overall_ensemble_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(\"\\nSUCCESS: The multi-model MLP ensemble is more accurate!\")\n",
    "else:\n",
    "    print(\"\\nRESULT: The single MLP model remains the most accurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training 9 diverse MLP models...\n",
      "\n",
      "--- Training MLP Model 1 (Seed: 1) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 0.0412 - val_loss: 0.0485 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0168 - val_loss: 0.0272 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0113 - val_loss: 0.0230 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0078 - val_loss: 0.0124 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.0054 - val_loss: 0.0110 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0042 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0032 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.0024 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0020 - val_loss: 0.0036 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.0021 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 0.0016 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0014 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0012 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.0010 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0011 - val_loss: 0.0080 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0012 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 9.2339e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 9.7430e-04 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 8.9090e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 7.4875e-04 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 7.7930e-04 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 8.0347e-04 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 6.8546e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 7.4981e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.4018e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 7.1359e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 5.7125e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 6.6180e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 5.8931e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 6.5278e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 6.0160e-04 - val_loss: 8.2045e-04 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 5.6915e-04 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 6.3849e-04 - val_loss: 7.8076e-04 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.3437e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.8714e-04 - val_loss: 8.2216e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 6.4374e-04 - val_loss: 7.4014e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.0152e-04 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 4.8371e-04 - val_loss: 8.7775e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.7135e-04 - val_loss: 8.5873e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 5.5977e-04 - val_loss: 8.0873e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 4.6580e-04 - val_loss: 8.3728e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 5.4529e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.8237e-04 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 5.3524e-04 - val_loss: 9.4104e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 5.0240e-04 - val_loss: 8.7620e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.7819e-04 - val_loss: 5.2658e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 3.7657e-04 - val_loss: 8.8834e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.8630e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.6223e-04 - val_loss: 4.0583e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.4082e-04 - val_loss: 6.0891e-04 - learning_rate: 0.0010\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 4.1890e-04 - val_loss: 4.7670e-04 - learning_rate: 0.0010\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.3647e-04 - val_loss: 8.5538e-04 - learning_rate: 0.0010\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.6811e-04 - val_loss: 8.8056e-04 - learning_rate: 0.0010\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.5300e-04 - val_loss: 5.0194e-04 - learning_rate: 0.0010\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 3.8392e-04 - val_loss: 3.5732e-04 - learning_rate: 0.0010\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.4916e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.2742e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.3806e-04 - val_loss: 5.1535e-04 - learning_rate: 0.0010\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 4.1778e-04 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.9418e-04 - val_loss: 6.4371e-04 - learning_rate: 0.0010\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 3.5213e-04 - val_loss: 8.2233e-04 - learning_rate: 0.0010\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.5159e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 4.0488e-04 - val_loss: 8.7587e-04 - learning_rate: 0.0010\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.1632e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.7497e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 4.3233e-04 - val_loss: 4.1924e-04 - learning_rate: 0.0010\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 3.4877e-04 - val_loss: 9.3383e-04 - learning_rate: 0.0010\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.8499e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.6683e-04 - val_loss: 4.3187e-04 - learning_rate: 0.0010\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.6133e-04 - val_loss: 6.0007e-04 - learning_rate: 0.0010\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 3.9743e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.3471e-04 - val_loss: 7.5996e-04 - learning_rate: 0.0010\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 3.5581e-04 - val_loss: 4.4204e-04 - learning_rate: 0.0010\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.7829e-04 - val_loss: 5.6632e-04 - learning_rate: 0.0010\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 4.4011e-04 - val_loss: 4.6414e-04 - learning_rate: 0.0010\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.4552e-04 - val_loss: 8.7712e-04 - learning_rate: 0.0010\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.6937e-04 - val_loss: 4.2544e-04 - learning_rate: 0.0010\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.6037e-04 - val_loss: 4.2696e-04 - learning_rate: 0.0010\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 3.9003e-04 - val_loss: 7.4650e-04 - learning_rate: 0.0010\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 3.4991e-04 - val_loss: 4.2537e-04 - learning_rate: 0.0010\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 4.0387e-04 - val_loss: 9.1204e-04 - learning_rate: 0.0010\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.6696e-04 - val_loss: 0.0173 - learning_rate: 0.0010\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 5.9436e-04 - val_loss: 9.4739e-04 - learning_rate: 0.0010\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.9925e-04 - val_loss: 5.7407e-04 - learning_rate: 0.0010\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.0081e-04 - val_loss: 4.2496e-04 - learning_rate: 0.0010\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 3.4546e-04 - val_loss: 5.9697e-04 - learning_rate: 0.0010\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 3.9975e-04 - val_loss: 4.1951e-04 - learning_rate: 0.0010\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.8781e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.2560e-04 - val_loss: 3.4350e-04 - learning_rate: 0.0010\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.4985e-04 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 3.9316e-04 - val_loss: 6.4937e-04 - learning_rate: 0.0010\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.1279e-04 - val_loss: 7.0129e-04 - learning_rate: 0.0010\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 3.6788e-04 - val_loss: 5.5537e-04 - learning_rate: 0.0010\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.0899e-04 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 3.2640e-04 - val_loss: 6.0381e-04 - learning_rate: 0.0010\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.6288e-04 - val_loss: 5.4773e-04 - learning_rate: 0.0010\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 3.9127e-04 - val_loss: 6.6885e-04 - learning_rate: 0.0010\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.3393e-04 - val_loss: 6.7518e-04 - learning_rate: 0.0010\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 3.6203e-04 - val_loss: 4.9239e-04 - learning_rate: 0.0010\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.6174e-04 - val_loss: 3.0089e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.3407e-04 - val_loss: 2.8897e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 2.5012e-04 - val_loss: 3.2875e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.3969e-04 - val_loss: 3.0975e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.7128e-04 - val_loss: 4.1991e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.4148e-04 - val_loss: 3.0065e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.4939e-04 - val_loss: 3.7702e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.4074e-04 - val_loss: 3.9617e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.5486e-04 - val_loss: 5.0016e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.4177e-04 - val_loss: 3.2871e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.4067e-04 - val_loss: 3.8528e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.4927e-04 - val_loss: 4.2579e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.5238e-04 - val_loss: 4.3455e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 2.3964e-04 - val_loss: 2.8371e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.3777e-04 - val_loss: 2.9818e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.3742e-04 - val_loss: 3.6397e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.4266e-04 - val_loss: 6.2386e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.4029e-04 - val_loss: 5.0446e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.3868e-04 - val_loss: 3.2005e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 2.3734e-04 - val_loss: 2.5031e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.2852e-04 - val_loss: 3.3586e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.2117e-04 - val_loss: 2.8414e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.4056e-04 - val_loss: 2.5518e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 2.2457e-04 - val_loss: 3.4656e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 2.2538e-04 - val_loss: 2.4502e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.2488e-04 - val_loss: 4.4008e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.2548e-04 - val_loss: 3.3757e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.3694e-04 - val_loss: 4.0895e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.3389e-04 - val_loss: 3.0454e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 2.2267e-04 - val_loss: 2.7196e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.2135e-04 - val_loss: 3.9565e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 2.2100e-04 - val_loss: 3.2449e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.2653e-04 - val_loss: 3.7990e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.3286e-04 - val_loss: 4.0420e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 2.1777e-04 - val_loss: 5.7359e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 2.2522e-04 - val_loss: 2.3389e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.2279e-04 - val_loss: 6.0929e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.2809e-04 - val_loss: 3.0919e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 2.2608e-04 - val_loss: 5.4112e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 2.1736e-04 - val_loss: 3.0236e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0597e-04 - val_loss: 3.5911e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.2281e-04 - val_loss: 2.5039e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.1165e-04 - val_loss: 2.2252e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.2501e-04 - val_loss: 2.8882e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.1319e-04 - val_loss: 2.6547e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.1329e-04 - val_loss: 2.7462e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.2159e-04 - val_loss: 2.3271e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.1777e-04 - val_loss: 2.7033e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 2.1858e-04 - val_loss: 2.5873e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.2287e-04 - val_loss: 3.7751e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 151/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0252e-04 - val_loss: 5.1470e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 152/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.7872e-04 - val_loss: 2.2639e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 153/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8031e-04 - val_loss: 2.2216e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 154/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.8180e-04 - val_loss: 2.3909e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 155/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8230e-04 - val_loss: 2.2477e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 156/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.8367e-04 - val_loss: 1.9999e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 157/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.8558e-04 - val_loss: 2.0581e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 158/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.8363e-04 - val_loss: 2.4270e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 159/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.8375e-04 - val_loss: 1.9929e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 160/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.8315e-04 - val_loss: 2.2000e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 161/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.7919e-04 - val_loss: 1.8534e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 162/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7881e-04 - val_loss: 2.1506e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 163/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.7697e-04 - val_loss: 2.3033e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 164/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.8075e-04 - val_loss: 2.0469e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 165/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.8099e-04 - val_loss: 2.7890e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 166/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.7902e-04 - val_loss: 2.5952e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 167/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.7780e-04 - val_loss: 2.2492e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 168/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 1.7903e-04 - val_loss: 1.9460e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 169/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7729e-04 - val_loss: 2.6915e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 170/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7543e-04 - val_loss: 2.2216e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 171/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7548e-04 - val_loss: 1.9421e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 172/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 1.7332e-04 - val_loss: 1.7919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 173/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7675e-04 - val_loss: 3.1402e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 174/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7219e-04 - val_loss: 2.0285e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 175/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.7758e-04 - val_loss: 2.2732e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 176/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.7149e-04 - val_loss: 2.2899e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 177/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 1.7176e-04 - val_loss: 1.9180e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 178/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7066e-04 - val_loss: 2.2533e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 179/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7621e-04 - val_loss: 2.2026e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 180/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.7510e-04 - val_loss: 2.5848e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 181/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.7316e-04 - val_loss: 1.9878e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 182/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6919e-04 - val_loss: 1.9826e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 183/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7319e-04 - val_loss: 1.9620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 184/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 1.6876e-04 - val_loss: 2.0544e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 185/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.6610e-04 - val_loss: 1.8221e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 186/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7252e-04 - val_loss: 2.0892e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 187/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.6876e-04 - val_loss: 1.7651e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 188/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.7394e-04 - val_loss: 2.1267e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 189/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.6581e-04 - val_loss: 2.1435e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 190/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.6670e-04 - val_loss: 2.0773e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 191/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.6943e-04 - val_loss: 2.6828e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 192/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.6722e-04 - val_loss: 1.9240e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 193/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.6979e-04 - val_loss: 1.8556e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 194/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.6276e-04 - val_loss: 2.0900e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 195/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6340e-04 - val_loss: 2.5874e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 196/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.6650e-04 - val_loss: 2.4041e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 197/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.6739e-04 - val_loss: 2.2287e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 198/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.6520e-04 - val_loss: 1.9914e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 199/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.6611e-04 - val_loss: 2.4190e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 200/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.5742e-04 - val_loss: 3.0006e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 201/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6280e-04 - val_loss: 2.0315e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 202/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.6546e-04 - val_loss: 1.8134e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 203/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.6932e-04 - val_loss: 1.7403e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 204/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.6295e-04 - val_loss: 1.9310e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 205/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.6165e-04 - val_loss: 1.8386e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 206/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6055e-04 - val_loss: 1.6594e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 207/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.5122e-04 - val_loss: 1.8623e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 208/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.5111e-04 - val_loss: 1.7181e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 209/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 1.5183e-04 - val_loss: 1.6712e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 210/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.5151e-04 - val_loss: 2.5623e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 211/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.5185e-04 - val_loss: 1.7605e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 212/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.5134e-04 - val_loss: 1.7678e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 213/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 1.5125e-04 - val_loss: 1.7273e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 214/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.5141e-04 - val_loss: 1.8584e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 215/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.5223e-04 - val_loss: 1.8939e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 216/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.5021e-04 - val_loss: 1.6293e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 217/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.5114e-04 - val_loss: 1.7229e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 218/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.4943e-04 - val_loss: 1.7022e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 219/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.5043e-04 - val_loss: 1.7380e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 220/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.4960e-04 - val_loss: 1.6858e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 221/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.5019e-04 - val_loss: 1.7948e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 222/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4935e-04 - val_loss: 2.1844e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 223/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.4988e-04 - val_loss: 1.9784e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 224/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.4797e-04 - val_loss: 1.6561e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 225/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.4961e-04 - val_loss: 1.5701e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 226/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4749e-04 - val_loss: 1.7547e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 227/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.4833e-04 - val_loss: 1.9563e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 228/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.4750e-04 - val_loss: 1.7128e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 229/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.4669e-04 - val_loss: 1.7070e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 230/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4793e-04 - val_loss: 1.7406e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 231/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.4760e-04 - val_loss: 1.7312e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 232/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.4575e-04 - val_loss: 1.6897e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 233/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.4617e-04 - val_loss: 1.7173e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 234/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4674e-04 - val_loss: 1.9842e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 235/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.4678e-04 - val_loss: 1.9550e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 236/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4599e-04 - val_loss: 1.6822e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 237/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.4566e-04 - val_loss: 2.1225e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 238/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.4559e-04 - val_loss: 1.6163e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 239/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4405e-04 - val_loss: 1.5675e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 240/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4545e-04 - val_loss: 1.7651e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 241/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.4539e-04 - val_loss: 1.6802e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 242/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4531e-04 - val_loss: 1.6081e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 243/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.4283e-04 - val_loss: 1.8235e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 244/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.4426e-04 - val_loss: 1.4916e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 245/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.4393e-04 - val_loss: 1.6718e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 246/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 1.4370e-04 - val_loss: 1.5087e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 247/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 1.4330e-04 - val_loss: 1.8287e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 248/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4381e-04 - val_loss: 1.5798e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 249/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.4298e-04 - val_loss: 1.5281e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 250/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.4329e-04 - val_loss: 1.8272e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 251/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.4362e-04 - val_loss: 1.8274e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 252/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.4160e-04 - val_loss: 1.5679e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 253/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.4159e-04 - val_loss: 1.7790e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 254/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.4323e-04 - val_loss: 2.1238e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 255/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.4330e-04 - val_loss: 1.6081e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 256/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4115e-04 - val_loss: 1.6150e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 257/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.3735e-04 - val_loss: 1.4490e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 258/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3744e-04 - val_loss: 1.5417e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 259/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3735e-04 - val_loss: 1.5289e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 260/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3787e-04 - val_loss: 1.4540e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 261/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.3789e-04 - val_loss: 1.5348e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 262/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3712e-04 - val_loss: 1.5101e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 263/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3759e-04 - val_loss: 1.5674e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 264/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3738e-04 - val_loss: 1.6805e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 265/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.3672e-04 - val_loss: 1.5142e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 266/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.3705e-04 - val_loss: 1.5172e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 267/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3684e-04 - val_loss: 1.4384e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 268/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3650e-04 - val_loss: 1.4939e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 269/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.3606e-04 - val_loss: 1.5510e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 270/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.3627e-04 - val_loss: 1.5581e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 271/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3698e-04 - val_loss: 1.6499e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 272/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.3576e-04 - val_loss: 1.4692e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 273/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.3528e-04 - val_loss: 1.5676e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 274/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.3607e-04 - val_loss: 1.6092e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 275/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.3548e-04 - val_loss: 1.4777e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 276/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3589e-04 - val_loss: 1.6518e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 277/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.3584e-04 - val_loss: 1.5954e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 278/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.3506e-04 - val_loss: 1.7193e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 279/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 1.3500e-04 - val_loss: 1.6284e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 280/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.3468e-04 - val_loss: 1.7021e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 281/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.3438e-04 - val_loss: 1.6852e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 282/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.3459e-04 - val_loss: 1.5103e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 283/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.3425e-04 - val_loss: 1.5491e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 284/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.3414e-04 - val_loss: 1.5455e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 285/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.3404e-04 - val_loss: 1.8310e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 286/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.3431e-04 - val_loss: 1.4997e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 287/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3412e-04 - val_loss: 1.5056e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 288/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3374e-04 - val_loss: 1.6248e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 289/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.3342e-04 - val_loss: 1.4439e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 290/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.3357e-04 - val_loss: 1.5092e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 291/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.3347e-04 - val_loss: 1.5286e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 292/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3339e-04 - val_loss: 1.5774e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 293/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.3373e-04 - val_loss: 1.5256e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 294/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.3303e-04 - val_loss: 1.6669e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 295/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.3277e-04 - val_loss: 1.6736e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 296/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.3297e-04 - val_loss: 1.5824e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 297/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.3261e-04 - val_loss: 1.6385e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 298/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.3247e-04 - val_loss: 1.4719e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 299/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.3253e-04 - val_loss: 1.5648e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 300/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.3211e-04 - val_loss: 2.0806e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 301/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.3272e-04 - val_loss: 1.4509e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 302/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.3171e-04 - val_loss: 1.5508e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 303/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.3231e-04 - val_loss: 1.5986e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 304/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.3150e-04 - val_loss: 1.4600e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 305/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.3137e-04 - val_loss: 1.6941e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 306/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.3174e-04 - val_loss: 1.5007e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 307/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.2989e-04 - val_loss: 1.5373e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 308/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.2965e-04 - val_loss: 1.4122e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 309/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2939e-04 - val_loss: 1.4751e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 310/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2954e-04 - val_loss: 1.4264e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 311/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2961e-04 - val_loss: 1.5021e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 312/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.2927e-04 - val_loss: 1.4158e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 313/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2955e-04 - val_loss: 1.4022e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 314/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.2940e-04 - val_loss: 1.4862e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 315/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2926e-04 - val_loss: 1.5517e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 316/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.2915e-04 - val_loss: 1.4168e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 317/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 1.2918e-04 - val_loss: 1.4212e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 318/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2905e-04 - val_loss: 1.4462e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 319/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2891e-04 - val_loss: 1.5692e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 320/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2875e-04 - val_loss: 1.4858e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 321/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 1.2866e-04 - val_loss: 1.5552e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 322/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2862e-04 - val_loss: 1.5111e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 323/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2881e-04 - val_loss: 1.4361e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 324/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.2851e-04 - val_loss: 1.4673e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 325/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 1.2845e-04 - val_loss: 1.5333e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 326/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2855e-04 - val_loss: 1.5032e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 327/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2830e-04 - val_loss: 1.4166e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 328/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 1.2815e-04 - val_loss: 1.5819e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 329/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2830e-04 - val_loss: 1.4306e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 330/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2798e-04 - val_loss: 1.4352e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 331/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2790e-04 - val_loss: 1.5139e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 332/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 1.2806e-04 - val_loss: 1.5943e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 333/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2745e-04 - val_loss: 1.4250e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 334/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2754e-04 - val_loss: 1.4630e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 335/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2823e-04 - val_loss: 1.4146e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 336/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.2727e-04 - val_loss: 1.5398e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 337/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2757e-04 - val_loss: 1.4715e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 338/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.2730e-04 - val_loss: 1.4261e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 339/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 1.2745e-04 - val_loss: 1.3885e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 340/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2725e-04 - val_loss: 1.3664e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 341/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2725e-04 - val_loss: 1.3767e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 342/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2708e-04 - val_loss: 1.3984e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 343/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.2694e-04 - val_loss: 1.3774e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 344/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2722e-04 - val_loss: 1.3718e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 345/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2700e-04 - val_loss: 1.4874e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 346/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2683e-04 - val_loss: 1.3905e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 347/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2658e-04 - val_loss: 1.4250e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 348/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2662e-04 - val_loss: 1.3704e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 349/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2670e-04 - val_loss: 1.5767e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 350/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2653e-04 - val_loss: 1.5031e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 351/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2649e-04 - val_loss: 1.4248e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 352/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2619e-04 - val_loss: 1.4745e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 353/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2638e-04 - val_loss: 1.4645e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 354/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.2634e-04 - val_loss: 1.4489e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 355/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2620e-04 - val_loss: 1.3558e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 356/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2583e-04 - val_loss: 1.3272e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 357/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.2522e-04 - val_loss: 1.3503e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 358/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2521e-04 - val_loss: 1.3461e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 359/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2509e-04 - val_loss: 1.3372e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 360/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2509e-04 - val_loss: 1.4230e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 361/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2525e-04 - val_loss: 1.3510e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 362/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2515e-04 - val_loss: 1.3326e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 363/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2500e-04 - val_loss: 1.3548e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 364/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2490e-04 - val_loss: 1.3570e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 365/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2505e-04 - val_loss: 1.3976e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 366/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2491e-04 - val_loss: 1.3764e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 367/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2474e-04 - val_loss: 1.4759e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 368/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.2488e-04 - val_loss: 1.4334e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 369/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2471e-04 - val_loss: 1.3579e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 370/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2469e-04 - val_loss: 1.3733e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 371/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2446e-04 - val_loss: 1.3934e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 372/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2480e-04 - val_loss: 1.3452e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 373/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2441e-04 - val_loss: 1.3496e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 374/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2452e-04 - val_loss: 1.3216e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 375/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.2445e-04 - val_loss: 1.3967e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 376/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2430e-04 - val_loss: 1.3609e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 377/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2438e-04 - val_loss: 1.3775e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 378/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2436e-04 - val_loss: 1.3929e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 379/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2442e-04 - val_loss: 1.4082e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 380/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2412e-04 - val_loss: 1.4271e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 381/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.2413e-04 - val_loss: 1.3642e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 382/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2416e-04 - val_loss: 1.3226e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 383/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2415e-04 - val_loss: 1.3736e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 384/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2407e-04 - val_loss: 1.4046e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 385/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2390e-04 - val_loss: 1.3114e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 386/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2397e-04 - val_loss: 1.3373e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 387/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2388e-04 - val_loss: 1.3448e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 388/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2385e-04 - val_loss: 1.3771e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 389/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2376e-04 - val_loss: 1.4251e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 390/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2384e-04 - val_loss: 1.3305e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 391/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.2379e-04 - val_loss: 1.3704e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 392/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2353e-04 - val_loss: 1.3344e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 393/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2367e-04 - val_loss: 1.3516e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 394/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2357e-04 - val_loss: 1.3254e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 395/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2347e-04 - val_loss: 1.3196e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 396/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2333e-04 - val_loss: 1.3378e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 397/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2354e-04 - val_loss: 1.3623e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 398/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2344e-04 - val_loss: 1.3562e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 399/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.2338e-04 - val_loss: 1.4117e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 400/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2336e-04 - val_loss: 1.4601e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 401/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2323e-04 - val_loss: 1.4207e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 402/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.2315e-04 - val_loss: 1.4336e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 403/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2309e-04 - val_loss: 1.3672e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 404/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2319e-04 - val_loss: 1.4037e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 405/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2310e-04 - val_loss: 1.3354e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 406/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2290e-04 - val_loss: 1.2997e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 407/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2260e-04 - val_loss: 1.3650e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 408/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2254e-04 - val_loss: 1.3534e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 409/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2254e-04 - val_loss: 1.3148e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 410/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2252e-04 - val_loss: 1.3491e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 411/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2256e-04 - val_loss: 1.3559e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 412/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2252e-04 - val_loss: 1.3404e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 413/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2245e-04 - val_loss: 1.3478e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 414/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2241e-04 - val_loss: 1.3382e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 415/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2241e-04 - val_loss: 1.3336e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 416/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2241e-04 - val_loss: 1.3405e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 417/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2227e-04 - val_loss: 1.3746e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 418/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2232e-04 - val_loss: 1.3473e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 419/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2235e-04 - val_loss: 1.3385e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 420/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2225e-04 - val_loss: 1.3401e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 421/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2222e-04 - val_loss: 1.3598e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 422/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2218e-04 - val_loss: 1.3882e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 423/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2212e-04 - val_loss: 1.3435e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 424/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2213e-04 - val_loss: 1.3329e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 425/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2207e-04 - val_loss: 1.4177e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 426/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 1.2216e-04 - val_loss: 1.3565e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 427/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2209e-04 - val_loss: 1.3638e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 428/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2213e-04 - val_loss: 1.3297e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 429/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2198e-04 - val_loss: 1.3715e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 430/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2205e-04 - val_loss: 1.3297e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 431/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2191e-04 - val_loss: 1.3138e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 432/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2190e-04 - val_loss: 1.3254e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 433/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2194e-04 - val_loss: 1.3321e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 434/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2183e-04 - val_loss: 1.3395e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 435/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2185e-04 - val_loss: 1.3260e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 436/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2178e-04 - val_loss: 1.3987e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 437/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2179e-04 - val_loss: 1.3243e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 438/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2183e-04 - val_loss: 1.3618e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 439/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2175e-04 - val_loss: 1.3264e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 440/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2171e-04 - val_loss: 1.3413e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 441/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2172e-04 - val_loss: 1.3563e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 442/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2168e-04 - val_loss: 1.3645e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 443/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.2171e-04 - val_loss: 1.3022e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 444/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.2172e-04 - val_loss: 1.3269e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 445/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.2154e-04 - val_loss: 1.3721e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 446/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2156e-04 - val_loss: 1.3450e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 447/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2154e-04 - val_loss: 1.3355e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 448/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2148e-04 - val_loss: 1.3436e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 449/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2150e-04 - val_loss: 1.3371e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 450/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.2151e-04 - val_loss: 1.3245e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 451/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2139e-04 - val_loss: 1.3096e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 452/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2143e-04 - val_loss: 1.3915e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 453/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.2141e-04 - val_loss: 1.3318e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 454/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2138e-04 - val_loss: 1.3239e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 455/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2139e-04 - val_loss: 1.3480e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 456/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2130e-04 - val_loss: 1.3504e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 457/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2110e-04 - val_loss: 1.3674e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 458/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2104e-04 - val_loss: 1.3383e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 459/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2103e-04 - val_loss: 1.3307e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 460/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2105e-04 - val_loss: 1.3828e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 461/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2104e-04 - val_loss: 1.3510e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 462/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2103e-04 - val_loss: 1.3592e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 463/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2100e-04 - val_loss: 1.3593e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 464/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2094e-04 - val_loss: 1.3604e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 465/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.2101e-04 - val_loss: 1.3424e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 466/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2097e-04 - val_loss: 1.3573e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 467/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2095e-04 - val_loss: 1.3126e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 468/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.2094e-04 - val_loss: 1.3381e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 469/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2090e-04 - val_loss: 1.3494e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 470/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.2085e-04 - val_loss: 1.3414e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 471/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2088e-04 - val_loss: 1.3326e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 472/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2085e-04 - val_loss: 1.3379e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 473/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 1.2087e-04 - val_loss: 1.3602e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 474/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2087e-04 - val_loss: 1.3547e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 475/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2082e-04 - val_loss: 1.3344e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 476/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2083e-04 - val_loss: 1.3457e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 477/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2082e-04 - val_loss: 1.3532e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 478/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2078e-04 - val_loss: 1.3129e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 479/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2076e-04 - val_loss: 1.3096e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 480/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2072e-04 - val_loss: 1.3125e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 481/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2068e-04 - val_loss: 1.3478e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 482/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2068e-04 - val_loss: 1.3448e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 483/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2073e-04 - val_loss: 1.3397e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 484/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2070e-04 - val_loss: 1.4045e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 485/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.2065e-04 - val_loss: 1.3337e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 486/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2069e-04 - val_loss: 1.3479e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 487/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2063e-04 - val_loss: 1.3052e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 488/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2063e-04 - val_loss: 1.3482e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 489/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 1.2062e-04 - val_loss: 1.3091e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 490/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2053e-04 - val_loss: 1.3640e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 491/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2061e-04 - val_loss: 1.3289e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 492/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2057e-04 - val_loss: 1.3683e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 493/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.2056e-04 - val_loss: 1.3187e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 494/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2053e-04 - val_loss: 1.3236e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 495/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2056e-04 - val_loss: 1.3400e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 496/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 1.2049e-04 - val_loss: 1.3332e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 497/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2043e-04 - val_loss: 1.3263e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 498/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2045e-04 - val_loss: 1.3496e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 499/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2047e-04 - val_loss: 1.3073e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 500/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2048e-04 - val_loss: 1.3369e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 501/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2044e-04 - val_loss: 1.3200e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 502/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2035e-04 - val_loss: 1.3333e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 503/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2041e-04 - val_loss: 1.3081e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 504/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2037e-04 - val_loss: 1.3041e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 505/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 1.2045e-04 - val_loss: 1.2997e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 506/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2037e-04 - val_loss: 1.3165e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 507/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2022e-04 - val_loss: 1.3151e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 508/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2023e-04 - val_loss: 1.3432e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 509/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2021e-04 - val_loss: 1.3241e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 510/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 1.2024e-04 - val_loss: 1.3166e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 511/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2023e-04 - val_loss: 1.2965e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 512/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.2021e-04 - val_loss: 1.3248e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 513/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.2020e-04 - val_loss: 1.3246e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 514/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2019e-04 - val_loss: 1.3223e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 515/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2018e-04 - val_loss: 1.3153e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 516/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2014e-04 - val_loss: 1.3288e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 517/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2015e-04 - val_loss: 1.3251e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 518/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2014e-04 - val_loss: 1.3312e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 519/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.2014e-04 - val_loss: 1.3250e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 520/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2015e-04 - val_loss: 1.3465e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 521/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2013e-04 - val_loss: 1.3354e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 522/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2011e-04 - val_loss: 1.3160e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 523/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.2012e-04 - val_loss: 1.3378e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 524/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2010e-04 - val_loss: 1.3399e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 525/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.2008e-04 - val_loss: 1.3654e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 526/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2008e-04 - val_loss: 1.3269e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 527/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2008e-04 - val_loss: 1.3200e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 528/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 1.2003e-04 - val_loss: 1.3343e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 529/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.2006e-04 - val_loss: 1.3399e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 530/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.2003e-04 - val_loss: 1.3368e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 531/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2004e-04 - val_loss: 1.3183e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 532/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.2000e-04 - val_loss: 1.3234e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 533/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2002e-04 - val_loss: 1.3395e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 534/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.2002e-04 - val_loss: 1.3419e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 535/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2001e-04 - val_loss: 1.3092e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 536/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2000e-04 - val_loss: 1.3458e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 537/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.1998e-04 - val_loss: 1.3260e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 538/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.1997e-04 - val_loss: 1.3448e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 539/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1996e-04 - val_loss: 1.3311e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 540/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.1998e-04 - val_loss: 1.3346e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 541/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1996e-04 - val_loss: 1.3427e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 542/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.1992e-04 - val_loss: 1.3135e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 543/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1995e-04 - val_loss: 1.3483e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 544/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1997e-04 - val_loss: 1.3007e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 545/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1994e-04 - val_loss: 1.3170e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 546/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.1991e-04 - val_loss: 1.3153e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 547/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.1989e-04 - val_loss: 1.3284e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 548/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.1986e-04 - val_loss: 1.3328e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 549/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.1989e-04 - val_loss: 1.3235e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 550/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1993e-04 - val_loss: 1.3198e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 551/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1990e-04 - val_loss: 1.3398e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 552/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.1985e-04 - val_loss: 1.3109e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 553/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1985e-04 - val_loss: 1.3261e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 554/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1985e-04 - val_loss: 1.3249e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 555/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.1985e-04 - val_loss: 1.2991e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 556/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1984e-04 - val_loss: 1.3095e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 557/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1982e-04 - val_loss: 1.3266e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 558/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1984e-04 - val_loss: 1.3342e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 559/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1980e-04 - val_loss: 1.3184e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 560/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1979e-04 - val_loss: 1.3174e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 561/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1978e-04 - val_loss: 1.3420e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 562/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.1978e-04 - val_loss: 1.3175e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 563/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1976e-04 - val_loss: 1.3417e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 564/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.1977e-04 - val_loss: 1.3382e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 565/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.1977e-04 - val_loss: 1.3271e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 566/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.1976e-04 - val_loss: 1.3284e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 567/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1976e-04 - val_loss: 1.3381e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 568/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.1976e-04 - val_loss: 1.3329e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 569/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1976e-04 - val_loss: 1.3237e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 570/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1974e-04 - val_loss: 1.3253e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 571/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1975e-04 - val_loss: 1.3457e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 572/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1973e-04 - val_loss: 1.3289e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 573/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.1974e-04 - val_loss: 1.3321e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 574/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.1971e-04 - val_loss: 1.3293e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 575/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 1.1975e-04 - val_loss: 1.3176e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 576/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1972e-04 - val_loss: 1.3143e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 577/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1972e-04 - val_loss: 1.3178e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 578/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1972e-04 - val_loss: 1.3195e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 579/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1970e-04 - val_loss: 1.3344e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 580/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1971e-04 - val_loss: 1.3170e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 581/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 1.1970e-04 - val_loss: 1.3103e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 582/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.1970e-04 - val_loss: 1.3101e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 583/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 1.1968e-04 - val_loss: 1.3188e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 584/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1972e-04 - val_loss: 1.3076e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 585/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1970e-04 - val_loss: 1.3174e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 586/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.1967e-04 - val_loss: 1.3117e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 587/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1969e-04 - val_loss: 1.3264e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 588/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.1967e-04 - val_loss: 1.3243e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 589/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1966e-04 - val_loss: 1.3411e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 590/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1966e-04 - val_loss: 1.3187e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 591/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1965e-04 - val_loss: 1.3298e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 592/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1967e-04 - val_loss: 1.3133e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 593/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.1966e-04 - val_loss: 1.3164e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 594/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.1966e-04 - val_loss: 1.3198e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 595/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1964e-04 - val_loss: 1.3074e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 596/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.1963e-04 - val_loss: 1.3085e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 597/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1962e-04 - val_loss: 1.3141e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 598/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1963e-04 - val_loss: 1.3346e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 599/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1959e-04 - val_loss: 1.3283e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 600/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.1962e-04 - val_loss: 1.3158e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 601/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1964e-04 - val_loss: 1.3288e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 602/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1961e-04 - val_loss: 1.3129e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 603/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1960e-04 - val_loss: 1.3117e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 604/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1960e-04 - val_loss: 1.3101e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 605/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.1961e-04 - val_loss: 1.3209e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 606/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.1962e-04 - val_loss: 1.3216e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 607/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.1960e-04 - val_loss: 1.3062e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 608/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1959e-04 - val_loss: 1.3255e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 609/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.1959e-04 - val_loss: 1.3123e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 610/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.1959e-04 - val_loss: 1.3150e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 611/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1959e-04 - val_loss: 1.3211e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 612/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.1958e-04 - val_loss: 1.3206e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 613/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.1959e-04 - val_loss: 1.3098e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 614/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.1958e-04 - val_loss: 1.3175e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 615/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.1957e-04 - val_loss: 1.3066e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 616/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.1957e-04 - val_loss: 1.3011e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 617/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.1956e-04 - val_loss: 1.3026e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 618/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1956e-04 - val_loss: 1.3110e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 619/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1955e-04 - val_loss: 1.3214e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 620/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1956e-04 - val_loss: 1.3076e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 621/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.1956e-04 - val_loss: 1.3138e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 622/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1954e-04 - val_loss: 1.3098e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 623/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.1955e-04 - val_loss: 1.3123e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 624/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.1955e-04 - val_loss: 1.3213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 625/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1955e-04 - val_loss: 1.3115e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 626/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.1954e-04 - val_loss: 1.3141e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 627/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1955e-04 - val_loss: 1.3090e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 628/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.1954e-04 - val_loss: 1.3121e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 629/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1953e-04 - val_loss: 1.3114e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 630/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1952e-04 - val_loss: 1.3124e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 631/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.1954e-04 - val_loss: 1.3067e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 632/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1951e-04 - val_loss: 1.3055e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 633/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.1953e-04 - val_loss: 1.3074e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 634/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1953e-04 - val_loss: 1.3097e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 635/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.1952e-04 - val_loss: 1.3141e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 636/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1953e-04 - val_loss: 1.3014e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 637/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1953e-04 - val_loss: 1.3057e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 638/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1951e-04 - val_loss: 1.3109e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 639/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.1951e-04 - val_loss: 1.3002e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 640/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1951e-04 - val_loss: 1.3063e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 641/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1950e-04 - val_loss: 1.3065e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 642/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1950e-04 - val_loss: 1.3054e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 643/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.1950e-04 - val_loss: 1.3170e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 644/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1949e-04 - val_loss: 1.3172e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 645/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1950e-04 - val_loss: 1.3000e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 646/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.1951e-04 - val_loss: 1.3077e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 647/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.1951e-04 - val_loss: 1.3015e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 648/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 1.1949e-04 - val_loss: 1.3073e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 649/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1949e-04 - val_loss: 1.3114e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 650/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.1948e-04 - val_loss: 1.3038e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 651/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.1951e-04 - val_loss: 1.3069e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 652/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1947e-04 - val_loss: 1.3051e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 653/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1949e-04 - val_loss: 1.3201e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 654/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1948e-04 - val_loss: 1.3054e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 655/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1946e-04 - val_loss: 1.3083e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 656/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.1949e-04 - val_loss: 1.3122e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 657/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1951e-04 - val_loss: 1.3154e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 658/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.1950e-04 - val_loss: 1.3160e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 659/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1951e-04 - val_loss: 1.3136e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 660/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.1950e-04 - val_loss: 1.3115e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 661/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.1949e-04 - val_loss: 1.3145e-04 - learning_rate: 2.4414e-07\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step\n",
      "Model 1 Test Set RMSE: 0.003779 AU\n",
      "\n",
      "--- Training MLP Model 2 (Seed: 2) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841us/step - loss: 0.0476 - val_loss: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0163 - val_loss: 0.0283 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0111 - val_loss: 0.0217 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.0079 - val_loss: 0.0143 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0056 - val_loss: 0.0135 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0042 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0032 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.0024 - val_loss: 0.0068 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.0023 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.0017 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0015 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.0014 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0012 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 0.0014 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0012 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.0010 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 9.6822e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.0010 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 9.0797e-04 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.8721e-04 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.9513e-04 - val_loss: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 8.2920e-04 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 8.2059e-04 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 7.0453e-04 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 7.5125e-04 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 7.2078e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 5.9660e-04 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 7.2595e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.8523e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 5.8051e-04 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 8.0770e-04 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 5.7195e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.9883e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.1762e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 5.5616e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.2197e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 5.4589e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 5.9044e-04 - val_loss: 8.8154e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 4.7021e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 5.5195e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 4.6935e-04 - val_loss: 9.5819e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.7864e-04 - val_loss: 6.7694e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.5834e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.7340e-04 - val_loss: 8.6351e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 4.7928e-04 - val_loss: 6.5995e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.1226e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 5.1227e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.3999e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.2002e-04 - val_loss: 6.6868e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.2170e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 3.2688e-04 - val_loss: 4.9118e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 3.1574e-04 - val_loss: 5.6916e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.0929e-04 - val_loss: 5.7608e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 3.2733e-04 - val_loss: 6.4590e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 3.3189e-04 - val_loss: 4.7992e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.0753e-04 - val_loss: 6.9255e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 3.0212e-04 - val_loss: 7.5501e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.3155e-04 - val_loss: 4.8887e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.9633e-04 - val_loss: 6.8689e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.8573e-04 - val_loss: 3.9961e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 3.0081e-04 - val_loss: 3.8563e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.9668e-04 - val_loss: 4.9494e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.8836e-04 - val_loss: 4.7287e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.8787e-04 - val_loss: 3.2990e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 2.8310e-04 - val_loss: 3.5941e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.8022e-04 - val_loss: 5.0823e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 2.7657e-04 - val_loss: 3.5980e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.9052e-04 - val_loss: 8.2452e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 2.8653e-04 - val_loss: 3.3439e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.7956e-04 - val_loss: 3.1739e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.7852e-04 - val_loss: 3.3288e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.8271e-04 - val_loss: 8.2015e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.6768e-04 - val_loss: 3.8993e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.6712e-04 - val_loss: 4.0149e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.7791e-04 - val_loss: 3.5228e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.7057e-04 - val_loss: 4.2823e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 2.6176e-04 - val_loss: 2.8939e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.7288e-04 - val_loss: 7.8969e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.7076e-04 - val_loss: 3.4149e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.6443e-04 - val_loss: 3.3678e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.7108e-04 - val_loss: 4.0596e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.5306e-04 - val_loss: 3.5872e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.6038e-04 - val_loss: 3.9069e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.6074e-04 - val_loss: 5.6781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.4856e-04 - val_loss: 4.4189e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.5536e-04 - val_loss: 3.5011e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.5958e-04 - val_loss: 3.0798e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.4063e-04 - val_loss: 3.5790e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.5103e-04 - val_loss: 8.7819e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.4517e-04 - val_loss: 2.9385e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.7454e-04 - val_loss: 2.5305e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 2.4076e-04 - val_loss: 2.9464e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.5019e-04 - val_loss: 2.9801e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.4304e-04 - val_loss: 2.7621e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.5253e-04 - val_loss: 3.0882e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.3092e-04 - val_loss: 3.0441e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 2.5432e-04 - val_loss: 4.2820e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 2.4343e-04 - val_loss: 4.2531e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.2567e-04 - val_loss: 2.7018e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 2.4857e-04 - val_loss: 4.8216e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.0080e-04 - val_loss: 2.4649e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.0273e-04 - val_loss: 2.5759e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.0520e-04 - val_loss: 2.4050e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0352e-04 - val_loss: 2.5403e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.0437e-04 - val_loss: 2.9854e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0550e-04 - val_loss: 2.5831e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.0571e-04 - val_loss: 2.7205e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 2.0328e-04 - val_loss: 2.6082e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.0053e-04 - val_loss: 2.3557e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.0586e-04 - val_loss: 2.3786e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.9865e-04 - val_loss: 2.7353e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.0260e-04 - val_loss: 2.5689e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9865e-04 - val_loss: 3.1445e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.9851e-04 - val_loss: 2.2426e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.9961e-04 - val_loss: 2.3690e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.9850e-04 - val_loss: 2.3390e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.9274e-04 - val_loss: 2.2922e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.9407e-04 - val_loss: 3.9571e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 2.0400e-04 - val_loss: 2.2166e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.9064e-04 - val_loss: 2.2887e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.9123e-04 - val_loss: 2.9537e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9729e-04 - val_loss: 2.1290e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.9151e-04 - val_loss: 3.6950e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9422e-04 - val_loss: 2.1162e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.8805e-04 - val_loss: 2.1461e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8894e-04 - val_loss: 2.1643e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.9382e-04 - val_loss: 2.1620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.8676e-04 - val_loss: 2.7139e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.8885e-04 - val_loss: 2.4450e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.8748e-04 - val_loss: 2.3794e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8669e-04 - val_loss: 2.9870e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.9118e-04 - val_loss: 7.1174e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.9812e-04 - val_loss: 2.1713e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8419e-04 - val_loss: 1.9784e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.8526e-04 - val_loss: 1.9412e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.8489e-04 - val_loss: 2.2061e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8453e-04 - val_loss: 2.1450e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.8893e-04 - val_loss: 2.0325e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.8108e-04 - val_loss: 2.0851e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.7992e-04 - val_loss: 2.3957e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8384e-04 - val_loss: 2.7032e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.8036e-04 - val_loss: 2.0488e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.8624e-04 - val_loss: 2.2436e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.7837e-04 - val_loss: 1.9124e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.7933e-04 - val_loss: 1.9293e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.7981e-04 - val_loss: 1.9963e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.7868e-04 - val_loss: 2.2217e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.7672e-04 - val_loss: 2.4915e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8012e-04 - val_loss: 1.8853e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.7384e-04 - val_loss: 2.0817e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step\n",
      "Model 2 Test Set RMSE: 0.141092 AU\n",
      "\n",
      "--- Training MLP Model 3 (Seed: 3) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794us/step - loss: 0.0413 - val_loss: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.0168 - val_loss: 0.0280 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.0114 - val_loss: 0.0214 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0079 - val_loss: 0.0143 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0055 - val_loss: 0.0114 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0041 - val_loss: 0.0142 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.0033 - val_loss: 0.0080 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0025 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.0022 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.0017 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.0016 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.0014 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.0015 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0011 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0011 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 9.8722e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 9.7788e-04 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 9.6388e-04 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 9.2148e-04 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.0011 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 8.4416e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 6.7964e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.7816e-04 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 7.9546e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 7.3140e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 6.6182e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 7.5125e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 6.7143e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 5.7297e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.4132e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 6.1524e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 6.2321e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 5.9521e-04 - val_loss: 9.3181e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 5.0807e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 5.7236e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 5.9223e-04 - val_loss: 7.3653e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.4631e-04 - val_loss: 6.9137e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 5.1023e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 4.8238e-04 - val_loss: 9.8406e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 5.2127e-04 - val_loss: 6.6955e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 4.6303e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.8213e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 6.4093e-04 - val_loss: 5.9402e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 3.9986e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 4.4842e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 5.1530e-04 - val_loss: 4.8554e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 4.2902e-04 - val_loss: 9.3296e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 4.7305e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 5.0146e-04 - val_loss: 8.7022e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.8178e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 3.3112e-04 - val_loss: 5.5572e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.1317e-04 - val_loss: 6.4677e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.1132e-04 - val_loss: 5.5959e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.2314e-04 - val_loss: 4.6212e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 3.2156e-04 - val_loss: 4.1506e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 3.1458e-04 - val_loss: 0.0013 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 3.2110e-04 - val_loss: 5.5574e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 3.1462e-04 - val_loss: 3.7301e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 3.0300e-04 - val_loss: 3.5835e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 3.0206e-04 - val_loss: 4.4957e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.9046e-04 - val_loss: 5.5210e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 3.1142e-04 - val_loss: 3.7546e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.8399e-04 - val_loss: 4.1134e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.8832e-04 - val_loss: 5.0327e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 2.9545e-04 - val_loss: 3.2287e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.9632e-04 - val_loss: 6.3100e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 2.7147e-04 - val_loss: 2.9588e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.8725e-04 - val_loss: 3.6958e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.8325e-04 - val_loss: 2.9820e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.6070e-04 - val_loss: 3.3754e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.7599e-04 - val_loss: 5.6783e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 3.0322e-04 - val_loss: 4.9352e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.9017e-04 - val_loss: 2.9875e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.6503e-04 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 2.9560e-04 - val_loss: 7.6114e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.6598e-04 - val_loss: 3.5281e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 2.5622e-04 - val_loss: 3.0112e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 2.5990e-04 - val_loss: 2.9363e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.5572e-04 - val_loss: 3.1864e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 2.7470e-04 - val_loss: 5.6164e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.6467e-04 - val_loss: 4.6651e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 2.7207e-04 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 2.5589e-04 - val_loss: 3.1275e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.5322e-04 - val_loss: 3.2074e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 2.4823e-04 - val_loss: 2.5379e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 2.6140e-04 - val_loss: 3.1008e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.7737e-04 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.6862e-04 - val_loss: 2.7976e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 2.2566e-04 - val_loss: 2.5248e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.5528e-04 - val_loss: 6.0474e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.4304e-04 - val_loss: 6.5022e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 2.4990e-04 - val_loss: 3.7128e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.3697e-04 - val_loss: 2.9553e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 2.2847e-04 - val_loss: 3.2104e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 2.4762e-04 - val_loss: 3.3890e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.4431e-04 - val_loss: 3.1347e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.4268e-04 - val_loss: 3.0309e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.4545e-04 - val_loss: 5.0121e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.3287e-04 - val_loss: 2.3634e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.4495e-04 - val_loss: 0.0014 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 2.1789e-04 - val_loss: 2.2577e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9877e-04 - val_loss: 2.0988e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.9816e-04 - val_loss: 2.4376e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.0001e-04 - val_loss: 2.8529e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.0319e-04 - val_loss: 2.3666e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 2.0257e-04 - val_loss: 2.6231e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.0404e-04 - val_loss: 2.2517e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.9833e-04 - val_loss: 2.9602e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.0494e-04 - val_loss: 2.1641e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.9887e-04 - val_loss: 2.0314e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9781e-04 - val_loss: 2.2616e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.9535e-04 - val_loss: 3.5058e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.9850e-04 - val_loss: 2.0285e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 1.9555e-04 - val_loss: 2.2768e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9727e-04 - val_loss: 2.7060e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.0035e-04 - val_loss: 2.1282e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.9446e-04 - val_loss: 2.0151e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.8881e-04 - val_loss: 3.3086e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 1.9914e-04 - val_loss: 2.5132e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9380e-04 - val_loss: 2.1403e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8631e-04 - val_loss: 2.3357e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 1.9098e-04 - val_loss: 2.4568e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 1.8704e-04 - val_loss: 2.0304e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.9157e-04 - val_loss: 2.6804e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.8773e-04 - val_loss: 1.9849e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.8408e-04 - val_loss: 2.2142e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.8595e-04 - val_loss: 2.1095e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 1.8659e-04 - val_loss: 2.0962e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8301e-04 - val_loss: 2.3150e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.8382e-04 - val_loss: 2.6230e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.8143e-04 - val_loss: 3.4169e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.9190e-04 - val_loss: 2.3872e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.8102e-04 - val_loss: 2.0401e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.8812e-04 - val_loss: 2.1804e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7729e-04 - val_loss: 2.1881e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7912e-04 - val_loss: 2.0323e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 1.8051e-04 - val_loss: 1.8844e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.7832e-04 - val_loss: 2.6218e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.8997e-04 - val_loss: 2.0177e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.7573e-04 - val_loss: 2.0865e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7770e-04 - val_loss: 2.2896e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.7751e-04 - val_loss: 1.9032e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 1.7770e-04 - val_loss: 1.9956e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 1.7906e-04 - val_loss: 2.1837e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 1.7462e-04 - val_loss: 3.6816e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7848e-04 - val_loss: 2.2481e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.7507e-04 - val_loss: 2.1286e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.7279e-04 - val_loss: 1.8329e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.8243e-04 - val_loss: 2.0347e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.7650e-04 - val_loss: 2.0945e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step\n",
      "Model 3 Test Set RMSE: 0.148594 AU\n",
      "\n",
      "--- Training MLP Model 4 (Seed: 4) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796us/step - loss: 0.0455 - val_loss: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0173 - val_loss: 0.0296 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.0122 - val_loss: 0.0190 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0084 - val_loss: 0.0132 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0061 - val_loss: 0.0245 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.0045 - val_loss: 0.0097 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0034 - val_loss: 0.0158 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0027 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.0023 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0020 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.0018 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0015 - val_loss: 0.0070 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.0013 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0012 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.0012 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.0011 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 9.9061e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 9.2153e-04 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 8.0860e-04 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.0010 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.3639e-04 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 9.8896e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 7.1075e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 7.1374e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 7.7266e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 7.6729e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 6.0899e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 6.0301e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 6.6481e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 6.3636e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 5.4483e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.7889e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.5184e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 7.2055e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.3850e-04 - val_loss: 9.7281e-04 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 4.5285e-04 - val_loss: 6.8275e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 5.3153e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 5.4958e-04 - val_loss: 6.9532e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 5.0952e-04 - val_loss: 7.3042e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 5.6185e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 4.7886e-04 - val_loss: 6.2524e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 4.5327e-04 - val_loss: 7.2261e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 5.8917e-04 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.3130e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.7173e-04 - val_loss: 8.5872e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 4.6117e-04 - val_loss: 6.4031e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.5154e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.9531e-04 - val_loss: 5.9374e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.3445e-04 - val_loss: 8.6115e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 4.3678e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.3314e-04 - val_loss: 4.6948e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 3.0680e-04 - val_loss: 4.4028e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.0987e-04 - val_loss: 4.5780e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.1413e-04 - val_loss: 5.3724e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.2539e-04 - val_loss: 5.0090e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 3.1782e-04 - val_loss: 4.1575e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.0436e-04 - val_loss: 3.8963e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 3.2943e-04 - val_loss: 3.5919e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 3.0851e-04 - val_loss: 3.2496e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.8781e-04 - val_loss: 3.9459e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.9136e-04 - val_loss: 6.9479e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.1338e-04 - val_loss: 4.0801e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.8627e-04 - val_loss: 0.0019 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.5501e-04 - val_loss: 3.3179e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.8974e-04 - val_loss: 3.3991e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.7804e-04 - val_loss: 4.0161e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.7898e-04 - val_loss: 3.3907e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.8571e-04 - val_loss: 6.2544e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 2.8122e-04 - val_loss: 4.9956e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.8323e-04 - val_loss: 2.8828e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.7473e-04 - val_loss: 4.5929e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.7168e-04 - val_loss: 0.0014 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.6869e-04 - val_loss: 3.1707e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.6076e-04 - val_loss: 3.3964e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 2.8606e-04 - val_loss: 5.5133e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 2.5518e-04 - val_loss: 5.2886e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.6637e-04 - val_loss: 3.8101e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.6411e-04 - val_loss: 3.3517e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.8047e-04 - val_loss: 3.7583e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.5708e-04 - val_loss: 3.7155e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 2.6938e-04 - val_loss: 5.7373e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.4591e-04 - val_loss: 5.2286e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.6453e-04 - val_loss: 2.5358e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.5490e-04 - val_loss: 6.5851e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.5742e-04 - val_loss: 2.6489e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.5934e-04 - val_loss: 4.2754e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 2.5650e-04 - val_loss: 4.4012e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 2.4426e-04 - val_loss: 4.0764e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.6729e-04 - val_loss: 7.5046e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.4093e-04 - val_loss: 4.0819e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.5579e-04 - val_loss: 3.4049e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 2.4798e-04 - val_loss: 2.5457e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.6131e-04 - val_loss: 5.0000e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.5250e-04 - val_loss: 3.1120e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.2676e-04 - val_loss: 2.7299e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.4840e-04 - val_loss: 2.6454e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.3877e-04 - val_loss: 3.0110e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.4326e-04 - val_loss: 7.0864e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.5029e-04 - val_loss: 2.9006e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 2.5064e-04 - val_loss: 2.6343e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9771e-04 - val_loss: 2.2834e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.9843e-04 - val_loss: 2.5577e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.9801e-04 - val_loss: 2.7615e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0645e-04 - val_loss: 2.3217e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 2.0395e-04 - val_loss: 2.7919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0150e-04 - val_loss: 2.4515e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.0182e-04 - val_loss: 2.5800e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0266e-04 - val_loss: 2.3067e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.0278e-04 - val_loss: 2.5494e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9877e-04 - val_loss: 2.5670e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 2.0217e-04 - val_loss: 2.5321e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0291e-04 - val_loss: 2.5141e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.9619e-04 - val_loss: 2.5732e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.9588e-04 - val_loss: 2.2963e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9472e-04 - val_loss: 2.5752e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.0015e-04 - val_loss: 2.5866e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.9894e-04 - val_loss: 2.1768e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.8684e-04 - val_loss: 2.6104e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.9404e-04 - val_loss: 2.4633e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.9735e-04 - val_loss: 4.7067e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.9177e-04 - val_loss: 2.1773e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.8476e-04 - val_loss: 2.0779e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9144e-04 - val_loss: 2.4356e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.9263e-04 - val_loss: 2.3417e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.8510e-04 - val_loss: 1.9470e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.8709e-04 - val_loss: 2.4944e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.8587e-04 - val_loss: 2.6201e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.8976e-04 - val_loss: 2.0366e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8361e-04 - val_loss: 2.5196e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.8980e-04 - val_loss: 2.2183e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.8592e-04 - val_loss: 2.5868e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.9063e-04 - val_loss: 1.9660e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.7967e-04 - val_loss: 2.3002e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8538e-04 - val_loss: 1.8209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.8101e-04 - val_loss: 2.0367e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8411e-04 - val_loss: 1.9901e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.8306e-04 - val_loss: 2.1545e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8206e-04 - val_loss: 3.1391e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 1.8346e-04 - val_loss: 3.3475e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.7617e-04 - val_loss: 2.0510e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.8177e-04 - val_loss: 2.4261e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.7739e-04 - val_loss: 2.1260e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.7556e-04 - val_loss: 2.3998e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.8384e-04 - val_loss: 1.8186e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.7861e-04 - val_loss: 3.0235e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.7453e-04 - val_loss: 2.7641e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.7681e-04 - val_loss: 1.7781e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7904e-04 - val_loss: 1.7606e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.7465e-04 - val_loss: 2.1387e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7162e-04 - val_loss: 1.9658e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step\n",
      "Model 4 Test Set RMSE: 0.137001 AU\n",
      "\n",
      "--- Training MLP Model 5 (Seed: 5) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - loss: 0.0446 - val_loss: 0.0486 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.0162 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0110 - val_loss: 0.0227 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0075 - val_loss: 0.0221 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0054 - val_loss: 0.0204 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0039 - val_loss: 0.0152 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0032 - val_loss: 0.0245 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0024 - val_loss: 0.0146 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.0020 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0017 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0018 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.0013 - val_loss: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.0014 - val_loss: 0.0128 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0012 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.0010 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.0011 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 9.7023e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 9.1103e-04 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.0011 - val_loss: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0011 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 6.6053e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 7.8395e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 6.6521e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 7.0686e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 8.1655e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 7.3442e-04 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 7.9967e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.5805e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 7.1951e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 5.3481e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 6.7174e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 6.2648e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 5.1283e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.1694e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 7.4326e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 5.1666e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.6923e-04 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 6.0356e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.9736e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 5.0169e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.5953e-04 - val_loss: 7.8708e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.8897e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 6.3299e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 4.7786e-04 - val_loss: 5.1944e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 3.9983e-04 - val_loss: 7.4848e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 4.8606e-04 - val_loss: 9.8577e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.3071e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.5018e-04 - val_loss: 6.9300e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 4.2266e-04 - val_loss: 8.4057e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 5.5080e-04 - val_loss: 9.2600e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 3.2915e-04 - val_loss: 4.8145e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 3.1253e-04 - val_loss: 5.0059e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 3.0739e-04 - val_loss: 5.6926e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 3.3875e-04 - val_loss: 8.6423e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.1517e-04 - val_loss: 4.3781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 3.2102e-04 - val_loss: 5.5493e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 3.2644e-04 - val_loss: 5.5546e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 3.1127e-04 - val_loss: 6.6786e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 3.2582e-04 - val_loss: 4.8800e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.8631e-04 - val_loss: 4.1712e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 3.0096e-04 - val_loss: 3.6731e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.8449e-04 - val_loss: 4.0241e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.0817e-04 - val_loss: 3.5359e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.9412e-04 - val_loss: 3.7576e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 3.0275e-04 - val_loss: 4.8278e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 3.0715e-04 - val_loss: 4.9466e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.6917e-04 - val_loss: 4.2191e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.9101e-04 - val_loss: 4.0886e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.8004e-04 - val_loss: 4.2770e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.6625e-04 - val_loss: 3.1088e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.8572e-04 - val_loss: 3.2642e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.6463e-04 - val_loss: 4.1099e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.7212e-04 - val_loss: 5.0334e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.8227e-04 - val_loss: 3.8781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 2.7642e-04 - val_loss: 2.7446e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.6354e-04 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.0697e-04 - val_loss: 3.1878e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 2.7357e-04 - val_loss: 3.1817e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 2.5398e-04 - val_loss: 2.8785e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.5713e-04 - val_loss: 2.5885e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.8496e-04 - val_loss: 8.9083e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.1455e-04 - val_loss: 2.6615e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.4252e-04 - val_loss: 4.1838e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.5791e-04 - val_loss: 3.2193e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 2.6715e-04 - val_loss: 2.5916e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.4786e-04 - val_loss: 3.6944e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.7427e-04 - val_loss: 3.2832e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 2.3782e-04 - val_loss: 3.0827e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.4715e-04 - val_loss: 3.2284e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.4836e-04 - val_loss: 7.8483e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 2.5788e-04 - val_loss: 2.5883e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.3595e-04 - val_loss: 4.8899e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 2.5084e-04 - val_loss: 4.4293e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 2.3798e-04 - val_loss: 3.4725e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.5043e-04 - val_loss: 3.2360e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 2.5352e-04 - val_loss: 4.4677e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 2.2854e-04 - val_loss: 4.5063e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.5368e-04 - val_loss: 3.1846e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.4083e-04 - val_loss: 3.6971e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 2.2814e-04 - val_loss: 2.2340e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.0107e-04 - val_loss: 2.7192e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.0152e-04 - val_loss: 2.1685e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.0095e-04 - val_loss: 2.1851e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 2.1144e-04 - val_loss: 2.1932e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0173e-04 - val_loss: 2.0942e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.0641e-04 - val_loss: 2.6185e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.0379e-04 - val_loss: 2.1192e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.0601e-04 - val_loss: 2.1679e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0705e-04 - val_loss: 2.3466e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.0000e-04 - val_loss: 2.1579e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.9848e-04 - val_loss: 2.8147e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0281e-04 - val_loss: 2.1297e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9945e-04 - val_loss: 2.5905e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.9916e-04 - val_loss: 3.9953e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0395e-04 - val_loss: 3.3941e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9736e-04 - val_loss: 2.0559e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.8973e-04 - val_loss: 2.8433e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.9176e-04 - val_loss: 2.0289e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0252e-04 - val_loss: 3.0168e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.9140e-04 - val_loss: 2.3850e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 1.8982e-04 - val_loss: 3.0323e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.8845e-04 - val_loss: 2.0431e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.9574e-04 - val_loss: 3.4478e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 1.8907e-04 - val_loss: 2.0850e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8685e-04 - val_loss: 2.0699e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8690e-04 - val_loss: 2.0942e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.8970e-04 - val_loss: 2.2024e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8326e-04 - val_loss: 3.8231e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.8438e-04 - val_loss: 3.2227e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.8640e-04 - val_loss: 2.2613e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.8581e-04 - val_loss: 2.0271e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.8438e-04 - val_loss: 2.0756e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.8339e-04 - val_loss: 1.9439e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.8499e-04 - val_loss: 2.0989e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 1.7819e-04 - val_loss: 1.9500e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.8493e-04 - val_loss: 2.0055e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7966e-04 - val_loss: 1.9915e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.7985e-04 - val_loss: 2.8278e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.7917e-04 - val_loss: 1.9167e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8438e-04 - val_loss: 1.8326e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.7694e-04 - val_loss: 1.9355e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.7627e-04 - val_loss: 1.9808e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.7742e-04 - val_loss: 2.2143e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.8277e-04 - val_loss: 2.2293e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.7476e-04 - val_loss: 2.8392e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.7669e-04 - val_loss: 1.9113e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.7533e-04 - val_loss: 1.8199e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7913e-04 - val_loss: 2.5922e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.7507e-04 - val_loss: 2.1978e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.7299e-04 - val_loss: 2.0170e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step\n",
      "Model 5 Test Set RMSE: 0.171208 AU\n",
      "\n",
      "--- Training MLP Model 6 (Seed: 6) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 797us/step - loss: 0.0445 - val_loss: 0.0345 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.0171 - val_loss: 0.0252 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.0117 - val_loss: 0.0181 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.0084 - val_loss: 0.0198 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0059 - val_loss: 0.0108 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.0042 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.0033 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.0026 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0021 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.0020 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0015 - val_loss: 0.0106 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0013 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.0014 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.0010 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0012 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0011 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 8.3553e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 9.6600e-04 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 9.7402e-04 - val_loss: 0.0098 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 9.6050e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 7.7758e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 7.9880e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 6.5116e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 7.6468e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 6.8704e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 6.6707e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 5.9898e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 6.3620e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 5.7810e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 6.9225e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 4.7156e-04 - val_loss: 7.8836e-04 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 7.0137e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 4.8455e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 5.5373e-04 - val_loss: 8.6294e-04 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 5.3559e-04 - val_loss: 7.7264e-04 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 5.1775e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 5.1447e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 4.8697e-04 - val_loss: 6.6051e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 5.7930e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 5.9028e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 5.1097e-04 - val_loss: 5.0576e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 5.8069e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 4.7374e-04 - val_loss: 6.1923e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.0761e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 5.1015e-04 - val_loss: 6.6456e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 5.0401e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.6611e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.2399e-04 - val_loss: 8.6337e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.8072e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.4287e-04 - val_loss: 6.8084e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 3.2057e-04 - val_loss: 3.9281e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 3.1335e-04 - val_loss: 3.6198e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 3.0824e-04 - val_loss: 5.2060e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 3.2190e-04 - val_loss: 4.6424e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 3.3749e-04 - val_loss: 7.0702e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 3.0763e-04 - val_loss: 7.2305e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 3.4855e-04 - val_loss: 3.9117e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 2.9753e-04 - val_loss: 3.5187e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 3.0545e-04 - val_loss: 3.8668e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 2.9971e-04 - val_loss: 8.1433e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 3.0285e-04 - val_loss: 3.2660e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 3.0633e-04 - val_loss: 3.1296e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 2.8533e-04 - val_loss: 3.1165e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 2.7519e-04 - val_loss: 3.1736e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 3.3222e-04 - val_loss: 3.4464e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 2.7241e-04 - val_loss: 3.3921e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 2.9426e-04 - val_loss: 3.8031e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 3.0740e-04 - val_loss: 4.0173e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 2.6780e-04 - val_loss: 3.9726e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 2.8172e-04 - val_loss: 4.9214e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 2.7250e-04 - val_loss: 3.4759e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 2.7297e-04 - val_loss: 4.0092e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 2.9328e-04 - val_loss: 3.2428e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 2.6127e-04 - val_loss: 3.1520e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 2.7914e-04 - val_loss: 3.2005e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.7103e-04 - val_loss: 2.9066e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 2.6446e-04 - val_loss: 5.8889e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 2.6150e-04 - val_loss: 2.8894e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 2.7096e-04 - val_loss: 5.2644e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - loss: 2.5093e-04 - val_loss: 3.0296e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 3.0506e-04 - val_loss: 2.5720e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 2.4395e-04 - val_loss: 3.0857e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 2.7857e-04 - val_loss: 3.9450e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 2.5903e-04 - val_loss: 3.6094e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 2.4030e-04 - val_loss: 5.6003e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 3.2104e-04 - val_loss: 3.4632e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 2.3073e-04 - val_loss: 2.9478e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 2.3944e-04 - val_loss: 2.8136e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 2.7558e-04 - val_loss: 2.7847e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 2.4374e-04 - val_loss: 3.4251e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 2.7899e-04 - val_loss: 2.6815e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 2.4845e-04 - val_loss: 2.7513e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 2.4847e-04 - val_loss: 3.9982e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 2.8074e-04 - val_loss: 0.0018 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 2.6145e-04 - val_loss: 3.4858e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 2.3682e-04 - val_loss: 6.8265e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 2.5184e-04 - val_loss: 4.5559e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 2.6872e-04 - val_loss: 0.0013 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 2.5262e-04 - val_loss: 2.2851e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 2.4840e-04 - val_loss: 4.2247e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 2.0496e-04 - val_loss: 2.1243e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 2.0372e-04 - val_loss: 2.4039e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2.0326e-04 - val_loss: 2.3636e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 2.0630e-04 - val_loss: 2.3604e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 2.0795e-04 - val_loss: 2.2229e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 2.0178e-04 - val_loss: 2.4415e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2.0548e-04 - val_loss: 3.7351e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 2.0565e-04 - val_loss: 2.6666e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 2.0195e-04 - val_loss: 2.7585e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 2.0444e-04 - val_loss: 2.2490e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.0507e-04 - val_loss: 2.5551e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 2.0624e-04 - val_loss: 2.7647e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 1.9727e-04 - val_loss: 2.4925e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 1.9907e-04 - val_loss: 2.0647e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9867e-04 - val_loss: 2.1788e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 1.9890e-04 - val_loss: 2.3504e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2.0048e-04 - val_loss: 3.2821e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9365e-04 - val_loss: 2.4444e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9631e-04 - val_loss: 3.2421e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9774e-04 - val_loss: 2.5122e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 1.9673e-04 - val_loss: 2.2249e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9371e-04 - val_loss: 2.1524e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 1.9302e-04 - val_loss: 2.2251e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 1.9019e-04 - val_loss: 4.0006e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 1.9285e-04 - val_loss: 3.0134e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9222e-04 - val_loss: 2.6769e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 1.9089e-04 - val_loss: 2.6039e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - loss: 1.9059e-04 - val_loss: 2.6739e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 1.8978e-04 - val_loss: 2.4008e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8947e-04 - val_loss: 2.7331e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 1.8950e-04 - val_loss: 2.5761e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 1.9164e-04 - val_loss: 2.2131e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9365e-04 - val_loss: 3.6408e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 1.9243e-04 - val_loss: 2.0977e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 1.8313e-04 - val_loss: 2.0786e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 1.8243e-04 - val_loss: 3.4754e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9069e-04 - val_loss: 2.4007e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 1.8568e-04 - val_loss: 2.8781e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 1.8758e-04 - val_loss: 1.9859e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8876e-04 - val_loss: 2.4977e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 1.8397e-04 - val_loss: 2.0618e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 1.8492e-04 - val_loss: 3.9695e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 1.9443e-04 - val_loss: 1.9977e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.7737e-04 - val_loss: 1.8486e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 1.8323e-04 - val_loss: 1.8387e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 1.7539e-04 - val_loss: 2.0457e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8049e-04 - val_loss: 1.8881e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 1.7890e-04 - val_loss: 1.9408e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 1.8456e-04 - val_loss: 2.9990e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8262e-04 - val_loss: 2.2299e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step\n",
      "Model 6 Test Set RMSE: 0.120244 AU\n",
      "\n",
      "--- Training MLP Model 7 (Seed: 7) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0480 - val_loss: 0.0334 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.0184 - val_loss: 0.0212 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 0.0123 - val_loss: 0.0164 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 0.0084 - val_loss: 0.0127 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0058 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 0.0043 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 0.0031 - val_loss: 0.0102 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0025 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 0.0022 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0017 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 0.0015 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.0015 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 0.0012 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 0.0011 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 0.0011 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 9.3614e-04 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 0.0013 - val_loss: 0.0115 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0010 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 7.7751e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 8.8109e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 7.8526e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 8.7633e-04 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 6.7778e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 7.6256e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 6.6229e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 8.4096e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 7.1470e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 5.5756e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 5.6727e-04 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 6.3859e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 6.5027e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 6.2108e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 6.5426e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - loss: 6.1367e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 6.6385e-04 - val_loss: 9.4008e-04 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 4.5654e-04 - val_loss: 7.3045e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 6.0276e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 5.0073e-04 - val_loss: 9.3279e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 5.0760e-04 - val_loss: 9.4170e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 5.2668e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 6.2734e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 4.5802e-04 - val_loss: 7.8770e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 4.9722e-04 - val_loss: 5.0127e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 3.9616e-04 - val_loss: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 6.5995e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 5.2036e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 4.6005e-04 - val_loss: 5.1939e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 4.6094e-04 - val_loss: 7.4552e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 4.7923e-04 - val_loss: 9.6109e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 4.4122e-04 - val_loss: 8.6261e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 3.3675e-04 - val_loss: 4.4194e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 3.1831e-04 - val_loss: 4.0403e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 3.1967e-04 - val_loss: 4.1104e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 3.3337e-04 - val_loss: 8.0840e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 3.2267e-04 - val_loss: 4.0764e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 3.3261e-04 - val_loss: 4.6068e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 3.2937e-04 - val_loss: 3.9223e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 3.0156e-04 - val_loss: 3.8621e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 3.1551e-04 - val_loss: 4.4887e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 3.2686e-04 - val_loss: 3.9666e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 2.9143e-04 - val_loss: 4.0042e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 2.9266e-04 - val_loss: 3.6244e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 3.1340e-04 - val_loss: 6.1594e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 3.0169e-04 - val_loss: 4.0723e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 2.9717e-04 - val_loss: 3.2863e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 2.9547e-04 - val_loss: 3.2724e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 2.7273e-04 - val_loss: 4.0847e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 3.0541e-04 - val_loss: 3.1104e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 2.8127e-04 - val_loss: 3.5698e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 3.0198e-04 - val_loss: 8.2117e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 2.8766e-04 - val_loss: 2.8389e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 2.8646e-04 - val_loss: 4.3669e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 2.6639e-04 - val_loss: 3.7357e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 2.9387e-04 - val_loss: 3.4852e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 2.6305e-04 - val_loss: 2.8318e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2.8388e-04 - val_loss: 3.9679e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 2.7169e-04 - val_loss: 3.6033e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 2.8169e-04 - val_loss: 3.0316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 3.0074e-04 - val_loss: 4.1312e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 2.6896e-04 - val_loss: 2.7119e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 2.5299e-04 - val_loss: 2.9454e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 2.7004e-04 - val_loss: 7.5316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 2.7791e-04 - val_loss: 5.8738e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 2.7348e-04 - val_loss: 2.5684e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 2.6274e-04 - val_loss: 4.4780e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 2.5500e-04 - val_loss: 2.8013e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 2.7245e-04 - val_loss: 3.0028e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 2.5034e-04 - val_loss: 3.2181e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 2.6991e-04 - val_loss: 5.2367e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 2.5449e-04 - val_loss: 9.1052e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 2.9341e-04 - val_loss: 5.0631e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 2.5677e-04 - val_loss: 2.7103e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 2.5595e-04 - val_loss: 4.1037e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 2.5300e-04 - val_loss: 4.5839e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 2.5621e-04 - val_loss: 2.8674e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2.3353e-04 - val_loss: 2.4871e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 2.4513e-04 - val_loss: 2.5390e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 2.6654e-04 - val_loss: 3.2450e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 2.4108e-04 - val_loss: 2.5595e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 3.3924e-04 - val_loss: 0.0043 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 2.3840e-04 - val_loss: 2.2841e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 2.0629e-04 - val_loss: 2.3992e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 2.0653e-04 - val_loss: 2.2465e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 2.0686e-04 - val_loss: 2.2215e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 2.0738e-04 - val_loss: 2.8153e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 2.1328e-04 - val_loss: 2.4953e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 2.0979e-04 - val_loss: 2.5418e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 2.0888e-04 - val_loss: 2.2777e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 2.1074e-04 - val_loss: 2.2198e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.1364e-04 - val_loss: 2.6356e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 2.0634e-04 - val_loss: 2.7741e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 2.1150e-04 - val_loss: 2.8340e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 2.1285e-04 - val_loss: 2.1438e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 2.0410e-04 - val_loss: 3.0767e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 2.0441e-04 - val_loss: 3.3623e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 2.0433e-04 - val_loss: 2.1947e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 2.0275e-04 - val_loss: 2.2917e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 2.0650e-04 - val_loss: 2.6424e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 2.0475e-04 - val_loss: 2.2032e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 2.0056e-04 - val_loss: 2.5096e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 1.9681e-04 - val_loss: 2.6785e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.0428e-04 - val_loss: 2.2826e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 2.0345e-04 - val_loss: 2.0356e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 1.9439e-04 - val_loss: 2.1709e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 1.9532e-04 - val_loss: 2.4137e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.9777e-04 - val_loss: 2.0645e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 1.9812e-04 - val_loss: 2.3243e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 1.9855e-04 - val_loss: 2.1805e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 1.9262e-04 - val_loss: 2.1849e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 1.9348e-04 - val_loss: 2.3418e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.9239e-04 - val_loss: 2.0262e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 1.9074e-04 - val_loss: 2.0919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 1.9009e-04 - val_loss: 3.0894e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 1.9353e-04 - val_loss: 2.9314e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 1.9146e-04 - val_loss: 2.3313e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 1.9346e-04 - val_loss: 2.3098e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 1.8888e-04 - val_loss: 2.6047e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.8649e-04 - val_loss: 1.9759e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.8919e-04 - val_loss: 2.2936e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 1.8688e-04 - val_loss: 2.2181e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 1.8968e-04 - val_loss: 3.2717e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 1.9473e-04 - val_loss: 2.2681e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.8499e-04 - val_loss: 2.1288e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 1.8028e-04 - val_loss: 1.9000e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.8334e-04 - val_loss: 2.1943e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 1.8684e-04 - val_loss: 1.9216e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 1.8678e-04 - val_loss: 1.9978e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 1.8121e-04 - val_loss: 2.1078e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.7853e-04 - val_loss: 2.2535e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 1.8286e-04 - val_loss: 2.0624e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step\n",
      "Model 7 Test Set RMSE: 0.105186 AU\n",
      "\n",
      "--- Training MLP Model 8 (Seed: 8) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852us/step - loss: 0.0441 - val_loss: 0.0416 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.0163 - val_loss: 0.0289 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.0111 - val_loss: 0.0339 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.0076 - val_loss: 0.0201 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.0056 - val_loss: 0.0172 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.0040 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 0.0032 - val_loss: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.0027 - val_loss: 0.0127 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 0.0021 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.0019 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.0017 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 0.0014 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.0014 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.0011 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.0013 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.0010 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 9.5922e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 9.4226e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 8.1078e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.0010 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 7.7864e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 7.3408e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 7.6205e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 8.7973e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 6.4035e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 7.8496e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 6.5415e-04 - val_loss: 9.5612e-04 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 5.3707e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 6.3595e-04 - val_loss: 9.4345e-04 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 6.5044e-04 - val_loss: 7.9195e-04 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 5.2035e-04 - val_loss: 7.2001e-04 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 5.6903e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 5.6919e-04 - val_loss: 8.3141e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 4.8104e-04 - val_loss: 9.8152e-04 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 6.7713e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.2912e-04 - val_loss: 9.2612e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 5.1258e-04 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 5.2975e-04 - val_loss: 5.9352e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 4.7853e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 5.2986e-04 - val_loss: 6.2768e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 4.4515e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.6857e-04 - val_loss: 7.8988e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.0660e-04 - val_loss: 4.3076e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1527e-04 - val_loss: 7.3599e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 4.4571e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 5.2112e-04 - val_loss: 6.1067e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 4.6728e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 3.9594e-04 - val_loss: 4.6016e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 5.0809e-04 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.3019e-04 - val_loss: 4.3805e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 3.0109e-04 - val_loss: 3.7797e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 2.9578e-04 - val_loss: 3.6576e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 2.9606e-04 - val_loss: 3.6864e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 3.0097e-04 - val_loss: 5.9727e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 3.2023e-04 - val_loss: 5.1382e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 3.0732e-04 - val_loss: 5.5000e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 3.0109e-04 - val_loss: 3.2227e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.8832e-04 - val_loss: 4.6064e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 3.2739e-04 - val_loss: 3.1096e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 2.8620e-04 - val_loss: 3.0901e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 2.7477e-04 - val_loss: 4.0237e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 2.7518e-04 - val_loss: 4.6219e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.9783e-04 - val_loss: 8.1298e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2.7281e-04 - val_loss: 3.8316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 999us/step - loss: 2.9550e-04 - val_loss: 2.6760e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.6868e-04 - val_loss: 3.7432e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.6140e-04 - val_loss: 3.3018e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.8846e-04 - val_loss: 3.3316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2.6784e-04 - val_loss: 2.9804e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 2.6440e-04 - val_loss: 3.1431e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 2.6358e-04 - val_loss: 3.3013e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 2.8778e-04 - val_loss: 3.6627e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 2.4840e-04 - val_loss: 2.9204e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 2.9229e-04 - val_loss: 2.9239e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 2.4878e-04 - val_loss: 3.8784e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 2.5956e-04 - val_loss: 2.4591e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.5328e-04 - val_loss: 2.8988e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 2.5920e-04 - val_loss: 3.2652e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.3795e-04 - val_loss: 2.8665e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 2.5137e-04 - val_loss: 5.7735e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.5539e-04 - val_loss: 2.6658e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 2.7701e-04 - val_loss: 5.2053e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.5536e-04 - val_loss: 3.8800e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.2521e-04 - val_loss: 2.3446e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 2.4805e-04 - val_loss: 2.5529e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.4322e-04 - val_loss: 4.3839e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 2.5653e-04 - val_loss: 3.1837e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 2.2966e-04 - val_loss: 4.3172e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 2.4288e-04 - val_loss: 2.4620e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 2.4545e-04 - val_loss: 4.4832e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 2.2769e-04 - val_loss: 2.3447e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 2.3848e-04 - val_loss: 4.5552e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 2.3424e-04 - val_loss: 4.9994e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 2.4862e-04 - val_loss: 2.5688e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 2.2368e-04 - val_loss: 2.7569e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.2731e-04 - val_loss: 2.9781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 2.2281e-04 - val_loss: 7.5836e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.3773e-04 - val_loss: 8.0312e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 2.3091e-04 - val_loss: 2.5579e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 2.4082e-04 - val_loss: 2.4931e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 1.9328e-04 - val_loss: 2.2173e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 1.9178e-04 - val_loss: 2.2495e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 1.9467e-04 - val_loss: 2.0544e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 1.9488e-04 - val_loss: 2.2964e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 1.9627e-04 - val_loss: 2.1681e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 1.9931e-04 - val_loss: 2.1064e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 1.9342e-04 - val_loss: 2.4325e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.9408e-04 - val_loss: 2.0031e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 2.0171e-04 - val_loss: 2.1045e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 1.9035e-04 - val_loss: 3.0184e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 1.9298e-04 - val_loss: 2.2992e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 1.9028e-04 - val_loss: 2.1145e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 1.9132e-04 - val_loss: 2.2491e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 1.9387e-04 - val_loss: 2.5381e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 1.8715e-04 - val_loss: 2.2136e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 1.8845e-04 - val_loss: 2.1571e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 1.8655e-04 - val_loss: 2.7427e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.8933e-04 - val_loss: 3.4572e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 1.8699e-04 - val_loss: 2.0380e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 1.8462e-04 - val_loss: 2.8801e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 1.8673e-04 - val_loss: 2.4195e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 1.8355e-04 - val_loss: 1.8576e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.8033e-04 - val_loss: 1.9529e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 1.8649e-04 - val_loss: 3.0852e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.8213e-04 - val_loss: 2.1394e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 1.7716e-04 - val_loss: 2.3446e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.7821e-04 - val_loss: 2.1511e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 1.8008e-04 - val_loss: 1.9269e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 1.8074e-04 - val_loss: 1.9213e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.7452e-04 - val_loss: 2.0625e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 1.8202e-04 - val_loss: 2.2649e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 1.7553e-04 - val_loss: 2.4095e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 1.7944e-04 - val_loss: 1.9785e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 1.7146e-04 - val_loss: 1.9758e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 1.8646e-04 - val_loss: 3.4393e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.8028e-04 - val_loss: 1.9809e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 1.7091e-04 - val_loss: 1.8341e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 1.7575e-04 - val_loss: 1.8800e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 1.7697e-04 - val_loss: 1.8660e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 1.7050e-04 - val_loss: 1.8209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 1.7269e-04 - val_loss: 1.8908e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 1.7167e-04 - val_loss: 2.1956e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.7945e-04 - val_loss: 1.8520e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 1.6662e-04 - val_loss: 1.6998e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 1.6797e-04 - val_loss: 1.7209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 1.7025e-04 - val_loss: 2.0405e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 1.7075e-04 - val_loss: 1.8877e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.7025e-04 - val_loss: 1.9792e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.6834e-04 - val_loss: 1.8686e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 1.6464e-04 - val_loss: 1.8609e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step\n",
      "Model 8 Test Set RMSE: 0.130518 AU\n",
      "\n",
      "--- Training MLP Model 9 (Seed: 9) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812us/step - loss: 0.0428 - val_loss: 0.0382 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.0176 - val_loss: 0.0275 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.0121 - val_loss: 0.0216 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0087 - val_loss: 0.0199 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.0061 - val_loss: 0.0143 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.0044 - val_loss: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.0036 - val_loss: 0.0103 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0026 - val_loss: 0.0078 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.0022 - val_loss: 0.0093 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.0020 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.0017 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.0014 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.0013 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 0.0013 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 0.0011 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 0.0012 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 9.6191e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 8.4618e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 9.4274e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 0.0011 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 6.8384e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 7.3224e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 9.5914e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 7.2903e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 7.1192e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 6.5967e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 6.7125e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 5.6647e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 7.1242e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 5.3806e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 6.1576e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 6.0722e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 5.6584e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 5.7587e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.7037e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 4.7398e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 5.2182e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 5.6348e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 5.4083e-04 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 5.9672e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 5.2560e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.8233e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 5.4385e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 4.8665e-04 - val_loss: 6.9777e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 4.7644e-04 - val_loss: 7.9304e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.5303e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 4.5348e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 5.3707e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 3.7829e-04 - val_loss: 7.4692e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.9466e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 3.1402e-04 - val_loss: 4.8648e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 3.0978e-04 - val_loss: 4.8527e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 3.1069e-04 - val_loss: 4.4502e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 3.1251e-04 - val_loss: 4.0831e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 3.1767e-04 - val_loss: 4.9604e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 3.2908e-04 - val_loss: 4.0918e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 3.3343e-04 - val_loss: 3.9649e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 2.9451e-04 - val_loss: 5.8165e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 3.1680e-04 - val_loss: 3.3669e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.9148e-04 - val_loss: 3.5593e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 3.0277e-04 - val_loss: 5.4972e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 2.8197e-04 - val_loss: 3.3816e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 3.0724e-04 - val_loss: 3.1472e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.7496e-04 - val_loss: 3.0665e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 2.9224e-04 - val_loss: 2.8598e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 2.7165e-04 - val_loss: 3.1026e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 2.9755e-04 - val_loss: 3.7114e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 2.7227e-04 - val_loss: 4.2637e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 2.8420e-04 - val_loss: 5.4160e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 2.6990e-04 - val_loss: 3.2841e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 2.6989e-04 - val_loss: 3.9652e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 2.6693e-04 - val_loss: 4.0568e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.6710e-04 - val_loss: 2.7920e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 2.6838e-04 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 2.9779e-04 - val_loss: 2.7796e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 2.9121e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.8266e-04 - val_loss: 2.5925e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.4506e-04 - val_loss: 2.7968e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.7078e-04 - val_loss: 2.5297e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 2.5490e-04 - val_loss: 9.6003e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.7943e-04 - val_loss: 7.3712e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.6521e-04 - val_loss: 2.8641e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.6395e-04 - val_loss: 3.3962e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.7471e-04 - val_loss: 2.8846e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.4553e-04 - val_loss: 3.6945e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 2.3296e-04 - val_loss: 6.6387e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.4726e-04 - val_loss: 2.3969e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 2.4655e-04 - val_loss: 5.9328e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.4852e-04 - val_loss: 3.3150e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 2.4371e-04 - val_loss: 6.4855e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 2.5397e-04 - val_loss: 2.8830e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.5818e-04 - val_loss: 3.1706e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 2.2847e-04 - val_loss: 2.7281e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 2.3999e-04 - val_loss: 3.1771e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.3428e-04 - val_loss: 2.9174e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 2.4275e-04 - val_loss: 2.4588e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 2.3735e-04 - val_loss: 4.5365e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.5991e-04 - val_loss: 5.4565e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 2.5727e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 2.3661e-04 - val_loss: 2.6367e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 1.9752e-04 - val_loss: 2.1933e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 1.9468e-04 - val_loss: 2.0857e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.9736e-04 - val_loss: 2.1754e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 1.9916e-04 - val_loss: 2.3659e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.0013e-04 - val_loss: 2.7335e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 1.9972e-04 - val_loss: 2.4758e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 2.0191e-04 - val_loss: 3.5541e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.0294e-04 - val_loss: 2.3350e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.9595e-04 - val_loss: 2.0179e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.9461e-04 - val_loss: 2.0071e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 1.9298e-04 - val_loss: 2.5066e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 1.9341e-04 - val_loss: 2.7124e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9522e-04 - val_loss: 2.1172e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 1.9648e-04 - val_loss: 2.7776e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9484e-04 - val_loss: 1.9856e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 1.9015e-04 - val_loss: 1.9239e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9048e-04 - val_loss: 2.0219e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 1.8895e-04 - val_loss: 1.8927e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 1.9103e-04 - val_loss: 1.8866e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 1.8427e-04 - val_loss: 1.9567e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 1.9235e-04 - val_loss: 2.2327e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 1.8325e-04 - val_loss: 2.0606e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.8740e-04 - val_loss: 1.7947e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 1.8077e-04 - val_loss: 1.9976e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 1.8360e-04 - val_loss: 4.0120e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 1.8643e-04 - val_loss: 1.9656e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 1.8360e-04 - val_loss: 4.8416e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.8833e-04 - val_loss: 1.9066e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 1.8138e-04 - val_loss: 2.4234e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 1.8408e-04 - val_loss: 2.5000e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 1.8253e-04 - val_loss: 3.8673e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.7632e-04 - val_loss: 1.8440e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 1.8177e-04 - val_loss: 1.8703e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 1.7915e-04 - val_loss: 1.8756e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 1.7691e-04 - val_loss: 1.9698e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 1.8435e-04 - val_loss: 2.1973e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 1.8110e-04 - val_loss: 1.9076e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.7535e-04 - val_loss: 1.8189e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 1.7397e-04 - val_loss: 2.1538e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 1.8164e-04 - val_loss: 1.7776e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.7271e-04 - val_loss: 1.8284e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 1.8738e-04 - val_loss: 2.3156e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 1.7016e-04 - val_loss: 1.8853e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 1.7347e-04 - val_loss: 3.3119e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 1.7313e-04 - val_loss: 1.9469e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.7030e-04 - val_loss: 2.1229e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 1.7663e-04 - val_loss: 2.0995e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 1.6671e-04 - val_loss: 1.6842e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 1.6929e-04 - val_loss: 2.9857e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 1.7855e-04 - val_loss: 2.0609e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step\n",
      "Model 9 Test Set RMSE: 0.125965 AU\n",
      "\n",
      "--- Final Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.088973 AU\n",
      "Ensemble Y-coordinate RMSE: 0.095098 AU\n",
      "Ensemble Z-coordinate RMSE: 0.078441 AU\n",
      "\n",
      "--- Final Comparison ---\n",
      "Best Single MLP Overall RMSE: 0.003100 AU\n",
      "Multi-MLP Ensemble Overall RMSE: 0.078441 AU\n",
      "\n",
      "--- Best Individual Model Summary ---\n",
      "Best Individual Model (Index 1) achieved RMSE: 0.003779 AU\n",
      "The random seed used for this best model was: 1\n",
      "\n",
      "CONCLUSION: The original single MLP model remains the most accurate.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CRITICAL FIX REMOVED: tf.config.run_functions_eagerly(True) has been removed.\n",
    "# This should revert to fast Graph Execution mode.\n",
    "\n",
    "# --- ASSUME YOUR DATA IS ALREADY LOADED AND PREPROCESSED ---\n",
    "# X_train_scaled, y_train, X_test_scaled, y_test\n",
    "# (Assuming your data variables are correctly loaded in the environment)\n",
    "\n",
    "# --- NEW: OPTIMIZE DATA LOADING FOR PERFORMANCE (FASTEST PATH) ---\n",
    "# We define raw slices once and optimize the pipeline per model inside the loop.\n",
    "BATCH_SIZE = 32 # Standard batch size\n",
    "AUTOTUNE = tf.data.AUTOTUNE \n",
    "\n",
    "# 1. Convert NumPy arrays to raw, unshuffled tf.data.Dataset slices (defined once)\n",
    "raw_train_dataset = Dataset.from_tensor_slices((X_train_scaled, y_train.values.astype(np.float32)))\n",
    "raw_test_dataset = Dataset.from_tensor_slices((X_test_scaled, y_test.values.astype(np.float32)))\n",
    "\n",
    "\n",
    "# --- MODEL CONFIGURATION ---\n",
    "NUM_MODELS = 9 # Assuming you set this to 9 in your environment\n",
    "# Set a high number of epochs since Early Stopping will handle when to stop\n",
    "EPOCHS = 5000   \n",
    "INPUT_SHAPE = X_train_scaled.shape[1] # Number of input features\n",
    "models = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Variables for tracking the best individual model\n",
    "best_individual_rmse = float('inf')\n",
    "best_model_index = -1\n",
    "best_model_seed = -1\n",
    "best_model_predictions = None\n",
    "# Replace the value below with your actual best single MLP RMSE (approx. 0.0031)\n",
    "BEST_SINGLE_MLP_RMSE = 0.003100\n",
    "\n",
    "\n",
    "# --- Define Callbacks for Training ---\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=150,        \n",
    "    restore_best_weights=True,\n",
    "    # REVERTED: Removed min_delta to use the default (0.0), which aligns with the original, best-performing setup.\n",
    ")\n",
    "\n",
    "lr_on_plateau_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5,         \n",
    "    patience=50,        \n",
    "    min_lr=1e-7         \n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, lr_on_plateau_callback]\n",
    "\n",
    "# --- 1. BUILD AND TRAIN THE INDIVIDUAL MLP MODELS ---\n",
    "print(f\"Building and training {NUM_MODELS} diverse MLP models...\")\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    tf.keras.backend.clear_session()\n",
    "    current_seed = i + 1\n",
    "    # Set the seed for model weight initialization\n",
    "    tf.keras.utils.set_random_seed(current_seed)\n",
    "    \n",
    "    # --- CRITICAL FIX: Create unique shuffled datasets for each model ---\n",
    "    # 2. Shuffle, batch, and prefetch for optimal pipeline speed\n",
    "    # We use the current_seed to ensure each model sees the data in a different order.\n",
    "    train_dataset = raw_train_dataset.shuffle(buffer_size=1024, seed=current_seed).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    test_dataset = raw_test_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    \n",
    "    # --- MODEL ARCHITECTURE REVERTED TO ORIGINAL HIGH-PERFORMANCE STRUCTURE ---\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', \n",
    "              kernel_regularizer=l2(0.0001), # Reverted L2 to original 0.0001\n",
    "              input_shape=(INPUT_SHAPE,)), \n",
    "        \n",
    "        Dense(256, activation='relu',\n",
    "              kernel_regularizer=l2(0.0001)), # Reverted L2 to original 0.0001\n",
    "        \n",
    "        Dense(128, activation='relu', \n",
    "              kernel_regularizer=l2(0.0001)), # Reverted L2 to original 0.0001\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "\n",
    "        Dense(3, activation='linear') \n",
    "    ])\n",
    "    # --- END REVERT ---\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"\\n--- Training MLP Model {i+1} (Seed: {current_seed}) ---\")\n",
    "    \n",
    "    # Use the optimized datasets for fastest training\n",
    "    model.fit(\n",
    "        train_dataset, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=test_dataset, \n",
    "        callbacks=callbacks, \n",
    "        verbose=1 \n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    # Generate predictions using the raw NumPy array (this usually runs fine)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    # Calculate and print the individual model's RMSE for comparison\n",
    "    rmse_x = np.sqrt(mean_squared_error(y_test.values[:, 0], y_pred[:, 0]))\n",
    "    rmse_y = np.sqrt(mean_squared_error(y_test.values[:, 1], y_pred[:, 1]))\n",
    "    rmse_z = np.sqrt(mean_squared_error(y_test.values[:, 2], y_pred[:, 2]))\n",
    "    overall_rmse = np.mean([rmse_x, rmse_y, rmse_z])\n",
    "    \n",
    "    print(f\"Model {i+1} Test Set RMSE: {overall_rmse:.6f} AU\")\n",
    "    \n",
    "    if overall_rmse < best_individual_rmse:\n",
    "        best_individual_rmse = overall_rmse\n",
    "        best_model_index = i\n",
    "        best_model_seed = current_seed\n",
    "        best_model_predictions = y_pred.copy()\n",
    "\n",
    "# --- 2. CREATE THE ENSEMBLE PREDICTION ---\n",
    "y_ensemble_pred_au = np.mean(y_pred_list, axis=0)\n",
    "\n",
    "# --- 3. EVALUATE THE ENSEMBLE PERFORMANCE ---\n",
    "print(\"\\n--- Final Ensemble Evaluation ---\")\n",
    "rmse_x_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 0], y_ensemble_pred_au[:, 0]))\n",
    "rmse_y_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 1], y_ensemble_pred_au[:, 1]))\n",
    "rmse_z_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 2], y_ensemble_pred_au[:, 2]))\n",
    "overall_ensemble_rmse = np.mean([rmse_x_ensemble, rmse_y_ensemble, rmse_z_ensemble])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# --- 4. Final Comparison and Extraction of Best Model ---\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "print(f\"Best Single MLP Overall RMSE: {BEST_SINGLE_MLP_RMSE:.6f} AU\")\n",
    "print(f\"Multi-MLP Ensemble Overall RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "print(f\"\\n--- Best Individual Model Summary ---\")\n",
    "print(f\"Best Individual Model (Index {best_model_index+1}) achieved RMSE: {best_individual_rmse:.6f} AU\")\n",
    "print(f\"The random seed used for this best model was: {best_model_seed}\")\n",
    "\n",
    "# --- 5. SAVING THE BEST MODEL ---\n",
    "if overall_ensemble_rmse < best_individual_rmse and overall_ensemble_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(\"\\nCONCLUSION: The Multi-MLP Ensemble is the most accurate predictor.\")\n",
    "    MODEL_SAVE_NAME = \"final_best_ensemble_predictions.npy\"\n",
    "    np.save(MODEL_SAVE_NAME, y_ensemble_pred_au)\n",
    "    print(f\"Saved FINAL ENSEMBLE PREDICTIONS to: {MODEL_SAVE_NAME}\")\n",
    "    \n",
    "elif best_individual_rmse < overall_ensemble_rmse and best_individual_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(f\"\\nCONCLUSION: A new individual MLP model (Index {best_model_index+1}) is the most accurate predictor.\")\n",
    "    \n",
    "    final_best_model = models[best_model_index]\n",
    "    MODEL_SAVE_NAME = f\"best_mlp_model_seed_{best_model_seed}.h5\" \n",
    "    \n",
    "    # Check if file exists before saving\n",
    "    if os.path.exists(MODEL_SAVE_NAME):\n",
    "        print(f\"Warning: File {MODEL_SAVE_NAME} already exists. Skipping save to prevent overwrite.\")\n",
    "    else:\n",
    "        final_best_model.save(MODEL_SAVE_NAME)\n",
    "        print(f\"Saved FINAL BEST INDIVIDUAL MODEL to: {MODEL_SAVE_NAME}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nCONCLUSION: The original single MLP model remains the most accurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
