{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skyfield.api import load, Topos, Star, utc\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import stars_utils\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_FILENAME = 'mars_position_predictor_final.keras' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JPL ephemeris data file\n",
    "planets = load('de421.bsp')\n",
    "ts = load.timescale()\n",
    "mars = planets['mars']\n",
    "earth = planets['earth']\n",
    "\n",
    "# 2. Define Time Range (e.g., 50 years of data, 1970 to 2020)\n",
    "start_date = datetime(1970, 1, 1)\n",
    "end_date = datetime(2025, 1, 1)\n",
    "time_step = timedelta(days=1)  # Data point every week\n",
    "\n",
    "# Generate a list of dates\n",
    "dates = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    dates.append(current_date)\n",
    "    current_date += time_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created successfully with 20090 data points.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert Python datetimes to Skyfield time objects\n",
    "timezone_aware_dates = [d.replace(tzinfo=utc) for d in dates]\n",
    "t = ts.utc(timezone_aware_dates)\n",
    "\n",
    "# 3. Calculate Geocentric Position (Position as seen from Earth)\n",
    "# astrometric_position is the calculation of a body's position in space\n",
    "# as seen from a specific location (Earth).\n",
    "astrometric = earth.at(t).observe(mars)\n",
    "\n",
    "# Get Right Ascension and Declination\n",
    "# These are your primary target variables for prediction!\n",
    "ra, dec, distance = astrometric.radec()\n",
    "\n",
    "# 4. Create the DataFrame (Your Dataset)\n",
    "data = {\n",
    "    'Time_UTC': dates,\n",
    "    'Julian_Date': t.tdb,\n",
    "    'RA_deg': ra.degrees,      # Right Ascension (Target Variable)\n",
    "    'Dec_deg': dec.degrees     # Declination (Target Variable)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 5. Save the Dataset\n",
    "df.to_csv('mars_ephemeris_data.csv', index=False)\n",
    "print(\"Dataset created successfully with\", len(df), \"data points.\")\n",
    "\n",
    "mars_sf_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20090, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_sf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the astronomic xyz coordinates of mars from earth's pov\n",
    "astrometric = earth.at(t).observe(mars)\n",
    "pos_vector_au = astrometric.xyz.au #pos_vector_au is a 3xN NumPy array, where N is the number of time steps (t).\n",
    "\n",
    "data = {\n",
    "    'Time_UTC': dates,\n",
    "    'Julian_Date': t.tdb,\n",
    "    \n",
    "    # Extract the X, Y, and Z components\n",
    "    'X_au': pos_vector_au[0],  # X-coordinate\n",
    "    'Y_au': pos_vector_au[1],  # Y-coordinate\n",
    "    'Z_au': pos_vector_au[2]   # Z-coordinate\n",
    "}\n",
    "\n",
    "mars_sf_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_min = mars_sf_df.Julian_Date.min()\n",
    "mars_sf_df[\"Time_Index\"] = mars_sf_df.Julian_Date - JD_min\n",
    "time_index = mars_sf_df[\"Time_Index\"]\n",
    "\n",
    "mars_sf_df[\"Time_Index_2\"] = time_index ** 2\n",
    "mars_sf_df[\"Time_Index_3\"] = time_index ** 3\n",
    "\n",
    "# Add features to the dataframe\n",
    "PERIOD_YEAR = 365.25\n",
    "PERIOD_MARS = 686.98\n",
    "PERIOD_SYNODIC = 780.0\n",
    "PERIOD_JUPITER = 4332.6 # mars cycles are influenced by jupiters movement. added this for fixing.\n",
    "\n",
    "# Earth's Annual Cycle Features\n",
    "mars_sf_df[\"Sin_Year\"] = np.sin(2*np.pi*time_index/PERIOD_YEAR)\n",
    "mars_sf_df[\"Cos_Year\"] = np.cos(2*np.pi*time_index/PERIOD_YEAR)\n",
    "\n",
    "# Mars's Orbital Cycle Features (NEW and CRUCIAL)\n",
    "mars_sf_df['Sin_Mars'] = np.sin(2 * np.pi * time_index / PERIOD_MARS)\n",
    "mars_sf_df['Cos_Mars'] = np.cos(2 * np.pi * time_index / PERIOD_MARS)\n",
    "\n",
    "# Mars-Earth Synodic Cycle Features\n",
    "mars_sf_df[\"Sin_Synodic\"] = np.sin(2*np.pi*time_index/PERIOD_SYNODIC)\n",
    "mars_sf_df[\"Cos_Synodic\"] = np.cos(2*np.pi*time_index/PERIOD_SYNODIC)\n",
    "\n",
    "# Jupiter's Annual Cycle Features\n",
    "mars_sf_df['Sin_Jupiter'] = np.sin(2 * np.pi * time_index / PERIOD_JUPITER)\n",
    "mars_sf_df['Cos_Jupiter'] = np.cos(2 * np.pi * time_index / PERIOD_JUPITER)\n",
    "\n",
    "mars_sf_df['Sin_Year_Sin_Synodic'] = mars_sf_df['Sin_Year'] * mars_sf_df['Sin_Synodic']\n",
    "mars_sf_df['Sin_Year_Cos_Synodic'] = mars_sf_df['Sin_Year'] * mars_sf_df['Cos_Synodic']\n",
    "mars_sf_df['Cos_Year_Sin_Synodic'] = mars_sf_df['Cos_Year'] * mars_sf_df['Sin_Synodic']\n",
    "mars_sf_df['Cos_Year_Cos_Synodic'] = mars_sf_df['Cos_Year'] * mars_sf_df['Cos_Synodic']\n",
    "\n",
    "# Define features (x) and targets (y) for the model \n",
    "FEATURES = [\n",
    "    'Time_Index', 'Time_Index_2', 'Time_Index_3', \n",
    "    'Sin_Year', 'Cos_Year',\n",
    "    'Sin_Mars', 'Cos_Mars',\n",
    "    'Sin_Synodic', 'Cos_Synodic',\n",
    "]\n",
    "TARGETS = ['X_au', 'Y_au', 'Z_au']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mars_sf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X = \u001b[43mmars_sf_df\u001b[49m[FEATURES]\n\u001b[32m      2\u001b[39m y = mars_sf_df[TARGETS]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 80/20 split of the data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'mars_sf_df' is not defined"
     ]
    }
   ],
   "source": [
    "X = mars_sf_df[FEATURES]\n",
    "y = mars_sf_df[TARGETS]\n",
    "\n",
    "# 80/20 split of the data\n",
    "split_point = int(len(mars_sf_df)*0.8)\n",
    "\n",
    "X_train = X[:split_point]\n",
    "X_test = X[split_point:]\n",
    "\n",
    "y_train = y[:split_point]\n",
    "y_test = y[split_point:]\n",
    "\n",
    "# apply standart scalar\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "SCALER_FILEPATH = os.path.join(MODEL_DIR, 'feature_scaler.pkl')\n",
    "stars_utils.save_scaler(scaler, SCALER_FILEPATH)\n",
    "\n",
    "# Fit and transform the training targets\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "# Transform the testing targets\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "print(f\"Total data points: {len(X)}\")\n",
    "print(f\"Training period ends at index {split_point} (Date: {df.iloc[split_point]['Time_UTC']})\")\n",
    "print(f\"Testing period starts at index {split_point} (Date: {df.iloc[split_point]['Time_UTC']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Neural Network training (100 epochs)...\n",
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 886us/step - loss: 0.0532 - val_loss: 0.0392 - learning_rate: 0.0010\n",
      "Epoch 2/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0209 - val_loss: 0.0257 - learning_rate: 0.0010\n",
      "Epoch 3/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.0129 - val_loss: 0.0178 - learning_rate: 0.0010\n",
      "Epoch 4/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.0084 - val_loss: 0.0131 - learning_rate: 0.0010\n",
      "Epoch 5/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.0056 - val_loss: 0.0149 - learning_rate: 0.0010\n",
      "Epoch 6/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0043 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 7/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0032 - val_loss: 0.0087 - learning_rate: 0.0010\n",
      "Epoch 8/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0028 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 9/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.0022 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 10/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.0021 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 11/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.0018 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 12/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.0016 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 13/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 14/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.0014 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 15/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 0.0017 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 16/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 17/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0013 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 18/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0013 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 19/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 20/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0013 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 21/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 22/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 23/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 24/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 25/10000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0011 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 26/10000\n",
      "\u001b[1m260/503\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.0013"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# use 10000 spochs with batch size of 32\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Neural Network training (100 epochs)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_scaled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m              \u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel training complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# MLP NN Implemantation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# use early stopping for when there is no improvement for 150 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=150, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Reduce the LR by 50% (factor=0.5), If no improvement for 50 epochs, drop the LR, Don't let the LR drop below 1e-7 (0.0000001)\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,       \n",
    "    patience=50,      \n",
    "    min_lr=1e-7       \n",
    ")\n",
    "\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) \n",
    "regularizer_strength = 0.001 \n",
    "\n",
    "model = Sequential([\n",
    "    # Apply kernel_regularizer to hidden layers\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0001), \n",
    "          input_shape=(X_train_scaled.shape[1],)), \n",
    "    \n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0001)), \n",
    "    \n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(3, activation='linear') \n",
    "])\n",
    "\n",
    "model.compile(optimizer=custom_optimizer, loss='mse') \n",
    "\n",
    "# use 10000 spochs with batch size of 32\n",
    "print(\"\\nStarting Neural Network training (100 epochs)...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train_scaled, \n",
    "    epochs=10000,             \n",
    "    batch_size=32,          \n",
    "    validation_data=(X_test_scaled, y_test_scaled),\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    verbose=1              \n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step\n",
      "\n",
      "--- Model Evaluation (Neural Network Test Set) ---\n",
      "Overall Averaged RMSE: 0.019528 AU\n",
      "X-coordinate RMSE: 0.003944 AU\n",
      "Y-coordinate RMSE: 0.003822 AU\n",
      "Z-coordinate RMSE: 0.001565 AU\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# get inverse value (becuase of the scaling)\n",
    "y_pred_mlp_au = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate the loss (MSE) on the test set\n",
    "y_test_np = y_test.values\n",
    "loss_mse = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "rmse_au = np.sqrt(loss_mse) \n",
    "\n",
    "# Calculate RMSE for each coordinate individually\n",
    "rmse_x = np.sqrt(mean_squared_error(y_test_np[:, 0], y_pred_mlp_au[:, 0]))\n",
    "rmse_y = np.sqrt(mean_squared_error(y_test_np[:, 1], y_pred_mlp_au[:, 1]))\n",
    "rmse_z = np.sqrt(mean_squared_error(y_test_np[:, 2], y_pred_mlp_au[:, 2]))\n",
    "\n",
    "print(\"\\n--- Model Evaluation (Neural Network Test Set) ---\")\n",
    "print(f\"Overall Averaged RMSE: {rmse_au:.6f} AU\")\n",
    "print(f\"X-coordinate RMSE: {rmse_x:.6f} AU\")\n",
    "print(f\"Y-coordinate RMSE: {rmse_y:.6f} AU\")\n",
    "print(f\"Z-coordinate RMSE: {rmse_z:.6f} AU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to models/mars_position_predictor_final.keras\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_FILENAME = 'mars_position_predictor_final.keras' \n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# os.makedirs(MODEL_DIR, exist_ok=True) \n",
    "\n",
    "# Save the model\n",
    "model.save(os.path.join(MODEL_DIR, MODEL_FILENAME))\n",
    "\n",
    "print(f\"Model saved successfully to {os.path.join(MODEL_DIR, MODEL_FILENAME)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Predicted RA/Dec Coordinates (Degrees) ---\n",
      "   Predicted_RA_deg  Predicted_Dec_deg\n",
      "0        191.896774          -2.759812\n",
      "1        192.311584          -2.923764\n",
      "2        192.723251          -3.086056\n",
      "3        193.131683          -3.246643\n",
      "4        193.522125          -3.398860\n"
     ]
    }
   ],
   "source": [
    "# Run the conversion function\n",
    "radec_predictions = stars_utils.xyz_to_radec(y_pred_mlp_au)\n",
    "\n",
    "print(\"--- Predicted RA/Dec Coordinates (Degrees) ---\")\n",
    "print(radec_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training 3 diverse MLP models...\n",
      "\n",
      "--- Training MLP Model 1 ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - loss: 0.0556 - val_loss: 0.0392 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0226 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.0144 - val_loss: 0.0168 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.0093 - val_loss: 0.0109 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0063 - val_loss: 0.0084 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.0047 - val_loss: 0.0067 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0036 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0031 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 0.0024 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0023 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0020 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0020 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.0016 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0016 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.0016 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.0014 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.0014 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.0016 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0013 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0012 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0011 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0010 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0010 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 9.5101e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0010 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 0.0010 - val_loss: 8.9948e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.0011 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 8.7592e-04 - val_loss: 8.6309e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 9.1155e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 9.2967e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 9.2375e-04 - val_loss: 8.4130e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 8.7091e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 9.2739e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 8.6216e-04 - val_loss: 9.2754e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 9.2258e-04 - val_loss: 9.8139e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 9.6546e-04 - val_loss: 9.3370e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 9.8100e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 8.1583e-04 - val_loss: 7.9075e-04 - learning_rate: 0.0010\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.0010 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 8.1817e-04 - val_loss: 8.7728e-04 - learning_rate: 0.0010\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 8.4040e-04 - val_loss: 8.5532e-04 - learning_rate: 0.0010\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 8.6962e-04 - val_loss: 8.8010e-04 - learning_rate: 0.0010\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 8.7326e-04 - val_loss: 9.2959e-04 - learning_rate: 0.0010\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 8.7380e-04 - val_loss: 8.8793e-04 - learning_rate: 0.0010\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 8.0692e-04 - val_loss: 9.3748e-04 - learning_rate: 0.0010\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 8.4274e-04 - val_loss: 9.9855e-04 - learning_rate: 0.0010\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 8.3315e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 8.5083e-04 - val_loss: 9.4423e-04 - learning_rate: 0.0010\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 8.0213e-04 - val_loss: 8.9512e-04 - learning_rate: 0.0010\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 7.9718e-04 - val_loss: 7.9676e-04 - learning_rate: 0.0010\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 9.5663e-04 - val_loss: 8.9741e-04 - learning_rate: 0.0010\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 7.3947e-04 - val_loss: 8.0197e-04 - learning_rate: 0.0010\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.8391e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 7.9729e-04 - val_loss: 8.6009e-04 - learning_rate: 0.0010\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 8.3189e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 7.6606e-04 - val_loss: 8.4141e-04 - learning_rate: 0.0010\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 7.8527e-04 - val_loss: 8.0482e-04 - learning_rate: 0.0010\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 7.9349e-04 - val_loss: 7.3647e-04 - learning_rate: 0.0010\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.7965e-04 - val_loss: 7.9329e-04 - learning_rate: 0.0010\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 7.7304e-04 - val_loss: 8.5592e-04 - learning_rate: 0.0010\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 8.5108e-04 - val_loss: 7.2578e-04 - learning_rate: 0.0010\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.9418e-04 - val_loss: 7.8033e-04 - learning_rate: 0.0010\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 7.5892e-04 - val_loss: 9.6052e-04 - learning_rate: 0.0010\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 7.3936e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 7.6865e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 7.3627e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 7.7335e-04 - val_loss: 7.8715e-04 - learning_rate: 0.0010\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 6.9232e-04 - val_loss: 9.7798e-04 - learning_rate: 0.0010\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.5453e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 7.1622e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 7.5605e-04 - val_loss: 8.1900e-04 - learning_rate: 0.0010\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 7.0710e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 7.4665e-04 - val_loss: 8.8270e-04 - learning_rate: 0.0010\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 6.9461e-04 - val_loss: 9.5543e-04 - learning_rate: 0.0010\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 8.2684e-04 - val_loss: 8.6676e-04 - learning_rate: 0.0010\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.6805e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 7.0052e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.0771e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 7.2648e-04 - val_loss: 8.3963e-04 - learning_rate: 0.0010\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.7627e-04 - val_loss: 7.3533e-04 - learning_rate: 0.0010\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 7.1990e-04 - val_loss: 9.7393e-04 - learning_rate: 0.0010\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 7.0266e-04 - val_loss: 9.6539e-04 - learning_rate: 0.0010\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 6.9792e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 6.7007e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 7.0947e-04 - val_loss: 7.6992e-04 - learning_rate: 0.0010\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 6.5961e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 7.0210e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.9769e-04 - val_loss: 8.2037e-04 - learning_rate: 0.0010\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 5.9810e-04 - val_loss: 5.9384e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 5.9340e-04 - val_loss: 6.3048e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.9471e-04 - val_loss: 6.0390e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 6.0421e-04 - val_loss: 6.1515e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 6.0055e-04 - val_loss: 6.6183e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 6.0262e-04 - val_loss: 7.3817e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 5.9900e-04 - val_loss: 7.2230e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 6.0481e-04 - val_loss: 6.3428e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.9417e-04 - val_loss: 6.5115e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.9445e-04 - val_loss: 6.5439e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.9135e-04 - val_loss: 6.5722e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.8972e-04 - val_loss: 6.5955e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.8810e-04 - val_loss: 6.8193e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.8557e-04 - val_loss: 6.6526e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.8310e-04 - val_loss: 6.5272e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.8180e-04 - val_loss: 6.1561e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 5.8101e-04 - val_loss: 6.7270e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.8759e-04 - val_loss: 6.3477e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.7961e-04 - val_loss: 5.8940e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.7167e-04 - val_loss: 5.9892e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.7839e-04 - val_loss: 5.9022e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.7553e-04 - val_loss: 5.7483e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.7227e-04 - val_loss: 5.8220e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 5.7097e-04 - val_loss: 5.8573e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.6702e-04 - val_loss: 5.7714e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.6983e-04 - val_loss: 5.8734e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.6614e-04 - val_loss: 5.7982e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.6489e-04 - val_loss: 5.7558e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 5.6591e-04 - val_loss: 6.1262e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.6668e-04 - val_loss: 6.1938e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.6239e-04 - val_loss: 6.1732e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.6256e-04 - val_loss: 6.2310e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 5.6614e-04 - val_loss: 6.9588e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 5.5863e-04 - val_loss: 6.8722e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 5.5708e-04 - val_loss: 6.5156e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.5762e-04 - val_loss: 6.7351e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 5.5425e-04 - val_loss: 6.2631e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 5.5322e-04 - val_loss: 6.0427e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.5102e-04 - val_loss: 5.7197e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.5290e-04 - val_loss: 6.1860e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.4775e-04 - val_loss: 5.7658e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 5.4832e-04 - val_loss: 5.6545e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.4712e-04 - val_loss: 5.6338e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.4734e-04 - val_loss: 5.9385e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 5.4520e-04 - val_loss: 5.9716e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.4364e-04 - val_loss: 5.7939e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 5.4303e-04 - val_loss: 5.6977e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.4199e-04 - val_loss: 5.6939e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 151/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.4137e-04 - val_loss: 5.7297e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 152/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 5.4022e-04 - val_loss: 5.6495e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 153/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.3873e-04 - val_loss: 5.7619e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 154/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 5.0712e-04 - val_loss: 5.1371e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 155/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.0733e-04 - val_loss: 5.3772e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 156/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 5.0687e-04 - val_loss: 5.3042e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 157/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.0938e-04 - val_loss: 5.3584e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 158/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.0955e-04 - val_loss: 5.3620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 159/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.0865e-04 - val_loss: 5.3589e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 160/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 5.0757e-04 - val_loss: 5.3554e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 161/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 5.0650e-04 - val_loss: 5.3479e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 162/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 5.0546e-04 - val_loss: 5.3324e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 163/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 5.0457e-04 - val_loss: 5.3127e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 164/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 5.0365e-04 - val_loss: 5.2798e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 165/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 5.0276e-04 - val_loss: 5.2625e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 166/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 5.0179e-04 - val_loss: 5.2367e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 167/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 5.0090e-04 - val_loss: 5.2180e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 168/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 5.0005e-04 - val_loss: 5.2093e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 169/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.9915e-04 - val_loss: 5.1938e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 170/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.9834e-04 - val_loss: 5.1906e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 171/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.9740e-04 - val_loss: 5.1945e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 172/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.9637e-04 - val_loss: 5.2065e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 173/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 4.9542e-04 - val_loss: 5.2152e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 174/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.9456e-04 - val_loss: 5.2617e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 175/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 4.9374e-04 - val_loss: 5.2808e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 176/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.9290e-04 - val_loss: 5.2899e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 177/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.9215e-04 - val_loss: 5.2765e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 178/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.9129e-04 - val_loss: 5.2833e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 179/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 4.9060e-04 - val_loss: 5.2584e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 180/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.8995e-04 - val_loss: 5.2265e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 181/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.8923e-04 - val_loss: 5.2435e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 182/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.8842e-04 - val_loss: 5.2453e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 183/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.8785e-04 - val_loss: 5.2097e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 184/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.8710e-04 - val_loss: 5.1950e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 185/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.8652e-04 - val_loss: 5.1754e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 186/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.8575e-04 - val_loss: 5.1807e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 187/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.8523e-04 - val_loss: 5.1486e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 188/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.8456e-04 - val_loss: 5.1361e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 189/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.8403e-04 - val_loss: 5.1324e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 190/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.8326e-04 - val_loss: 5.1183e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 191/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.8265e-04 - val_loss: 5.0947e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 192/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.8197e-04 - val_loss: 5.0717e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 193/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.8130e-04 - val_loss: 5.0520e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 194/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.8082e-04 - val_loss: 5.0104e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 195/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.7999e-04 - val_loss: 5.0148e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 196/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 4.7943e-04 - val_loss: 4.9963e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 197/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.7874e-04 - val_loss: 5.0005e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 198/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.7805e-04 - val_loss: 5.0160e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 199/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.7744e-04 - val_loss: 5.0046e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 200/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.7692e-04 - val_loss: 4.9835e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 201/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 4.7629e-04 - val_loss: 4.9630e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 202/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.7563e-04 - val_loss: 4.9932e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 203/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.7487e-04 - val_loss: 5.0621e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 204/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 4.6542e-04 - val_loss: 4.7709e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 205/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.6543e-04 - val_loss: 4.7923e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 206/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.6500e-04 - val_loss: 4.7630e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 207/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.6554e-04 - val_loss: 4.7636e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 208/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.6532e-04 - val_loss: 4.7550e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 209/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.6490e-04 - val_loss: 4.7492e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 210/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.6447e-04 - val_loss: 4.7468e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 211/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.6400e-04 - val_loss: 4.7458e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 212/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.6354e-04 - val_loss: 4.7394e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 213/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.6310e-04 - val_loss: 4.7357e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 214/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.6264e-04 - val_loss: 4.7337e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 215/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.6218e-04 - val_loss: 4.7306e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 216/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.6176e-04 - val_loss: 4.7262e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 217/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.6134e-04 - val_loss: 4.7249e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 218/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.6093e-04 - val_loss: 4.7210e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 219/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.6051e-04 - val_loss: 4.7236e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 220/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.6009e-04 - val_loss: 4.7231e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 221/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.5967e-04 - val_loss: 4.7208e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 222/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.5926e-04 - val_loss: 4.7167e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 223/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.5884e-04 - val_loss: 4.7110e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 224/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.5845e-04 - val_loss: 4.7075e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 225/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.5804e-04 - val_loss: 4.7029e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 226/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.5763e-04 - val_loss: 4.6997e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 227/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 4.5721e-04 - val_loss: 4.6870e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 228/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.5681e-04 - val_loss: 4.6831e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 229/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.5641e-04 - val_loss: 4.6800e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 230/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 4.5601e-04 - val_loss: 4.6714e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 231/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.5560e-04 - val_loss: 4.6666e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 232/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.5522e-04 - val_loss: 4.6598e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 233/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.5483e-04 - val_loss: 4.6562e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 234/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.5445e-04 - val_loss: 4.6507e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 235/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.5407e-04 - val_loss: 4.6463e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 236/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.5368e-04 - val_loss: 4.6393e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 237/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.5330e-04 - val_loss: 4.6370e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 238/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.5293e-04 - val_loss: 4.6330e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 239/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.5255e-04 - val_loss: 4.6291e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 240/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.5218e-04 - val_loss: 4.6201e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 241/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.5182e-04 - val_loss: 4.6124e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 242/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.5144e-04 - val_loss: 4.6092e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 243/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.5109e-04 - val_loss: 4.6072e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 244/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.5071e-04 - val_loss: 4.6007e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 245/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.5036e-04 - val_loss: 4.5944e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 246/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.4998e-04 - val_loss: 4.5880e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 247/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.4963e-04 - val_loss: 4.5836e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 248/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.4931e-04 - val_loss: 4.5797e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 249/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.4895e-04 - val_loss: 4.5762e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 250/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.4862e-04 - val_loss: 4.5743e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 251/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.4828e-04 - val_loss: 4.5712e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 252/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.4797e-04 - val_loss: 4.5703e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 253/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 4.4765e-04 - val_loss: 4.5693e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 254/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.4731e-04 - val_loss: 4.5671e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 255/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.4453e-04 - val_loss: 4.4988e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 256/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.4428e-04 - val_loss: 4.4993e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 257/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.4416e-04 - val_loss: 4.5050e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 258/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.4405e-04 - val_loss: 4.5128e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 259/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.4388e-04 - val_loss: 4.5166e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 260/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.4367e-04 - val_loss: 4.5173e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 261/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.4344e-04 - val_loss: 4.5177e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 262/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.4322e-04 - val_loss: 4.5178e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 263/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.4299e-04 - val_loss: 4.5186e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 264/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.4276e-04 - val_loss: 4.5183e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 265/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.4253e-04 - val_loss: 4.5175e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 266/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.4230e-04 - val_loss: 4.5168e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 267/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.4207e-04 - val_loss: 4.5150e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 268/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.4184e-04 - val_loss: 4.5143e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 269/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.4162e-04 - val_loss: 4.5151e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 270/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.4139e-04 - val_loss: 4.5126e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 271/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.4116e-04 - val_loss: 4.5121e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 272/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.4094e-04 - val_loss: 4.5116e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 273/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.4071e-04 - val_loss: 4.5116e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 274/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.4049e-04 - val_loss: 4.5102e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 275/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.4026e-04 - val_loss: 4.5098e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 276/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.4005e-04 - val_loss: 4.5083e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 277/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3983e-04 - val_loss: 4.5059e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 278/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.3960e-04 - val_loss: 4.5046e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 279/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.3939e-04 - val_loss: 4.5030e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 280/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.3916e-04 - val_loss: 4.5014e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 281/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.3895e-04 - val_loss: 4.5009e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 282/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.3872e-04 - val_loss: 4.4999e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 283/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.3851e-04 - val_loss: 4.5014e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 284/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.3829e-04 - val_loss: 4.5003e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 285/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3808e-04 - val_loss: 4.5005e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 286/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.3786e-04 - val_loss: 4.4997e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 287/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 4.3764e-04 - val_loss: 4.4991e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 288/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.3742e-04 - val_loss: 4.4988e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 289/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.3720e-04 - val_loss: 4.4971e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 290/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.3698e-04 - val_loss: 4.4975e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 291/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.3676e-04 - val_loss: 4.4956e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 292/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.3655e-04 - val_loss: 4.4936e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 293/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3633e-04 - val_loss: 4.4927e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 294/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.3612e-04 - val_loss: 4.4914e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 295/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.3591e-04 - val_loss: 4.4896e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 296/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.3569e-04 - val_loss: 4.4879e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 297/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.3548e-04 - val_loss: 4.4870e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 298/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.3526e-04 - val_loss: 4.4838e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 299/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3504e-04 - val_loss: 4.4814e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 300/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.3483e-04 - val_loss: 4.4779e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 301/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.3461e-04 - val_loss: 4.4714e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 302/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.3439e-04 - val_loss: 4.4656e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 303/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.3417e-04 - val_loss: 4.4610e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 304/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.3395e-04 - val_loss: 4.4552e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 305/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.3256e-04 - val_loss: 4.4054e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 306/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.3253e-04 - val_loss: 4.3964e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 307/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.3249e-04 - val_loss: 4.3912e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 308/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.3241e-04 - val_loss: 4.3889e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 309/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.3230e-04 - val_loss: 4.3877e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 310/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.3218e-04 - val_loss: 4.3867e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 311/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.3206e-04 - val_loss: 4.3860e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 312/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.3193e-04 - val_loss: 4.3852e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 313/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.3180e-04 - val_loss: 4.3843e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 314/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.3167e-04 - val_loss: 4.3837e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 315/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.3155e-04 - val_loss: 4.3827e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 316/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.3142e-04 - val_loss: 4.3820e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 317/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.3129e-04 - val_loss: 4.3811e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 318/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.3117e-04 - val_loss: 4.3804e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 319/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.3104e-04 - val_loss: 4.3797e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 320/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.3091e-04 - val_loss: 4.3789e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 321/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.3079e-04 - val_loss: 4.3777e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 322/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.3066e-04 - val_loss: 4.3771e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 323/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.3054e-04 - val_loss: 4.3766e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 324/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.3041e-04 - val_loss: 4.3758e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 325/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.3029e-04 - val_loss: 4.3749e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 326/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.3016e-04 - val_loss: 4.3741e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 327/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 4.3003e-04 - val_loss: 4.3736e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 328/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.2991e-04 - val_loss: 4.3730e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 329/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.2978e-04 - val_loss: 4.3718e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 330/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2966e-04 - val_loss: 4.3708e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 331/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.2953e-04 - val_loss: 4.3700e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 332/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2941e-04 - val_loss: 4.3693e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 333/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2928e-04 - val_loss: 4.3686e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 334/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.2916e-04 - val_loss: 4.3676e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 335/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.2903e-04 - val_loss: 4.3666e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 336/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.2891e-04 - val_loss: 4.3654e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 337/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2878e-04 - val_loss: 4.3647e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 338/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 4.2866e-04 - val_loss: 4.3633e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 339/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.2853e-04 - val_loss: 4.3623e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 340/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.2840e-04 - val_loss: 4.3607e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 341/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.2827e-04 - val_loss: 4.3589e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 342/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2815e-04 - val_loss: 4.3577e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 343/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 4.2802e-04 - val_loss: 4.3564e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 344/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2789e-04 - val_loss: 4.3549e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 345/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.2777e-04 - val_loss: 4.3528e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 346/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.2764e-04 - val_loss: 4.3506e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 347/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.2752e-04 - val_loss: 4.3481e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 348/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2739e-04 - val_loss: 4.3458e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 349/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.2727e-04 - val_loss: 4.3433e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 350/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.2714e-04 - val_loss: 4.3410e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 351/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2702e-04 - val_loss: 4.3386e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 352/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.2690e-04 - val_loss: 4.3367e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 353/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.2677e-04 - val_loss: 4.3350e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 354/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.2665e-04 - val_loss: 4.3334e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 355/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.2580e-04 - val_loss: 4.3504e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 356/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2577e-04 - val_loss: 4.3457e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 357/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.2575e-04 - val_loss: 4.3426e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 358/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.2570e-04 - val_loss: 4.3408e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 359/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.2563e-04 - val_loss: 4.3395e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 360/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.2555e-04 - val_loss: 4.3378e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 361/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.2547e-04 - val_loss: 4.3365e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 362/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.2539e-04 - val_loss: 4.3354e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 363/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.2531e-04 - val_loss: 4.3341e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 364/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.2523e-04 - val_loss: 4.3329e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 365/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.2515e-04 - val_loss: 4.3312e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 366/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.2507e-04 - val_loss: 4.3298e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 367/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.2499e-04 - val_loss: 4.3286e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 368/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.2490e-04 - val_loss: 4.3277e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 369/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.2482e-04 - val_loss: 4.3266e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 370/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.2474e-04 - val_loss: 4.3253e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 371/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.2466e-04 - val_loss: 4.3241e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 372/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2458e-04 - val_loss: 4.3231e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 373/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.2450e-04 - val_loss: 4.3221e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 374/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.2442e-04 - val_loss: 4.3209e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 375/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 4.2434e-04 - val_loss: 4.3196e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 376/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.2426e-04 - val_loss: 4.3182e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 377/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.2418e-04 - val_loss: 4.3168e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 378/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 4.2410e-04 - val_loss: 4.3155e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 379/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.2402e-04 - val_loss: 4.3138e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 380/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 4.2394e-04 - val_loss: 4.3124e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 381/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.2385e-04 - val_loss: 4.3111e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 382/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.2377e-04 - val_loss: 4.3100e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 383/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.2369e-04 - val_loss: 4.3088e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 384/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2361e-04 - val_loss: 4.3077e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 385/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.2353e-04 - val_loss: 4.3066e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 386/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.2345e-04 - val_loss: 4.3055e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 387/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.2337e-04 - val_loss: 4.3043e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 388/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.2329e-04 - val_loss: 4.3031e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 389/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.2321e-04 - val_loss: 4.3022e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 390/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2314e-04 - val_loss: 4.3011e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 391/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2306e-04 - val_loss: 4.2999e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 392/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.2298e-04 - val_loss: 4.2990e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 393/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 4.2290e-04 - val_loss: 4.2980e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 394/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.2283e-04 - val_loss: 4.2968e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 395/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.2275e-04 - val_loss: 4.2958e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 396/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.2267e-04 - val_loss: 4.2948e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 397/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2260e-04 - val_loss: 4.2936e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 398/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2252e-04 - val_loss: 4.2924e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 399/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2244e-04 - val_loss: 4.2915e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 400/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.2237e-04 - val_loss: 4.2908e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 401/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.2229e-04 - val_loss: 4.2898e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 402/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.2221e-04 - val_loss: 4.2888e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 403/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 4.2214e-04 - val_loss: 4.2880e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 404/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2206e-04 - val_loss: 4.2872e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 405/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.2169e-04 - val_loss: 4.2809e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 406/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 4.2167e-04 - val_loss: 4.2807e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 407/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2164e-04 - val_loss: 4.2805e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 408/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.2160e-04 - val_loss: 4.2803e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 409/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.2156e-04 - val_loss: 4.2800e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 410/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.2152e-04 - val_loss: 4.2797e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 411/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.2147e-04 - val_loss: 4.2795e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 412/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.2143e-04 - val_loss: 4.2793e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 413/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.2138e-04 - val_loss: 4.2790e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 414/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.2134e-04 - val_loss: 4.2786e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 415/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 4.2129e-04 - val_loss: 4.2784e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 416/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.2125e-04 - val_loss: 4.2780e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 417/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.2120e-04 - val_loss: 4.2776e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 418/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.2116e-04 - val_loss: 4.2772e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 419/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.2111e-04 - val_loss: 4.2769e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 420/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.2107e-04 - val_loss: 4.2766e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 421/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.2103e-04 - val_loss: 4.2762e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 422/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2098e-04 - val_loss: 4.2758e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 423/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.2094e-04 - val_loss: 4.2754e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 424/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 4.2089e-04 - val_loss: 4.2750e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 425/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.2085e-04 - val_loss: 4.2746e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 426/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.2080e-04 - val_loss: 4.2742e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 427/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.2076e-04 - val_loss: 4.2738e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 428/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.2071e-04 - val_loss: 4.2734e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 429/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.2067e-04 - val_loss: 4.2729e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 430/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.2062e-04 - val_loss: 4.2724e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 431/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.2058e-04 - val_loss: 4.2719e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 432/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.2053e-04 - val_loss: 4.2716e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 433/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.2049e-04 - val_loss: 4.2712e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 434/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.2045e-04 - val_loss: 4.2707e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 435/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2040e-04 - val_loss: 4.2703e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 436/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2036e-04 - val_loss: 4.2698e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 437/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.2031e-04 - val_loss: 4.2693e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 438/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.2027e-04 - val_loss: 4.2689e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 439/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.2022e-04 - val_loss: 4.2685e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 440/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.2018e-04 - val_loss: 4.2680e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 441/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.2014e-04 - val_loss: 4.2676e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 442/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.2009e-04 - val_loss: 4.2672e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 443/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.2005e-04 - val_loss: 4.2667e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 444/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.2000e-04 - val_loss: 4.2664e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 445/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1996e-04 - val_loss: 4.2660e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 446/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1992e-04 - val_loss: 4.2656e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 447/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1987e-04 - val_loss: 4.2652e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 448/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1983e-04 - val_loss: 4.2648e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 449/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1978e-04 - val_loss: 4.2644e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 450/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1974e-04 - val_loss: 4.2639e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 451/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1970e-04 - val_loss: 4.2635e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 452/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1965e-04 - val_loss: 4.2630e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 453/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1961e-04 - val_loss: 4.2626e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 454/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1957e-04 - val_loss: 4.2621e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 455/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.1938e-04 - val_loss: 4.2531e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 456/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1936e-04 - val_loss: 4.2534e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 457/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1934e-04 - val_loss: 4.2534e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 458/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1932e-04 - val_loss: 4.2532e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 459/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1929e-04 - val_loss: 4.2530e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 460/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1927e-04 - val_loss: 4.2528e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 461/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1925e-04 - val_loss: 4.2526e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 462/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1922e-04 - val_loss: 4.2523e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 463/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1920e-04 - val_loss: 4.2521e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 464/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 4.1917e-04 - val_loss: 4.2519e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 465/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1915e-04 - val_loss: 4.2517e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 466/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1912e-04 - val_loss: 4.2515e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 467/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1910e-04 - val_loss: 4.2512e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 468/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1907e-04 - val_loss: 4.2509e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 469/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1905e-04 - val_loss: 4.2506e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 470/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 4.1902e-04 - val_loss: 4.2504e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 471/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1900e-04 - val_loss: 4.2501e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 472/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1897e-04 - val_loss: 4.2499e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 473/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1895e-04 - val_loss: 4.2497e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 474/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1892e-04 - val_loss: 4.2494e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 475/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 4.1890e-04 - val_loss: 4.2492e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 476/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1887e-04 - val_loss: 4.2489e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 477/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1885e-04 - val_loss: 4.2487e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 478/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1882e-04 - val_loss: 4.2484e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 479/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1880e-04 - val_loss: 4.2481e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 480/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1877e-04 - val_loss: 4.2479e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 481/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1875e-04 - val_loss: 4.2476e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 482/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.1872e-04 - val_loss: 4.2474e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 483/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1870e-04 - val_loss: 4.2471e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 484/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 4.1867e-04 - val_loss: 4.2468e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 485/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1865e-04 - val_loss: 4.2466e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 486/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1862e-04 - val_loss: 4.2463e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 487/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1860e-04 - val_loss: 4.2460e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 488/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1857e-04 - val_loss: 4.2458e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 489/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1855e-04 - val_loss: 4.2455e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 490/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1853e-04 - val_loss: 4.2453e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 491/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1850e-04 - val_loss: 4.2450e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 492/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1848e-04 - val_loss: 4.2448e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 493/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1845e-04 - val_loss: 4.2445e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 494/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1843e-04 - val_loss: 4.2443e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 495/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1840e-04 - val_loss: 4.2440e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 496/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 4.1838e-04 - val_loss: 4.2438e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 497/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1835e-04 - val_loss: 4.2436e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 498/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1833e-04 - val_loss: 4.2434e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 499/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1830e-04 - val_loss: 4.2432e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 500/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1828e-04 - val_loss: 4.2430e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 501/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1825e-04 - val_loss: 4.2427e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 502/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1823e-04 - val_loss: 4.2425e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 503/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1821e-04 - val_loss: 4.2422e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 504/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1818e-04 - val_loss: 4.2420e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 505/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1806e-04 - val_loss: 4.2359e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 506/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 4.1805e-04 - val_loss: 4.2357e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 507/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1804e-04 - val_loss: 4.2356e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 508/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1803e-04 - val_loss: 4.2355e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 509/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1801e-04 - val_loss: 4.2354e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 510/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.1800e-04 - val_loss: 4.2353e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 511/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 4.1799e-04 - val_loss: 4.2351e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 512/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1797e-04 - val_loss: 4.2350e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 513/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1796e-04 - val_loss: 4.2349e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 514/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1794e-04 - val_loss: 4.2347e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 515/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1793e-04 - val_loss: 4.2346e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 516/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1792e-04 - val_loss: 4.2345e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 517/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1790e-04 - val_loss: 4.2344e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 518/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1789e-04 - val_loss: 4.2342e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 519/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1787e-04 - val_loss: 4.2341e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 520/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1786e-04 - val_loss: 4.2340e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 521/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1785e-04 - val_loss: 4.2339e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 522/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1783e-04 - val_loss: 4.2337e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 523/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 4.1782e-04 - val_loss: 4.2336e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 524/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.1780e-04 - val_loss: 4.2335e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 525/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1779e-04 - val_loss: 4.2333e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 526/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1778e-04 - val_loss: 4.2332e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 527/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1776e-04 - val_loss: 4.2331e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 528/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1775e-04 - val_loss: 4.2330e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 529/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1774e-04 - val_loss: 4.2328e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 530/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1772e-04 - val_loss: 4.2327e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 531/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1771e-04 - val_loss: 4.2326e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 532/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1769e-04 - val_loss: 4.2324e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 533/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.1768e-04 - val_loss: 4.2323e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 534/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1767e-04 - val_loss: 4.2322e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 535/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1765e-04 - val_loss: 4.2321e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 536/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1764e-04 - val_loss: 4.2319e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 537/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1762e-04 - val_loss: 4.2318e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 538/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1761e-04 - val_loss: 4.2317e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 539/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1760e-04 - val_loss: 4.2316e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 540/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1758e-04 - val_loss: 4.2314e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 541/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1757e-04 - val_loss: 4.2313e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 542/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1755e-04 - val_loss: 4.2312e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 543/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1754e-04 - val_loss: 4.2310e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 544/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.1753e-04 - val_loss: 4.2309e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 545/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1751e-04 - val_loss: 4.2307e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 546/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1750e-04 - val_loss: 4.2306e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 547/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1749e-04 - val_loss: 4.2305e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 548/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1747e-04 - val_loss: 4.2303e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 549/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1746e-04 - val_loss: 4.2302e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 550/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1744e-04 - val_loss: 4.2301e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 551/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1743e-04 - val_loss: 4.2299e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 552/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1742e-04 - val_loss: 4.2298e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 553/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1740e-04 - val_loss: 4.2296e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 554/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1739e-04 - val_loss: 4.2295e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 555/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1733e-04 - val_loss: 4.2294e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 556/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.1732e-04 - val_loss: 4.2293e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 557/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1732e-04 - val_loss: 4.2292e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 558/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1731e-04 - val_loss: 4.2291e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 559/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1730e-04 - val_loss: 4.2290e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 560/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1729e-04 - val_loss: 4.2289e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 561/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1729e-04 - val_loss: 4.2289e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 562/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1728e-04 - val_loss: 4.2288e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 563/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.1727e-04 - val_loss: 4.2287e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 564/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1726e-04 - val_loss: 4.2286e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 565/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1726e-04 - val_loss: 4.2285e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 566/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1725e-04 - val_loss: 4.2285e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 567/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 4.1724e-04 - val_loss: 4.2284e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 568/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1723e-04 - val_loss: 4.2283e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 569/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.1723e-04 - val_loss: 4.2282e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 570/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1722e-04 - val_loss: 4.2281e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 571/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1721e-04 - val_loss: 4.2281e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 572/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1720e-04 - val_loss: 4.2280e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 573/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1720e-04 - val_loss: 4.2279e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 574/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1719e-04 - val_loss: 4.2278e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 575/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1718e-04 - val_loss: 4.2278e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 576/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1717e-04 - val_loss: 4.2277e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 577/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1717e-04 - val_loss: 4.2276e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 578/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1716e-04 - val_loss: 4.2275e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 579/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1715e-04 - val_loss: 4.2275e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 580/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1714e-04 - val_loss: 4.2274e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 581/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1714e-04 - val_loss: 4.2273e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 582/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1713e-04 - val_loss: 4.2272e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 583/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1712e-04 - val_loss: 4.2271e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 584/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1711e-04 - val_loss: 4.2271e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 585/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1711e-04 - val_loss: 4.2270e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 586/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1710e-04 - val_loss: 4.2269e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 587/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.1709e-04 - val_loss: 4.2268e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 588/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.1708e-04 - val_loss: 4.2267e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 589/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1708e-04 - val_loss: 4.2267e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 590/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1707e-04 - val_loss: 4.2266e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 591/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 4.1706e-04 - val_loss: 4.2265e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 592/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1705e-04 - val_loss: 4.2265e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 593/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1705e-04 - val_loss: 4.2264e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 594/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1704e-04 - val_loss: 4.2263e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 595/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1703e-04 - val_loss: 4.2262e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 596/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.1702e-04 - val_loss: 4.2261e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 597/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1702e-04 - val_loss: 4.2261e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 598/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1701e-04 - val_loss: 4.2260e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 599/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1700e-04 - val_loss: 4.2259e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 600/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1699e-04 - val_loss: 4.2258e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 601/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1699e-04 - val_loss: 4.2258e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 602/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1698e-04 - val_loss: 4.2257e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 603/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1697e-04 - val_loss: 4.2256e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 604/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1696e-04 - val_loss: 4.2255e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 605/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 4.1694e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 606/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1693e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 607/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1693e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 608/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1693e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 609/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.1692e-04 - val_loss: 4.2228e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 610/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.1692e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 611/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1691e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 612/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1691e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 613/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1691e-04 - val_loss: 4.2227e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 614/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.1690e-04 - val_loss: 4.2226e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 615/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1690e-04 - val_loss: 4.2226e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 616/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1689e-04 - val_loss: 4.2226e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 617/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1689e-04 - val_loss: 4.2225e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 618/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.1689e-04 - val_loss: 4.2225e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 619/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.1688e-04 - val_loss: 4.2225e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 620/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1688e-04 - val_loss: 4.2224e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 621/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1687e-04 - val_loss: 4.2224e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 622/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1687e-04 - val_loss: 4.2223e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 623/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1687e-04 - val_loss: 4.2223e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 624/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1686e-04 - val_loss: 4.2223e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 625/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1686e-04 - val_loss: 4.2222e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 626/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.1686e-04 - val_loss: 4.2222e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 627/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.1685e-04 - val_loss: 4.2222e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 628/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1685e-04 - val_loss: 4.2221e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 629/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1684e-04 - val_loss: 4.2221e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 630/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1684e-04 - val_loss: 4.2220e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 631/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 4.1684e-04 - val_loss: 4.2220e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 632/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1683e-04 - val_loss: 4.2219e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 633/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1683e-04 - val_loss: 4.2219e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 634/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 4.1682e-04 - val_loss: 4.2218e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 635/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 4.1682e-04 - val_loss: 4.2218e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 636/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 4.1682e-04 - val_loss: 4.2218e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 637/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1681e-04 - val_loss: 4.2217e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 638/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.1681e-04 - val_loss: 4.2217e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 639/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1680e-04 - val_loss: 4.2216e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 640/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1680e-04 - val_loss: 4.2216e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 641/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 4.1680e-04 - val_loss: 4.2215e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 642/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1679e-04 - val_loss: 4.2215e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 643/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 4.1679e-04 - val_loss: 4.2215e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 644/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1679e-04 - val_loss: 4.2214e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 645/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.1678e-04 - val_loss: 4.2214e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 646/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1678e-04 - val_loss: 4.2213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 647/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 4.1677e-04 - val_loss: 4.2213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 648/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1677e-04 - val_loss: 4.2213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 649/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1677e-04 - val_loss: 4.2212e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 650/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1676e-04 - val_loss: 4.2212e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 651/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1676e-04 - val_loss: 4.2212e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 652/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1675e-04 - val_loss: 4.2211e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 653/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1675e-04 - val_loss: 4.2211e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 654/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1675e-04 - val_loss: 4.2210e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 655/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 4.1673e-04 - val_loss: 4.2187e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 656/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 657/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 658/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 659/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 660/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1672e-04 - val_loss: 4.2186e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 661/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 662/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 663/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 664/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 665/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 4.1671e-04 - val_loss: 4.2185e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 666/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 667/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 668/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 669/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 670/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1670e-04 - val_loss: 4.2184e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 671/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 672/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 673/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 674/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 675/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1669e-04 - val_loss: 4.2183e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 676/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 677/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 678/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 679/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 680/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1668e-04 - val_loss: 4.2182e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 681/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1668e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 682/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 683/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 684/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 685/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1667e-04 - val_loss: 4.2181e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 686/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 687/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 688/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 689/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 690/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1666e-04 - val_loss: 4.2180e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 691/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1666e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 692/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 4.1665e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 693/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1665e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 694/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1665e-04 - val_loss: 4.2179e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 695/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 4.1665e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 696/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1665e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 697/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1664e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 698/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1664e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 699/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1664e-04 - val_loss: 4.2178e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 700/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1664e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 701/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1664e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 702/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1663e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 703/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1663e-04 - val_loss: 4.2177e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 704/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1663e-04 - val_loss: 4.2176e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 705/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 706/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 707/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 708/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 709/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 710/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 711/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1662e-04 - val_loss: 4.2177e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 712/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1662e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 713/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1662e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 714/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1662e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 715/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 716/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 717/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 718/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 719/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 720/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 721/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 722/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 723/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 724/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 725/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.1661e-04 - val_loss: 4.2176e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 726/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 727/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 728/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 729/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 730/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1661e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 731/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 732/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 733/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 734/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 735/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 736/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 737/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 738/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 739/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 740/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1660e-04 - val_loss: 4.2175e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 741/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 742/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 743/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 744/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 745/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 746/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 747/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.1660e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 748/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 749/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 750/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 751/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 752/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 753/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 754/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.2207e-07\n",
      "Epoch 755/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 756/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 757/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 758/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 759/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 760/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 761/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 762/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 763/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 764/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 765/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 766/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 767/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 4.1659e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 768/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 769/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 770/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 771/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 772/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 773/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1658e-04 - val_loss: 4.2174e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 774/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 775/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 776/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 777/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 778/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 779/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 780/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 781/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 782/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 783/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 784/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 785/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 786/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 787/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 788/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 789/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 790/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 791/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 792/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 793/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 794/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 795/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1658e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 796/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1657e-04 - val_loss: 4.2173e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 797/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 798/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 799/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 800/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 801/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 802/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 803/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 804/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 805/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 806/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 807/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 808/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 809/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 810/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 811/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 812/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 813/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 814/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 815/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 816/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 817/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 818/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 819/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 820/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 821/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 4.1657e-04 - val_loss: 4.2172e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 822/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 4.1657e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 823/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 4.1657e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 824/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 4.1657e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 825/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 826/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 827/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 828/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 829/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 830/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 831/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 832/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 833/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 834/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 835/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 836/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 837/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 838/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 839/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 840/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 841/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 842/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 843/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 4.1656e-04 - val_loss: 4.2171e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 844/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 845/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 846/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 847/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 848/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 849/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 850/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 851/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 852/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "Epoch 853/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1656e-04 - val_loss: 4.2170e-04 - learning_rate: 1.0000e-07\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step\n",
      "Model 1 Test Set RMSE: 0.002859 AU\n",
      "\n",
      "--- Training MLP Model 2 ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 0.0592 - val_loss: 0.0410 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0218 - val_loss: 0.0235 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0138 - val_loss: 0.0158 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0092 - val_loss: 0.0117 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0063 - val_loss: 0.0095 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0046 - val_loss: 0.0091 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0039 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.0031 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0027 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0024 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0020 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0020 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0020 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0017 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.0016 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0016 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.0016 - val_loss: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0014 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.0012 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.0013 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 0.0013 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0013 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0011 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0012 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0011 - val_loss: 9.6725e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0010 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 9.9506e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0011 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.0010 - val_loss: 9.6028e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0010 - val_loss: 9.2748e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 9.5384e-04 - val_loss: 9.0202e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0010 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 9.4500e-04 - val_loss: 9.2950e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0010 - val_loss: 9.1055e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 9.3765e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.0010 - val_loss: 9.0060e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 9.8892e-04 - val_loss: 8.8312e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 8.8856e-04 - val_loss: 9.2904e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0010 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 7.9893e-04 - val_loss: 8.0624e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.9332e-04 - val_loss: 7.9899e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 7.9372e-04 - val_loss: 8.5157e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 8.0808e-04 - val_loss: 8.9539e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 7.9338e-04 - val_loss: 8.4283e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 8.0979e-04 - val_loss: 7.9306e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.9577e-04 - val_loss: 8.8208e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 7.7623e-04 - val_loss: 7.9572e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 8.0173e-04 - val_loss: 8.7250e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 7.8750e-04 - val_loss: 8.2414e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 7.6900e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 7.7222e-04 - val_loss: 9.8606e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 7.6998e-04 - val_loss: 8.3467e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.6248e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.6343e-04 - val_loss: 7.7579e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.5951e-04 - val_loss: 7.9209e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 7.5479e-04 - val_loss: 7.8208e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 7.5080e-04 - val_loss: 8.1270e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.4514e-04 - val_loss: 8.5590e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.4205e-04 - val_loss: 8.6702e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 7.3874e-04 - val_loss: 8.5155e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 7.3490e-04 - val_loss: 8.6692e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 7.3119e-04 - val_loss: 8.8783e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.2800e-04 - val_loss: 8.6425e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 7.2456e-04 - val_loss: 8.4528e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.2092e-04 - val_loss: 8.6000e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.1825e-04 - val_loss: 8.5758e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 7.1398e-04 - val_loss: 8.8894e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 7.1119e-04 - val_loss: 8.6452e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.0844e-04 - val_loss: 8.0501e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 7.0560e-04 - val_loss: 7.8399e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.0134e-04 - val_loss: 8.1077e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.9395e-04 - val_loss: 8.0754e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 7.0757e-04 - val_loss: 7.0068e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 6.9248e-04 - val_loss: 7.0828e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 6.9064e-04 - val_loss: 7.0356e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.8868e-04 - val_loss: 6.9449e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.8698e-04 - val_loss: 7.0830e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 6.8285e-04 - val_loss: 7.0560e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.7996e-04 - val_loss: 7.0333e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.7860e-04 - val_loss: 7.2070e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 6.7319e-04 - val_loss: 7.0366e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 6.7646e-04 - val_loss: 6.8608e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.7062e-04 - val_loss: 7.1688e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.6318e-04 - val_loss: 6.6294e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.6224e-04 - val_loss: 7.0750e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.7923e-04 - val_loss: 6.5467e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 6.5443e-04 - val_loss: 6.7436e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 6.6949e-04 - val_loss: 6.5065e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.5350e-04 - val_loss: 6.5782e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.1554e-04 - val_loss: 6.1467e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 6.1204e-04 - val_loss: 6.1795e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 6.1296e-04 - val_loss: 6.1821e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 6.1554e-04 - val_loss: 6.2273e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.1546e-04 - val_loss: 6.2132e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 6.1171e-04 - val_loss: 6.2031e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.1323e-04 - val_loss: 6.1190e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.0789e-04 - val_loss: 6.1585e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.0939e-04 - val_loss: 6.0441e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 6.0509e-04 - val_loss: 6.0680e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.0464e-04 - val_loss: 6.0078e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 6.0252e-04 - val_loss: 6.0048e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.0071e-04 - val_loss: 5.9671e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.9839e-04 - val_loss: 5.9910e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.9849e-04 - val_loss: 5.9476e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 5.9481e-04 - val_loss: 5.9659e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.9572e-04 - val_loss: 5.9089e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.9199e-04 - val_loss: 5.9297e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.9218e-04 - val_loss: 5.8849e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.8888e-04 - val_loss: 5.9075e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 5.8872e-04 - val_loss: 5.8620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.8517e-04 - val_loss: 5.8642e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.8703e-04 - val_loss: 5.8176e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.8248e-04 - val_loss: 5.8265e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.8230e-04 - val_loss: 5.8057e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.8159e-04 - val_loss: 5.7919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 5.7897e-04 - val_loss: 5.7564e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.7540e-04 - val_loss: 5.7697e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.7732e-04 - val_loss: 5.7917e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.7264e-04 - val_loss: 5.7707e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 5.7330e-04 - val_loss: 5.7349e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.6862e-04 - val_loss: 5.7271e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.6998e-04 - val_loss: 5.7002e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 5.6759e-04 - val_loss: 5.7781e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 5.6640e-04 - val_loss: 5.8024e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 5.6468e-04 - val_loss: 5.8119e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.6361e-04 - val_loss: 5.8007e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.6225e-04 - val_loss: 5.7647e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.6022e-04 - val_loss: 5.6803e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 5.6057e-04 - val_loss: 5.7004e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.5843e-04 - val_loss: 5.6842e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 5.5602e-04 - val_loss: 5.6115e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.5541e-04 - val_loss: 5.5972e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 5.5409e-04 - val_loss: 5.5876e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.5275e-04 - val_loss: 5.5905e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 5.5127e-04 - val_loss: 5.5794e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 5.5036e-04 - val_loss: 5.5697e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 5.4948e-04 - val_loss: 5.5605e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 5.4891e-04 - val_loss: 5.5406e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.4792e-04 - val_loss: 5.5164e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430us/step\n",
      "Model 2 Test Set RMSE: 0.115300 AU\n",
      "\n",
      "--- Training MLP Model 3 ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step - loss: 0.0525 - val_loss: 0.0503 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0224 - val_loss: 0.0318 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0141 - val_loss: 0.0203 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0091 - val_loss: 0.0187 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0063 - val_loss: 0.0191 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.0047 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0036 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.0028 - val_loss: 0.0087 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0024 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0022 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0019 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0022 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.0016 - val_loss: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0016 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.0015 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.0014 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0017 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0012 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.0016 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0011 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.0011 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0011 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.0011 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0011 - val_loss: 9.6447e-04 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 0.0010 - val_loss: 9.3234e-04 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 9.9642e-04 - val_loss: 9.5364e-04 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.0010 - val_loss: 9.4387e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 9.1934e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 9.8708e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 0.0010 - val_loss: 9.6737e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 9.5339e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 9.5424e-04 - val_loss: 8.4752e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 9.6358e-04 - val_loss: 8.8428e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 9.5975e-04 - val_loss: 8.6761e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 9.4230e-04 - val_loss: 9.1710e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 9.2020e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 9.3411e-04 - val_loss: 8.9830e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 9.2636e-04 - val_loss: 8.8293e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 9.1449e-04 - val_loss: 9.1616e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 8.4992e-04 - val_loss: 9.2206e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 9.5978e-04 - val_loss: 9.6026e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 8.8596e-04 - val_loss: 9.8447e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 8.8123e-04 - val_loss: 7.9363e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 7.5280e-04 - val_loss: 7.5902e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 7.5166e-04 - val_loss: 7.5731e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 7.5998e-04 - val_loss: 7.6959e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 7.6668e-04 - val_loss: 7.6116e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.7181e-04 - val_loss: 8.2877e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.4822e-04 - val_loss: 7.3660e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 7.5455e-04 - val_loss: 7.3206e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 7.4333e-04 - val_loss: 7.2230e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 7.4921e-04 - val_loss: 7.2086e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 7.3751e-04 - val_loss: 7.2580e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 7.3144e-04 - val_loss: 7.1344e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 7.3486e-04 - val_loss: 7.2369e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 7.1480e-04 - val_loss: 7.2408e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 7.2922e-04 - val_loss: 7.1549e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 7.1399e-04 - val_loss: 7.0961e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.0400e-04 - val_loss: 7.2067e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 7.0797e-04 - val_loss: 9.1501e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 7.0423e-04 - val_loss: 8.6561e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 7.0041e-04 - val_loss: 6.9555e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 6.9259e-04 - val_loss: 7.7157e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.9279e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 6.9155e-04 - val_loss: 6.7980e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 6.8411e-04 - val_loss: 7.7542e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 6.8061e-04 - val_loss: 8.6355e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.8657e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 6.8163e-04 - val_loss: 7.0531e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 6.6760e-04 - val_loss: 7.5591e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 6.7592e-04 - val_loss: 9.9951e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 6.7330e-04 - val_loss: 6.8514e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 6.6330e-04 - val_loss: 7.0573e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 6.5667e-04 - val_loss: 7.3311e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 6.6101e-04 - val_loss: 7.4637e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.5450e-04 - val_loss: 7.1730e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 6.5056e-04 - val_loss: 7.2811e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 6.4935e-04 - val_loss: 7.1370e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 6.4875e-04 - val_loss: 6.9825e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 6.5010e-04 - val_loss: 6.7995e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 6.3979e-04 - val_loss: 8.0499e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 6.4367e-04 - val_loss: 6.9911e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.4409e-04 - val_loss: 6.4734e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 6.4001e-04 - val_loss: 6.4637e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 6.2439e-04 - val_loss: 6.9735e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 6.3919e-04 - val_loss: 6.5078e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 6.2344e-04 - val_loss: 6.9424e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 6.2757e-04 - val_loss: 6.8582e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 6.2290e-04 - val_loss: 7.0039e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 6.1995e-04 - val_loss: 6.6088e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 6.2128e-04 - val_loss: 7.6363e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.1675e-04 - val_loss: 6.7022e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 6.1668e-04 - val_loss: 7.7172e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.7606e-04 - val_loss: 6.2191e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.7274e-04 - val_loss: 6.1104e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 5.7351e-04 - val_loss: 5.9417e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.7554e-04 - val_loss: 6.0389e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.7484e-04 - val_loss: 6.1142e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 5.7499e-04 - val_loss: 6.0856e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.7097e-04 - val_loss: 6.1209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.7246e-04 - val_loss: 5.9409e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.6645e-04 - val_loss: 6.0480e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.6977e-04 - val_loss: 5.8881e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.6357e-04 - val_loss: 6.0170e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 5.6516e-04 - val_loss: 5.9054e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.6234e-04 - val_loss: 6.0092e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.6136e-04 - val_loss: 6.0102e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.5946e-04 - val_loss: 6.0436e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.5842e-04 - val_loss: 6.0098e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.5688e-04 - val_loss: 6.0215e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.5588e-04 - val_loss: 5.9515e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.5448e-04 - val_loss: 5.9706e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.5306e-04 - val_loss: 5.9492e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 5.5171e-04 - val_loss: 5.9841e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.5085e-04 - val_loss: 5.9402e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 5.4940e-04 - val_loss: 5.9925e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 5.4829e-04 - val_loss: 5.9478e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 5.4707e-04 - val_loss: 5.9529e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.4612e-04 - val_loss: 5.9044e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 5.4435e-04 - val_loss: 5.9431e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 5.4410e-04 - val_loss: 5.8876e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.4259e-04 - val_loss: 5.9079e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 5.4162e-04 - val_loss: 5.8632e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.4022e-04 - val_loss: 5.8595e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 5.3923e-04 - val_loss: 5.7632e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 5.3858e-04 - val_loss: 5.7505e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.3733e-04 - val_loss: 5.7088e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 5.3648e-04 - val_loss: 5.6809e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 5.3523e-04 - val_loss: 5.6326e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 5.3444e-04 - val_loss: 5.6183e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 5.3315e-04 - val_loss: 5.5566e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 5.3209e-04 - val_loss: 5.5313e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 5.3145e-04 - val_loss: 5.4845e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 5.3033e-04 - val_loss: 5.4687e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 5.2981e-04 - val_loss: 5.4457e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 5.2861e-04 - val_loss: 5.4807e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 5.2773e-04 - val_loss: 5.4749e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.2693e-04 - val_loss: 5.4706e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 5.2615e-04 - val_loss: 5.4299e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 5.2506e-04 - val_loss: 5.4115e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 5.2469e-04 - val_loss: 5.3795e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.2347e-04 - val_loss: 5.3586e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 5.2291e-04 - val_loss: 5.3325e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438us/step\n",
      "Model 3 Test Set RMSE: 0.137518 AU\n",
      "\n",
      "--- Final Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.091990 AU\n",
      "Ensemble Y-coordinate RMSE: 0.079537 AU\n",
      "Ensemble Z-coordinate RMSE: 0.040822 AU\n",
      "Overall Ensemble RMSE: 0.070783 AU\n",
      "\n",
      "--- Final Comparison ---\n",
      "Best Single MLP Overall RMSE: 0.003100 AU\n",
      "Multi-MLP Ensemble Overall RMSE: 0.070783 AU\n",
      "\n",
      "RESULT: The single MLP model remains the most accurate.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# --- MODEL CONFIGURATION ---\n",
    "NUM_MODELS = 3 # Number of MLP models in the ensemble\n",
    "# Set a high number of epochs since Early Stopping will handle when to stop\n",
    "EPOCHS = 5000   \n",
    "INPUT_SHAPE = X_train_scaled.shape[1] # Number of input features\n",
    "models = []\n",
    "y_pred_list = []\n",
    "\n",
    "# --- Define Callbacks for Training ---\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', # Monitors validation loss\n",
    "    patience=150,        # Stops training after 150 epochs of no improvement\n",
    "    restore_best_weights=True,\n",
    "    min_delta = 1e-7 \n",
    "    )\n",
    "\n",
    "lr_on_plateau_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss', # Monitors validation loss\n",
    "    factor=0.5,         # Reduces learning rate by 50%\n",
    "    patience=50,        # If no improvement for 50 epochs, reduce LR\n",
    "    min_lr=1e-7         # The minimum learning rate to allow\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, lr_on_plateau_callback]\n",
    "\n",
    "# --- 1. BUILD AND TRAIN THE INDIVIDUAL MLP MODELS ---\n",
    "print(f\"Building and training {NUM_MODELS} diverse MLP models...\")\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    # Use a different random seed for each model to ensure diverse weight initializations\n",
    "    tf.keras.utils.set_random_seed(i + 1)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.0001), \n",
    "              input_shape=(INPUT_SHAPE,)), \n",
    "        \n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.0001)), \n",
    "        \n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "        Dense(3, activation='linear') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"\\n--- Training MLP Model {i+1} ---\")\n",
    "    model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train.values, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(X_test_scaled, y_test.values),\n",
    "        callbacks=callbacks, # Pass the list of callbacks here\n",
    "        verbose=1 # Changed to 1 to show the callback messages\n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    # Generate predictions for the current model and store them\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    # Calculate and print the individual model's RMSE for comparison\n",
    "    rmse_x = np.sqrt(mean_squared_error(y_test.values[:, 0], y_pred[:, 0]))\n",
    "    rmse_y = np.sqrt(mean_squared_error(y_test.values[:, 1], y_pred[:, 1]))\n",
    "    rmse_z = np.sqrt(mean_squared_error(y_test.values[:, 2], y_pred[:, 2]))\n",
    "    overall_rmse = np.mean([rmse_x, rmse_y, rmse_z])\n",
    "    \n",
    "    print(f\"Model {i+1} Test Set RMSE: {overall_rmse:.6f} AU\")\n",
    "\n",
    "# --- 2. CREATE THE ENSEMBLE PREDICTION ---\n",
    "# The ensemble prediction is the simple average of all individual model predictions.\n",
    "y_ensemble_pred_au = np.mean(y_pred_list, axis=0)\n",
    "\n",
    "# --- 3. EVALUATE THE ENSEMBLE PERFORMANCE ---\n",
    "print(\"\\n--- Final Ensemble Evaluation ---\")\n",
    "rmse_x_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 0], y_ensemble_pred_au[:, 0]))\n",
    "rmse_y_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 1], y_ensemble_pred_au[:, 1]))\n",
    "rmse_z_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 2], y_ensemble_pred_au[:, 2]))\n",
    "overall_ensemble_rmse = np.mean([rmse_x_ensemble, rmse_y_ensemble, rmse_z_ensemble])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {rmse_z_ensemble:.6f} AU\")\n",
    "print(f\"Overall Ensemble RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# --- 4. Compare with your best single MLP result ---\n",
    "# Replace the value below with your actual best MLP RMSE (approx. 0.0031)\n",
    "BEST_SINGLE_MLP_RMSE = 0.003100\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "print(f\"Best Single MLP Overall RMSE: {BEST_SINGLE_MLP_RMSE:.6f} AU\")\n",
    "print(f\"Multi-MLP Ensemble Overall RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# Determine if the ensemble is better\n",
    "if overall_ensemble_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(\"\\nSUCCESS: The multi-model MLP ensemble is more accurate!\")\n",
    "else:\n",
    "    print(\"\\nRESULT: The single MLP model remains the most accurate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to models/mars_position_predictor_mm1.keras\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'models'\n",
    "MODEL_FILENAME = 'mars_position_predictor_mm1.keras' \n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# os.makedirs(MODEL_DIR, exist_ok=True) \n",
    "\n",
    "# Save the model\n",
    "models[0].save(os.path.join(MODEL_DIR, MODEL_FILENAME))\n",
    "\n",
    "print(f\"Model saved successfully to {os.path.join(MODEL_DIR, MODEL_FILENAME)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MM1 Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.003033 AU\n",
      "Ensemble Y-coordinate RMSE: 0.003884 AU\n",
      "Ensemble Z-coordinate RMSE: 0.001660 AU\n",
      "Overall Ensemble RMSE: 0.002859 AU\n"
     ]
    }
   ],
   "source": [
    "y_pred_mm1 = y_pred_list[0]\n",
    "print(\"\\n--- MM1 Ensemble Evaluation ---\")\n",
    "rmse_x_mm1 = np.sqrt(mean_squared_error(y_test.values[:, 0], y_pred_mm1[:, 0]))\n",
    "rmse_y_mm1 = np.sqrt(mean_squared_error(y_test.values[:, 1], y_pred_mm1[:, 1]))\n",
    "rmse_z_mm1 = np.sqrt(mean_squared_error(y_test.values[:, 2], y_pred_mm1[:, 2]))\n",
    "overall_mm1_rmse = np.mean([rmse_x_mm1, rmse_y_mm1, rmse_z_mm1])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_mm1:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_mm1:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {rmse_z_mm1:.6f} AU\")\n",
    "print(f\"Overall Ensemble RMSE: {overall_mm1_rmse:.6f} AU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.002818 AU\n",
      "Ensemble Y-coordinate RMSE: 0.003294 AU\n",
      "Ensemble Z-coordinate RMSE: 0.001294 AU\n",
      "Overall Ensemble RMSE: 0.002468 AU\n",
      "\n",
      "--- Final Comparison ---\n",
      "Best Single MLP Overall RMSE: 0.003100 AU\n",
      "Multi-MLP Ensemble Overall RMSE: 0.002468 AU\n",
      "\n",
      "SUCCESS: The multi-model MLP ensemble is more accurate!\n"
     ]
    }
   ],
   "source": [
    "y_pred_mm1 = y_pred_list[0]\n",
    "y_pred_list_ens = [y_pred_mlp_au, y_pred_mm1]\n",
    "y_ensemble_pred_au = np.mean(y_pred_list_ens, axis=0)\n",
    "\n",
    "# --- 3. EVALUATE THE ENSEMBLE PERFORMANCE ---\n",
    "print(\"\\n--- Final Ensemble Evaluation ---\")\n",
    "rmse_x_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 0], y_ensemble_pred_au[:, 0]))\n",
    "rmse_y_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 1], y_ensemble_pred_au[:, 1]))\n",
    "rmse_z_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 2], y_ensemble_pred_au[:, 2]))\n",
    "overall_ensemble_rmse = np.mean([rmse_x_ensemble, rmse_y_ensemble, rmse_z_ensemble])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {rmse_z_ensemble:.6f} AU\")\n",
    "print(f\"Overall Ensemble RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# --- 4. Compare with your best single MLP result ---\n",
    "# Replace the value below with your actual best MLP RMSE (approx. 0.0031)\n",
    "BEST_SINGLE_MLP_RMSE = 0.003100\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "print(f\"Best Single MLP Overall RMSE: {BEST_SINGLE_MLP_RMSE:.6f} AU\")\n",
    "print(f\"Multi-MLP Ensemble Overall RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# Determine if the ensemble is better\n",
    "if overall_ensemble_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(\"\\nSUCCESS: The multi-model MLP ensemble is more accurate!\")\n",
    "else:\n",
    "    print(\"\\nRESULT: The single MLP model remains the most accurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training 9 diverse MLP models...\n",
      "\n",
      "--- Training MLP Model 1 (Seed: 1) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 0.0412 - val_loss: 0.0485 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0168 - val_loss: 0.0272 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.0113 - val_loss: 0.0230 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.0078 - val_loss: 0.0124 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.0054 - val_loss: 0.0110 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.0042 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.0032 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.0024 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.0020 - val_loss: 0.0036 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.0021 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 0.0016 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.0014 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0014 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0012 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.0010 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0011 - val_loss: 0.0080 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0012 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 9.2339e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 9.7430e-04 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 8.9090e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 7.4875e-04 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 7.7930e-04 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 8.0347e-04 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 6.8546e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 7.4981e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.4018e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 7.1359e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 5.7125e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 6.6180e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 5.8931e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 6.5278e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 6.0160e-04 - val_loss: 8.2045e-04 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 5.6915e-04 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 6.3849e-04 - val_loss: 7.8076e-04 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.3437e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.8714e-04 - val_loss: 8.2216e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 6.4374e-04 - val_loss: 7.4014e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.0152e-04 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 4.8371e-04 - val_loss: 8.7775e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.7135e-04 - val_loss: 8.5873e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 5.5977e-04 - val_loss: 8.0873e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 4.6580e-04 - val_loss: 8.3728e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 5.4529e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.8237e-04 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 5.3524e-04 - val_loss: 9.4104e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 5.0240e-04 - val_loss: 8.7620e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 4.7819e-04 - val_loss: 5.2658e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 3.7657e-04 - val_loss: 8.8834e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.8630e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.6223e-04 - val_loss: 4.0583e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.4082e-04 - val_loss: 6.0891e-04 - learning_rate: 0.0010\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 4.1890e-04 - val_loss: 4.7670e-04 - learning_rate: 0.0010\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.3647e-04 - val_loss: 8.5538e-04 - learning_rate: 0.0010\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.6811e-04 - val_loss: 8.8056e-04 - learning_rate: 0.0010\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.5300e-04 - val_loss: 5.0194e-04 - learning_rate: 0.0010\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 3.8392e-04 - val_loss: 3.5732e-04 - learning_rate: 0.0010\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.4916e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.2742e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.3806e-04 - val_loss: 5.1535e-04 - learning_rate: 0.0010\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 4.1778e-04 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.9418e-04 - val_loss: 6.4371e-04 - learning_rate: 0.0010\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 3.5213e-04 - val_loss: 8.2233e-04 - learning_rate: 0.0010\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.5159e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 4.0488e-04 - val_loss: 8.7587e-04 - learning_rate: 0.0010\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.1632e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.7497e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 4.3233e-04 - val_loss: 4.1924e-04 - learning_rate: 0.0010\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 3.4877e-04 - val_loss: 9.3383e-04 - learning_rate: 0.0010\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.8499e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.6683e-04 - val_loss: 4.3187e-04 - learning_rate: 0.0010\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.6133e-04 - val_loss: 6.0007e-04 - learning_rate: 0.0010\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 3.9743e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.3471e-04 - val_loss: 7.5996e-04 - learning_rate: 0.0010\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 3.5581e-04 - val_loss: 4.4204e-04 - learning_rate: 0.0010\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.7829e-04 - val_loss: 5.6632e-04 - learning_rate: 0.0010\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 4.4011e-04 - val_loss: 4.6414e-04 - learning_rate: 0.0010\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.4552e-04 - val_loss: 8.7712e-04 - learning_rate: 0.0010\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.6937e-04 - val_loss: 4.2544e-04 - learning_rate: 0.0010\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.6037e-04 - val_loss: 4.2696e-04 - learning_rate: 0.0010\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 3.9003e-04 - val_loss: 7.4650e-04 - learning_rate: 0.0010\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 3.4991e-04 - val_loss: 4.2537e-04 - learning_rate: 0.0010\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 4.0387e-04 - val_loss: 9.1204e-04 - learning_rate: 0.0010\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.6696e-04 - val_loss: 0.0173 - learning_rate: 0.0010\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 5.9436e-04 - val_loss: 9.4739e-04 - learning_rate: 0.0010\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.9925e-04 - val_loss: 5.7407e-04 - learning_rate: 0.0010\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.0081e-04 - val_loss: 4.2496e-04 - learning_rate: 0.0010\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 3.4546e-04 - val_loss: 5.9697e-04 - learning_rate: 0.0010\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 3.9975e-04 - val_loss: 4.1951e-04 - learning_rate: 0.0010\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.8781e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.2560e-04 - val_loss: 3.4350e-04 - learning_rate: 0.0010\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.4985e-04 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 3.9316e-04 - val_loss: 6.4937e-04 - learning_rate: 0.0010\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.1279e-04 - val_loss: 7.0129e-04 - learning_rate: 0.0010\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 3.6788e-04 - val_loss: 5.5537e-04 - learning_rate: 0.0010\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 4.0899e-04 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 3.2640e-04 - val_loss: 6.0381e-04 - learning_rate: 0.0010\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.6288e-04 - val_loss: 5.4773e-04 - learning_rate: 0.0010\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 3.9127e-04 - val_loss: 6.6885e-04 - learning_rate: 0.0010\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.3393e-04 - val_loss: 6.7518e-04 - learning_rate: 0.0010\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 3.6203e-04 - val_loss: 4.9239e-04 - learning_rate: 0.0010\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.6174e-04 - val_loss: 3.0089e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.3407e-04 - val_loss: 2.8897e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 2.5012e-04 - val_loss: 3.2875e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.3969e-04 - val_loss: 3.0975e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.7128e-04 - val_loss: 4.1991e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.4148e-04 - val_loss: 3.0065e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.4939e-04 - val_loss: 3.7702e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.4074e-04 - val_loss: 3.9617e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.5486e-04 - val_loss: 5.0016e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.4177e-04 - val_loss: 3.2871e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.4067e-04 - val_loss: 3.8528e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.4927e-04 - val_loss: 4.2579e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.5238e-04 - val_loss: 4.3455e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 2.3964e-04 - val_loss: 2.8371e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.3777e-04 - val_loss: 2.9818e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.3742e-04 - val_loss: 3.6397e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.4266e-04 - val_loss: 6.2386e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.4029e-04 - val_loss: 5.0446e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.3868e-04 - val_loss: 3.2005e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 2.3734e-04 - val_loss: 2.5031e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.2852e-04 - val_loss: 3.3586e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.2117e-04 - val_loss: 2.8414e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.4056e-04 - val_loss: 2.5518e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 2.2457e-04 - val_loss: 3.4656e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 2.2538e-04 - val_loss: 2.4502e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.2488e-04 - val_loss: 4.4008e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.2548e-04 - val_loss: 3.3757e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.3694e-04 - val_loss: 4.0895e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.3389e-04 - val_loss: 3.0454e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 2.2267e-04 - val_loss: 2.7196e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.2135e-04 - val_loss: 3.9565e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 2.2100e-04 - val_loss: 3.2449e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.2653e-04 - val_loss: 3.7990e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.3286e-04 - val_loss: 4.0420e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 2.1777e-04 - val_loss: 5.7359e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 2.2522e-04 - val_loss: 2.3389e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.2279e-04 - val_loss: 6.0929e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.2809e-04 - val_loss: 3.0919e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 2.2608e-04 - val_loss: 5.4112e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 2.1736e-04 - val_loss: 3.0236e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0597e-04 - val_loss: 3.5911e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.2281e-04 - val_loss: 2.5039e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.1165e-04 - val_loss: 2.2252e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.2501e-04 - val_loss: 2.8882e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.1319e-04 - val_loss: 2.6547e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.1329e-04 - val_loss: 2.7462e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.2159e-04 - val_loss: 2.3271e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.1777e-04 - val_loss: 2.7033e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 2.1858e-04 - val_loss: 2.5873e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.2287e-04 - val_loss: 3.7751e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 151/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0252e-04 - val_loss: 5.1470e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 152/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.7872e-04 - val_loss: 2.2639e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 153/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8031e-04 - val_loss: 2.2216e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 154/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.8180e-04 - val_loss: 2.3909e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 155/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8230e-04 - val_loss: 2.2477e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 156/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.8367e-04 - val_loss: 1.9999e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 157/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.8558e-04 - val_loss: 2.0581e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 158/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.8363e-04 - val_loss: 2.4270e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 159/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.8375e-04 - val_loss: 1.9929e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 160/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.8315e-04 - val_loss: 2.2000e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 161/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.7919e-04 - val_loss: 1.8534e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 162/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7881e-04 - val_loss: 2.1506e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 163/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.7697e-04 - val_loss: 2.3033e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 164/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.8075e-04 - val_loss: 2.0469e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 165/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.8099e-04 - val_loss: 2.7890e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 166/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.7902e-04 - val_loss: 2.5952e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 167/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.7780e-04 - val_loss: 2.2492e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 168/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 1.7903e-04 - val_loss: 1.9460e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 169/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7729e-04 - val_loss: 2.6915e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 170/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7543e-04 - val_loss: 2.2216e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 171/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7548e-04 - val_loss: 1.9421e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 172/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 1.7332e-04 - val_loss: 1.7919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 173/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7675e-04 - val_loss: 3.1402e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 174/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7219e-04 - val_loss: 2.0285e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 175/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.7758e-04 - val_loss: 2.2732e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 176/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.7149e-04 - val_loss: 2.2899e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 177/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 1.7176e-04 - val_loss: 1.9180e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 178/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7066e-04 - val_loss: 2.2533e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 179/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7621e-04 - val_loss: 2.2026e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 180/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.7510e-04 - val_loss: 2.5848e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 181/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.7316e-04 - val_loss: 1.9878e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 182/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6919e-04 - val_loss: 1.9826e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 183/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7319e-04 - val_loss: 1.9620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 184/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 1.6876e-04 - val_loss: 2.0544e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 185/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.6610e-04 - val_loss: 1.8221e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 186/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7252e-04 - val_loss: 2.0892e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 187/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.6876e-04 - val_loss: 1.7651e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 188/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.7394e-04 - val_loss: 2.1267e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 189/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.6581e-04 - val_loss: 2.1435e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 190/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.6670e-04 - val_loss: 2.0773e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 191/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.6943e-04 - val_loss: 2.6828e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 192/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.6722e-04 - val_loss: 1.9240e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 193/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.6979e-04 - val_loss: 1.8556e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 194/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.6276e-04 - val_loss: 2.0900e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 195/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6340e-04 - val_loss: 2.5874e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 196/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.6650e-04 - val_loss: 2.4041e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 197/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.6739e-04 - val_loss: 2.2287e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 198/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.6520e-04 - val_loss: 1.9914e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 199/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.6611e-04 - val_loss: 2.4190e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 200/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.5742e-04 - val_loss: 3.0006e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 201/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6280e-04 - val_loss: 2.0315e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 202/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.6546e-04 - val_loss: 1.8134e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 203/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.6932e-04 - val_loss: 1.7403e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 204/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.6295e-04 - val_loss: 1.9310e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 205/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.6165e-04 - val_loss: 1.8386e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 206/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.6055e-04 - val_loss: 1.6594e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 207/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.5122e-04 - val_loss: 1.8623e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 208/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.5111e-04 - val_loss: 1.7181e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 209/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 1.5183e-04 - val_loss: 1.6712e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 210/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.5151e-04 - val_loss: 2.5623e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 211/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.5185e-04 - val_loss: 1.7605e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 212/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.5134e-04 - val_loss: 1.7678e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 213/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 1.5125e-04 - val_loss: 1.7273e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 214/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.5141e-04 - val_loss: 1.8584e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 215/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.5223e-04 - val_loss: 1.8939e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 216/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.5021e-04 - val_loss: 1.6293e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 217/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.5114e-04 - val_loss: 1.7229e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 218/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.4943e-04 - val_loss: 1.7022e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 219/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.5043e-04 - val_loss: 1.7380e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 220/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.4960e-04 - val_loss: 1.6858e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 221/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.5019e-04 - val_loss: 1.7948e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 222/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4935e-04 - val_loss: 2.1844e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 223/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.4988e-04 - val_loss: 1.9784e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 224/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.4797e-04 - val_loss: 1.6561e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 225/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.4961e-04 - val_loss: 1.5701e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 226/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4749e-04 - val_loss: 1.7547e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 227/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.4833e-04 - val_loss: 1.9563e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 228/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.4750e-04 - val_loss: 1.7128e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 229/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.4669e-04 - val_loss: 1.7070e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 230/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4793e-04 - val_loss: 1.7406e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 231/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.4760e-04 - val_loss: 1.7312e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 232/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.4575e-04 - val_loss: 1.6897e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 233/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.4617e-04 - val_loss: 1.7173e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 234/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4674e-04 - val_loss: 1.9842e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 235/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.4678e-04 - val_loss: 1.9550e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 236/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.4599e-04 - val_loss: 1.6822e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 237/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.4566e-04 - val_loss: 2.1225e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 238/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.4559e-04 - val_loss: 1.6163e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 239/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4405e-04 - val_loss: 1.5675e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 240/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4545e-04 - val_loss: 1.7651e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 241/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.4539e-04 - val_loss: 1.6802e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 242/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4531e-04 - val_loss: 1.6081e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 243/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.4283e-04 - val_loss: 1.8235e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 244/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.4426e-04 - val_loss: 1.4916e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 245/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.4393e-04 - val_loss: 1.6718e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 246/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 1.4370e-04 - val_loss: 1.5087e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 247/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 1.4330e-04 - val_loss: 1.8287e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 248/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4381e-04 - val_loss: 1.5798e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 249/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.4298e-04 - val_loss: 1.5281e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 250/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.4329e-04 - val_loss: 1.8272e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 251/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.4362e-04 - val_loss: 1.8274e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 252/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.4160e-04 - val_loss: 1.5679e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 253/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.4159e-04 - val_loss: 1.7790e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 254/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.4323e-04 - val_loss: 2.1238e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 255/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.4330e-04 - val_loss: 1.6081e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 256/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.4115e-04 - val_loss: 1.6150e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 257/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.3735e-04 - val_loss: 1.4490e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 258/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3744e-04 - val_loss: 1.5417e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 259/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3735e-04 - val_loss: 1.5289e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 260/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3787e-04 - val_loss: 1.4540e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 261/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.3789e-04 - val_loss: 1.5348e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 262/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3712e-04 - val_loss: 1.5101e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 263/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3759e-04 - val_loss: 1.5674e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 264/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3738e-04 - val_loss: 1.6805e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 265/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.3672e-04 - val_loss: 1.5142e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 266/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.3705e-04 - val_loss: 1.5172e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 267/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3684e-04 - val_loss: 1.4384e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 268/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.3650e-04 - val_loss: 1.4939e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 269/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.3606e-04 - val_loss: 1.5510e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 270/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.3627e-04 - val_loss: 1.5581e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 271/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3698e-04 - val_loss: 1.6499e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 272/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.3576e-04 - val_loss: 1.4692e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 273/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.3528e-04 - val_loss: 1.5676e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 274/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.3607e-04 - val_loss: 1.6092e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 275/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.3548e-04 - val_loss: 1.4777e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 276/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3589e-04 - val_loss: 1.6518e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 277/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.3584e-04 - val_loss: 1.5954e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 278/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.3506e-04 - val_loss: 1.7193e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 279/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 1.3500e-04 - val_loss: 1.6284e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 280/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.3468e-04 - val_loss: 1.7021e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 281/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.3438e-04 - val_loss: 1.6852e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 282/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.3459e-04 - val_loss: 1.5103e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 283/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.3425e-04 - val_loss: 1.5491e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 284/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.3414e-04 - val_loss: 1.5455e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 285/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.3404e-04 - val_loss: 1.8310e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 286/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.3431e-04 - val_loss: 1.4997e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 287/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.3412e-04 - val_loss: 1.5056e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 288/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3374e-04 - val_loss: 1.6248e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 289/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.3342e-04 - val_loss: 1.4439e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 290/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.3357e-04 - val_loss: 1.5092e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 291/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.3347e-04 - val_loss: 1.5286e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 292/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.3339e-04 - val_loss: 1.5774e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 293/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.3373e-04 - val_loss: 1.5256e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 294/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.3303e-04 - val_loss: 1.6669e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 295/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.3277e-04 - val_loss: 1.6736e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 296/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.3297e-04 - val_loss: 1.5824e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 297/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.3261e-04 - val_loss: 1.6385e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 298/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.3247e-04 - val_loss: 1.4719e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 299/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.3253e-04 - val_loss: 1.5648e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 300/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.3211e-04 - val_loss: 2.0806e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 301/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.3272e-04 - val_loss: 1.4509e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 302/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.3171e-04 - val_loss: 1.5508e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 303/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.3231e-04 - val_loss: 1.5986e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 304/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.3150e-04 - val_loss: 1.4600e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 305/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.3137e-04 - val_loss: 1.6941e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 306/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.3174e-04 - val_loss: 1.5007e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 307/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.2989e-04 - val_loss: 1.5373e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 308/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.2965e-04 - val_loss: 1.4122e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 309/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2939e-04 - val_loss: 1.4751e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 310/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2954e-04 - val_loss: 1.4264e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 311/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2961e-04 - val_loss: 1.5021e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 312/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.2927e-04 - val_loss: 1.4158e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 313/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2955e-04 - val_loss: 1.4022e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 314/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.2940e-04 - val_loss: 1.4862e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 315/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2926e-04 - val_loss: 1.5517e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 316/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.2915e-04 - val_loss: 1.4168e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 317/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 1.2918e-04 - val_loss: 1.4212e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 318/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2905e-04 - val_loss: 1.4462e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 319/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2891e-04 - val_loss: 1.5692e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 320/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2875e-04 - val_loss: 1.4858e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 321/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 1.2866e-04 - val_loss: 1.5552e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 322/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2862e-04 - val_loss: 1.5111e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 323/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2881e-04 - val_loss: 1.4361e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 324/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.2851e-04 - val_loss: 1.4673e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 325/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 1.2845e-04 - val_loss: 1.5333e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 326/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2855e-04 - val_loss: 1.5032e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 327/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2830e-04 - val_loss: 1.4166e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 328/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 1.2815e-04 - val_loss: 1.5819e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 329/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2830e-04 - val_loss: 1.4306e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 330/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2798e-04 - val_loss: 1.4352e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 331/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2790e-04 - val_loss: 1.5139e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 332/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 1.2806e-04 - val_loss: 1.5943e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 333/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2745e-04 - val_loss: 1.4250e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 334/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2754e-04 - val_loss: 1.4630e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 335/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2823e-04 - val_loss: 1.4146e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 336/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1.2727e-04 - val_loss: 1.5398e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 337/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2757e-04 - val_loss: 1.4715e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 338/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.2730e-04 - val_loss: 1.4261e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 339/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 1.2745e-04 - val_loss: 1.3885e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 340/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2725e-04 - val_loss: 1.3664e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 341/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2725e-04 - val_loss: 1.3767e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 342/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2708e-04 - val_loss: 1.3984e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 343/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.2694e-04 - val_loss: 1.3774e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 344/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2722e-04 - val_loss: 1.3718e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 345/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2700e-04 - val_loss: 1.4874e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 346/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2683e-04 - val_loss: 1.3905e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 347/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2658e-04 - val_loss: 1.4250e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 348/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2662e-04 - val_loss: 1.3704e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 349/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2670e-04 - val_loss: 1.5767e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 350/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2653e-04 - val_loss: 1.5031e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 351/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2649e-04 - val_loss: 1.4248e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 352/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2619e-04 - val_loss: 1.4745e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 353/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2638e-04 - val_loss: 1.4645e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 354/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.2634e-04 - val_loss: 1.4489e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 355/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2620e-04 - val_loss: 1.3558e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 356/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2583e-04 - val_loss: 1.3272e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 357/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.2522e-04 - val_loss: 1.3503e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 358/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2521e-04 - val_loss: 1.3461e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 359/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2509e-04 - val_loss: 1.3372e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 360/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2509e-04 - val_loss: 1.4230e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 361/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2525e-04 - val_loss: 1.3510e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 362/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2515e-04 - val_loss: 1.3326e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 363/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2500e-04 - val_loss: 1.3548e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 364/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2490e-04 - val_loss: 1.3570e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 365/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2505e-04 - val_loss: 1.3976e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 366/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2491e-04 - val_loss: 1.3764e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 367/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2474e-04 - val_loss: 1.4759e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 368/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.2488e-04 - val_loss: 1.4334e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 369/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2471e-04 - val_loss: 1.3579e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 370/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2469e-04 - val_loss: 1.3733e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 371/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2446e-04 - val_loss: 1.3934e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 372/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2480e-04 - val_loss: 1.3452e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 373/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2441e-04 - val_loss: 1.3496e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 374/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2452e-04 - val_loss: 1.3216e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 375/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.2445e-04 - val_loss: 1.3967e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 376/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2430e-04 - val_loss: 1.3609e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 377/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2438e-04 - val_loss: 1.3775e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 378/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2436e-04 - val_loss: 1.3929e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 379/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2442e-04 - val_loss: 1.4082e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 380/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2412e-04 - val_loss: 1.4271e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 381/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.2413e-04 - val_loss: 1.3642e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 382/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2416e-04 - val_loss: 1.3226e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 383/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2415e-04 - val_loss: 1.3736e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 384/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2407e-04 - val_loss: 1.4046e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 385/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2390e-04 - val_loss: 1.3114e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 386/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2397e-04 - val_loss: 1.3373e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 387/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2388e-04 - val_loss: 1.3448e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 388/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2385e-04 - val_loss: 1.3771e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 389/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2376e-04 - val_loss: 1.4251e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 390/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2384e-04 - val_loss: 1.3305e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 391/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.2379e-04 - val_loss: 1.3704e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 392/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2353e-04 - val_loss: 1.3344e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 393/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2367e-04 - val_loss: 1.3516e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 394/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2357e-04 - val_loss: 1.3254e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 395/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2347e-04 - val_loss: 1.3196e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 396/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2333e-04 - val_loss: 1.3378e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 397/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2354e-04 - val_loss: 1.3623e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 398/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2344e-04 - val_loss: 1.3562e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 399/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.2338e-04 - val_loss: 1.4117e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 400/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2336e-04 - val_loss: 1.4601e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 401/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2323e-04 - val_loss: 1.4207e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 402/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.2315e-04 - val_loss: 1.4336e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 403/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2309e-04 - val_loss: 1.3672e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 404/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2319e-04 - val_loss: 1.4037e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 405/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2310e-04 - val_loss: 1.3354e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 406/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2290e-04 - val_loss: 1.2997e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 407/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2260e-04 - val_loss: 1.3650e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 408/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2254e-04 - val_loss: 1.3534e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 409/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2254e-04 - val_loss: 1.3148e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 410/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2252e-04 - val_loss: 1.3491e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 411/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2256e-04 - val_loss: 1.3559e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 412/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2252e-04 - val_loss: 1.3404e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 413/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2245e-04 - val_loss: 1.3478e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 414/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2241e-04 - val_loss: 1.3382e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 415/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2241e-04 - val_loss: 1.3336e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 416/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2241e-04 - val_loss: 1.3405e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 417/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2227e-04 - val_loss: 1.3746e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 418/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2232e-04 - val_loss: 1.3473e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 419/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2235e-04 - val_loss: 1.3385e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 420/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2225e-04 - val_loss: 1.3401e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 421/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2222e-04 - val_loss: 1.3598e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 422/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2218e-04 - val_loss: 1.3882e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 423/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2212e-04 - val_loss: 1.3435e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 424/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2213e-04 - val_loss: 1.3329e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 425/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2207e-04 - val_loss: 1.4177e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 426/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 1.2216e-04 - val_loss: 1.3565e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 427/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2209e-04 - val_loss: 1.3638e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 428/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2213e-04 - val_loss: 1.3297e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 429/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2198e-04 - val_loss: 1.3715e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 430/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2205e-04 - val_loss: 1.3297e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 431/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2191e-04 - val_loss: 1.3138e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 432/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2190e-04 - val_loss: 1.3254e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 433/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2194e-04 - val_loss: 1.3321e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 434/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2183e-04 - val_loss: 1.3395e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 435/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2185e-04 - val_loss: 1.3260e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 436/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2178e-04 - val_loss: 1.3987e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 437/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2179e-04 - val_loss: 1.3243e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 438/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2183e-04 - val_loss: 1.3618e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 439/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2175e-04 - val_loss: 1.3264e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 440/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2171e-04 - val_loss: 1.3413e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 441/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2172e-04 - val_loss: 1.3563e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 442/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.2168e-04 - val_loss: 1.3645e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 443/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.2171e-04 - val_loss: 1.3022e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 444/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.2172e-04 - val_loss: 1.3269e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 445/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 1.2154e-04 - val_loss: 1.3721e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 446/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2156e-04 - val_loss: 1.3450e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 447/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2154e-04 - val_loss: 1.3355e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 448/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2148e-04 - val_loss: 1.3436e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 449/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2150e-04 - val_loss: 1.3371e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 450/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.2151e-04 - val_loss: 1.3245e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 451/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2139e-04 - val_loss: 1.3096e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 452/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2143e-04 - val_loss: 1.3915e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 453/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.2141e-04 - val_loss: 1.3318e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 454/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2138e-04 - val_loss: 1.3239e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 455/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2139e-04 - val_loss: 1.3480e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 456/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.2130e-04 - val_loss: 1.3504e-04 - learning_rate: 7.8125e-06\n",
      "Epoch 457/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2110e-04 - val_loss: 1.3674e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 458/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2104e-04 - val_loss: 1.3383e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 459/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2103e-04 - val_loss: 1.3307e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 460/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2105e-04 - val_loss: 1.3828e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 461/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2104e-04 - val_loss: 1.3510e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 462/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2103e-04 - val_loss: 1.3592e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 463/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2100e-04 - val_loss: 1.3593e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 464/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2094e-04 - val_loss: 1.3604e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 465/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.2101e-04 - val_loss: 1.3424e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 466/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2097e-04 - val_loss: 1.3573e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 467/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2095e-04 - val_loss: 1.3126e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 468/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.2094e-04 - val_loss: 1.3381e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 469/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2090e-04 - val_loss: 1.3494e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 470/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.2085e-04 - val_loss: 1.3414e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 471/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.2088e-04 - val_loss: 1.3326e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 472/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2085e-04 - val_loss: 1.3379e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 473/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 1.2087e-04 - val_loss: 1.3602e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 474/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2087e-04 - val_loss: 1.3547e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 475/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2082e-04 - val_loss: 1.3344e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 476/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2083e-04 - val_loss: 1.3457e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 477/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2082e-04 - val_loss: 1.3532e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 478/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2078e-04 - val_loss: 1.3129e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 479/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2076e-04 - val_loss: 1.3096e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 480/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2072e-04 - val_loss: 1.3125e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 481/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2068e-04 - val_loss: 1.3478e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 482/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2068e-04 - val_loss: 1.3448e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 483/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.2073e-04 - val_loss: 1.3397e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 484/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2070e-04 - val_loss: 1.4045e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 485/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.2065e-04 - val_loss: 1.3337e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 486/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2069e-04 - val_loss: 1.3479e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 487/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2063e-04 - val_loss: 1.3052e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 488/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2063e-04 - val_loss: 1.3482e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 489/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 1.2062e-04 - val_loss: 1.3091e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 490/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.2053e-04 - val_loss: 1.3640e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 491/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2061e-04 - val_loss: 1.3289e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 492/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2057e-04 - val_loss: 1.3683e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 493/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.2056e-04 - val_loss: 1.3187e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 494/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2053e-04 - val_loss: 1.3236e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 495/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.2056e-04 - val_loss: 1.3400e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 496/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 1.2049e-04 - val_loss: 1.3332e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 497/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.2043e-04 - val_loss: 1.3263e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 498/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2045e-04 - val_loss: 1.3496e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 499/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.2047e-04 - val_loss: 1.3073e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 500/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.2048e-04 - val_loss: 1.3369e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 501/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2044e-04 - val_loss: 1.3200e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 502/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.2035e-04 - val_loss: 1.3333e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 503/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2041e-04 - val_loss: 1.3081e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 504/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2037e-04 - val_loss: 1.3041e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 505/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 1.2045e-04 - val_loss: 1.2997e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 506/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2037e-04 - val_loss: 1.3165e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 507/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.2022e-04 - val_loss: 1.3151e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 508/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.2023e-04 - val_loss: 1.3432e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 509/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2021e-04 - val_loss: 1.3241e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 510/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 1.2024e-04 - val_loss: 1.3166e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 511/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2023e-04 - val_loss: 1.2965e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 512/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.2021e-04 - val_loss: 1.3248e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 513/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.2020e-04 - val_loss: 1.3246e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 514/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2019e-04 - val_loss: 1.3223e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 515/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2018e-04 - val_loss: 1.3153e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 516/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.2014e-04 - val_loss: 1.3288e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 517/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2015e-04 - val_loss: 1.3251e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 518/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.2014e-04 - val_loss: 1.3312e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 519/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.2014e-04 - val_loss: 1.3250e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 520/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.2015e-04 - val_loss: 1.3465e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 521/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.2013e-04 - val_loss: 1.3354e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 522/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.2011e-04 - val_loss: 1.3160e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 523/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.2012e-04 - val_loss: 1.3378e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 524/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.2010e-04 - val_loss: 1.3399e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 525/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.2008e-04 - val_loss: 1.3654e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 526/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.2008e-04 - val_loss: 1.3269e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 527/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.2008e-04 - val_loss: 1.3200e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 528/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 1.2003e-04 - val_loss: 1.3343e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 529/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.2006e-04 - val_loss: 1.3399e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 530/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.2003e-04 - val_loss: 1.3368e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 531/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2004e-04 - val_loss: 1.3183e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 532/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.2000e-04 - val_loss: 1.3234e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 533/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.2002e-04 - val_loss: 1.3395e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 534/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.2002e-04 - val_loss: 1.3419e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 535/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.2001e-04 - val_loss: 1.3092e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 536/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.2000e-04 - val_loss: 1.3458e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 537/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.1998e-04 - val_loss: 1.3260e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 538/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.1997e-04 - val_loss: 1.3448e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 539/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1996e-04 - val_loss: 1.3311e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 540/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.1998e-04 - val_loss: 1.3346e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 541/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1996e-04 - val_loss: 1.3427e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 542/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.1992e-04 - val_loss: 1.3135e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 543/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1995e-04 - val_loss: 1.3483e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 544/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1997e-04 - val_loss: 1.3007e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 545/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1994e-04 - val_loss: 1.3170e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 546/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.1991e-04 - val_loss: 1.3153e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 547/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.1989e-04 - val_loss: 1.3284e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 548/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.1986e-04 - val_loss: 1.3328e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 549/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.1989e-04 - val_loss: 1.3235e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 550/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1993e-04 - val_loss: 1.3198e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 551/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1990e-04 - val_loss: 1.3398e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 552/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.1985e-04 - val_loss: 1.3109e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 553/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1985e-04 - val_loss: 1.3261e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 554/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1985e-04 - val_loss: 1.3249e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 555/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.1985e-04 - val_loss: 1.2991e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 556/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1984e-04 - val_loss: 1.3095e-04 - learning_rate: 1.9531e-06\n",
      "Epoch 557/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1982e-04 - val_loss: 1.3266e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 558/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1984e-04 - val_loss: 1.3342e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 559/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1980e-04 - val_loss: 1.3184e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 560/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1979e-04 - val_loss: 1.3174e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 561/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1978e-04 - val_loss: 1.3420e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 562/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.1978e-04 - val_loss: 1.3175e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 563/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1976e-04 - val_loss: 1.3417e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 564/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.1977e-04 - val_loss: 1.3382e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 565/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.1977e-04 - val_loss: 1.3271e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 566/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.1976e-04 - val_loss: 1.3284e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 567/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1976e-04 - val_loss: 1.3381e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 568/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.1976e-04 - val_loss: 1.3329e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 569/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1976e-04 - val_loss: 1.3237e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 570/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1974e-04 - val_loss: 1.3253e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 571/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1975e-04 - val_loss: 1.3457e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 572/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1973e-04 - val_loss: 1.3289e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 573/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.1974e-04 - val_loss: 1.3321e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 574/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.1971e-04 - val_loss: 1.3293e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 575/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 1.1975e-04 - val_loss: 1.3176e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 576/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1972e-04 - val_loss: 1.3143e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 577/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1972e-04 - val_loss: 1.3178e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 578/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1972e-04 - val_loss: 1.3195e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 579/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1970e-04 - val_loss: 1.3344e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 580/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1971e-04 - val_loss: 1.3170e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 581/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 1.1970e-04 - val_loss: 1.3103e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 582/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.1970e-04 - val_loss: 1.3101e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 583/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 1.1968e-04 - val_loss: 1.3188e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 584/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1972e-04 - val_loss: 1.3076e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 585/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1970e-04 - val_loss: 1.3174e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 586/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.1967e-04 - val_loss: 1.3117e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 587/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1969e-04 - val_loss: 1.3264e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 588/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.1967e-04 - val_loss: 1.3243e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 589/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1966e-04 - val_loss: 1.3411e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 590/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1966e-04 - val_loss: 1.3187e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 591/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1965e-04 - val_loss: 1.3298e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 592/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1967e-04 - val_loss: 1.3133e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 593/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.1966e-04 - val_loss: 1.3164e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 594/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.1966e-04 - val_loss: 1.3198e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 595/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1964e-04 - val_loss: 1.3074e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 596/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 1.1963e-04 - val_loss: 1.3085e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 597/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1962e-04 - val_loss: 1.3141e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 598/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1963e-04 - val_loss: 1.3346e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 599/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1959e-04 - val_loss: 1.3283e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 600/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.1962e-04 - val_loss: 1.3158e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 601/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1964e-04 - val_loss: 1.3288e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 602/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1961e-04 - val_loss: 1.3129e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 603/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1960e-04 - val_loss: 1.3117e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 604/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1960e-04 - val_loss: 1.3101e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 605/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 1.1961e-04 - val_loss: 1.3209e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 606/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.1962e-04 - val_loss: 1.3216e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 607/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.1960e-04 - val_loss: 1.3062e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 608/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1959e-04 - val_loss: 1.3255e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 609/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.1959e-04 - val_loss: 1.3123e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 610/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.1959e-04 - val_loss: 1.3150e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 611/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1959e-04 - val_loss: 1.3211e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 612/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.1958e-04 - val_loss: 1.3206e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 613/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.1959e-04 - val_loss: 1.3098e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 614/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.1958e-04 - val_loss: 1.3175e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 615/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.1957e-04 - val_loss: 1.3066e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 616/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.1957e-04 - val_loss: 1.3011e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 617/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.1956e-04 - val_loss: 1.3026e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 618/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1956e-04 - val_loss: 1.3110e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 619/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1955e-04 - val_loss: 1.3214e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 620/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1956e-04 - val_loss: 1.3076e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 621/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.1956e-04 - val_loss: 1.3138e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 622/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1954e-04 - val_loss: 1.3098e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 623/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.1955e-04 - val_loss: 1.3123e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 624/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.1955e-04 - val_loss: 1.3213e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 625/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1955e-04 - val_loss: 1.3115e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 626/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.1954e-04 - val_loss: 1.3141e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 627/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1955e-04 - val_loss: 1.3090e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 628/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.1954e-04 - val_loss: 1.3121e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 629/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1953e-04 - val_loss: 1.3114e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 630/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1952e-04 - val_loss: 1.3124e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 631/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.1954e-04 - val_loss: 1.3067e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 632/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1951e-04 - val_loss: 1.3055e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 633/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.1953e-04 - val_loss: 1.3074e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 634/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1953e-04 - val_loss: 1.3097e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 635/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.1952e-04 - val_loss: 1.3141e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 636/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.1953e-04 - val_loss: 1.3014e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 637/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1953e-04 - val_loss: 1.3057e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 638/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1951e-04 - val_loss: 1.3109e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 639/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.1951e-04 - val_loss: 1.3002e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 640/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.1951e-04 - val_loss: 1.3063e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 641/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.1950e-04 - val_loss: 1.3065e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 642/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.1950e-04 - val_loss: 1.3054e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 643/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.1950e-04 - val_loss: 1.3170e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 644/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.1949e-04 - val_loss: 1.3172e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 645/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.1950e-04 - val_loss: 1.3000e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 646/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 1.1951e-04 - val_loss: 1.3077e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 647/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.1951e-04 - val_loss: 1.3015e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 648/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 1.1949e-04 - val_loss: 1.3073e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 649/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.1949e-04 - val_loss: 1.3114e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 650/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.1948e-04 - val_loss: 1.3038e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 651/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.1951e-04 - val_loss: 1.3069e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 652/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.1947e-04 - val_loss: 1.3051e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 653/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.1949e-04 - val_loss: 1.3201e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 654/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.1948e-04 - val_loss: 1.3054e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 655/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.1946e-04 - val_loss: 1.3083e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 656/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.1949e-04 - val_loss: 1.3122e-04 - learning_rate: 4.8828e-07\n",
      "Epoch 657/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.1951e-04 - val_loss: 1.3154e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 658/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.1950e-04 - val_loss: 1.3160e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 659/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.1951e-04 - val_loss: 1.3136e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 660/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 1.1950e-04 - val_loss: 1.3115e-04 - learning_rate: 2.4414e-07\n",
      "Epoch 661/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.1949e-04 - val_loss: 1.3145e-04 - learning_rate: 2.4414e-07\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step\n",
      "Model 1 Test Set RMSE: 0.003779 AU\n",
      "\n",
      "--- Training MLP Model 2 (Seed: 2) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841us/step - loss: 0.0476 - val_loss: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0163 - val_loss: 0.0283 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0111 - val_loss: 0.0217 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.0079 - val_loss: 0.0143 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0056 - val_loss: 0.0135 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0042 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0032 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.0024 - val_loss: 0.0068 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.0023 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.0017 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.0015 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.0014 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0012 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 0.0014 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0012 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.0010 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 9.6822e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.0010 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 9.0797e-04 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 7.8721e-04 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.9513e-04 - val_loss: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 8.2920e-04 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 8.2059e-04 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 7.0453e-04 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 7.5125e-04 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 7.2078e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 5.9660e-04 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 7.2595e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 6.8523e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 5.8051e-04 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 8.0770e-04 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 5.7195e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.9883e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.1762e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 5.5616e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 5.2197e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 5.4589e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 5.9044e-04 - val_loss: 8.8154e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 4.7021e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 5.5195e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 4.6935e-04 - val_loss: 9.5819e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.7864e-04 - val_loss: 6.7694e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.5834e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 4.7340e-04 - val_loss: 8.6351e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 4.7928e-04 - val_loss: 6.5995e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.1226e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 5.1227e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 4.3999e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.2002e-04 - val_loss: 6.6868e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.2170e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 3.2688e-04 - val_loss: 4.9118e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 3.1574e-04 - val_loss: 5.6916e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.0929e-04 - val_loss: 5.7608e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 3.2733e-04 - val_loss: 6.4590e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 3.3189e-04 - val_loss: 4.7992e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.0753e-04 - val_loss: 6.9255e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 3.0212e-04 - val_loss: 7.5501e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.3155e-04 - val_loss: 4.8887e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.9633e-04 - val_loss: 6.8689e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.8573e-04 - val_loss: 3.9961e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 3.0081e-04 - val_loss: 3.8563e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.9668e-04 - val_loss: 4.9494e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.8836e-04 - val_loss: 4.7287e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.8787e-04 - val_loss: 3.2990e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 2.8310e-04 - val_loss: 3.5941e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.8022e-04 - val_loss: 5.0823e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 2.7657e-04 - val_loss: 3.5980e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.9052e-04 - val_loss: 8.2452e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 2.8653e-04 - val_loss: 3.3439e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.7956e-04 - val_loss: 3.1739e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.7852e-04 - val_loss: 3.3288e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.8271e-04 - val_loss: 8.2015e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.6768e-04 - val_loss: 3.8993e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.6712e-04 - val_loss: 4.0149e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.7791e-04 - val_loss: 3.5228e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.7057e-04 - val_loss: 4.2823e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 2.6176e-04 - val_loss: 2.8939e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.7288e-04 - val_loss: 7.8969e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.7076e-04 - val_loss: 3.4149e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.6443e-04 - val_loss: 3.3678e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.7108e-04 - val_loss: 4.0596e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.5306e-04 - val_loss: 3.5872e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.6038e-04 - val_loss: 3.9069e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.6074e-04 - val_loss: 5.6781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.4856e-04 - val_loss: 4.4189e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.5536e-04 - val_loss: 3.5011e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.5958e-04 - val_loss: 3.0798e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.4063e-04 - val_loss: 3.5790e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.5103e-04 - val_loss: 8.7819e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.4517e-04 - val_loss: 2.9385e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.7454e-04 - val_loss: 2.5305e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 2.4076e-04 - val_loss: 2.9464e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.5019e-04 - val_loss: 2.9801e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.4304e-04 - val_loss: 2.7621e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.5253e-04 - val_loss: 3.0882e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.3092e-04 - val_loss: 3.0441e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 2.5432e-04 - val_loss: 4.2820e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 2.4343e-04 - val_loss: 4.2531e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.2567e-04 - val_loss: 2.7018e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 2.4857e-04 - val_loss: 4.8216e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.0080e-04 - val_loss: 2.4649e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.0273e-04 - val_loss: 2.5759e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.0520e-04 - val_loss: 2.4050e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0352e-04 - val_loss: 2.5403e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.0437e-04 - val_loss: 2.9854e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0550e-04 - val_loss: 2.5831e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.0571e-04 - val_loss: 2.7205e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 2.0328e-04 - val_loss: 2.6082e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.0053e-04 - val_loss: 2.3557e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.0586e-04 - val_loss: 2.3786e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.9865e-04 - val_loss: 2.7353e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.0260e-04 - val_loss: 2.5689e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9865e-04 - val_loss: 3.1445e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.9851e-04 - val_loss: 2.2426e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.9961e-04 - val_loss: 2.3690e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.9850e-04 - val_loss: 2.3390e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.9274e-04 - val_loss: 2.2922e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.9407e-04 - val_loss: 3.9571e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 2.0400e-04 - val_loss: 2.2166e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.9064e-04 - val_loss: 2.2887e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.9123e-04 - val_loss: 2.9537e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9729e-04 - val_loss: 2.1290e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.9151e-04 - val_loss: 3.6950e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9422e-04 - val_loss: 2.1162e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.8805e-04 - val_loss: 2.1461e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8894e-04 - val_loss: 2.1643e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.9382e-04 - val_loss: 2.1620e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.8676e-04 - val_loss: 2.7139e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.8885e-04 - val_loss: 2.4450e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 1.8748e-04 - val_loss: 2.3794e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8669e-04 - val_loss: 2.9870e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.9118e-04 - val_loss: 7.1174e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.9812e-04 - val_loss: 2.1713e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8419e-04 - val_loss: 1.9784e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.8526e-04 - val_loss: 1.9412e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.8489e-04 - val_loss: 2.2061e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8453e-04 - val_loss: 2.1450e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.8893e-04 - val_loss: 2.0325e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.8108e-04 - val_loss: 2.0851e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 1.7992e-04 - val_loss: 2.3957e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8384e-04 - val_loss: 2.7032e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 1.8036e-04 - val_loss: 2.0488e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.8624e-04 - val_loss: 2.2436e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.7837e-04 - val_loss: 1.9124e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.7933e-04 - val_loss: 1.9293e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 1.7981e-04 - val_loss: 1.9963e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.7868e-04 - val_loss: 2.2217e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.7672e-04 - val_loss: 2.4915e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8012e-04 - val_loss: 1.8853e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.7384e-04 - val_loss: 2.0817e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step\n",
      "Model 2 Test Set RMSE: 0.141092 AU\n",
      "\n",
      "--- Training MLP Model 3 (Seed: 3) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794us/step - loss: 0.0413 - val_loss: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.0168 - val_loss: 0.0280 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.0114 - val_loss: 0.0214 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0079 - val_loss: 0.0143 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0055 - val_loss: 0.0114 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.0041 - val_loss: 0.0142 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.0033 - val_loss: 0.0080 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0025 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.0022 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.0017 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.0016 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.0014 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.0015 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0011 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0011 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 9.8722e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 9.7788e-04 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 9.6388e-04 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 9.2148e-04 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.0011 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 8.4416e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 6.7964e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.7816e-04 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 7.9546e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 7.3140e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 6.6182e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 7.5125e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 6.7143e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 5.7297e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 6.4132e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 6.1524e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 6.2321e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 5.9521e-04 - val_loss: 9.3181e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 5.0807e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 5.7236e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 5.9223e-04 - val_loss: 7.3653e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 5.4631e-04 - val_loss: 6.9137e-04 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 5.1023e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 4.8238e-04 - val_loss: 9.8406e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 5.2127e-04 - val_loss: 6.6955e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 4.6303e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.8213e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 6.4093e-04 - val_loss: 5.9402e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 3.9986e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 4.4842e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 5.1530e-04 - val_loss: 4.8554e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 4.2902e-04 - val_loss: 9.3296e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 4.7305e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 5.0146e-04 - val_loss: 8.7022e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.8178e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 3.3112e-04 - val_loss: 5.5572e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.1317e-04 - val_loss: 6.4677e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.1132e-04 - val_loss: 5.5959e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.2314e-04 - val_loss: 4.6212e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 3.2156e-04 - val_loss: 4.1506e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 3.1458e-04 - val_loss: 0.0013 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 3.2110e-04 - val_loss: 5.5574e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 3.1462e-04 - val_loss: 3.7301e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 3.0300e-04 - val_loss: 3.5835e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 3.0206e-04 - val_loss: 4.4957e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.9046e-04 - val_loss: 5.5210e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 3.1142e-04 - val_loss: 3.7546e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.8399e-04 - val_loss: 4.1134e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.8832e-04 - val_loss: 5.0327e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 2.9545e-04 - val_loss: 3.2287e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.9632e-04 - val_loss: 6.3100e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 2.7147e-04 - val_loss: 2.9588e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.8725e-04 - val_loss: 3.6958e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.8325e-04 - val_loss: 2.9820e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.6070e-04 - val_loss: 3.3754e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.7599e-04 - val_loss: 5.6783e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 3.0322e-04 - val_loss: 4.9352e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.9017e-04 - val_loss: 2.9875e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.6503e-04 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 2.9560e-04 - val_loss: 7.6114e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.6598e-04 - val_loss: 3.5281e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 2.5622e-04 - val_loss: 3.0112e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 2.5990e-04 - val_loss: 2.9363e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.5572e-04 - val_loss: 3.1864e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 2.7470e-04 - val_loss: 5.6164e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.6467e-04 - val_loss: 4.6651e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 2.7207e-04 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 2.5589e-04 - val_loss: 3.1275e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.5322e-04 - val_loss: 3.2074e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 2.4823e-04 - val_loss: 2.5379e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 2.6140e-04 - val_loss: 3.1008e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.7737e-04 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.6862e-04 - val_loss: 2.7976e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 2.2566e-04 - val_loss: 2.5248e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.5528e-04 - val_loss: 6.0474e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.4304e-04 - val_loss: 6.5022e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 2.4990e-04 - val_loss: 3.7128e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.3697e-04 - val_loss: 2.9553e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 2.2847e-04 - val_loss: 3.2104e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 2.4762e-04 - val_loss: 3.3890e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.4431e-04 - val_loss: 3.1347e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.4268e-04 - val_loss: 3.0309e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.4545e-04 - val_loss: 5.0121e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.3287e-04 - val_loss: 2.3634e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.4495e-04 - val_loss: 0.0014 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 2.1789e-04 - val_loss: 2.2577e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9877e-04 - val_loss: 2.0988e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.9816e-04 - val_loss: 2.4376e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.0001e-04 - val_loss: 2.8529e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.0319e-04 - val_loss: 2.3666e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 2.0257e-04 - val_loss: 2.6231e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.0404e-04 - val_loss: 2.2517e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.9833e-04 - val_loss: 2.9602e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.0494e-04 - val_loss: 2.1641e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.9887e-04 - val_loss: 2.0314e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9781e-04 - val_loss: 2.2616e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.9535e-04 - val_loss: 3.5058e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.9850e-04 - val_loss: 2.0285e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 1.9555e-04 - val_loss: 2.2768e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9727e-04 - val_loss: 2.7060e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.0035e-04 - val_loss: 2.1282e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.9446e-04 - val_loss: 2.0151e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.8881e-04 - val_loss: 3.3086e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 1.9914e-04 - val_loss: 2.5132e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.9380e-04 - val_loss: 2.1403e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8631e-04 - val_loss: 2.3357e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 1.9098e-04 - val_loss: 2.4568e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 1.8704e-04 - val_loss: 2.0304e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.9157e-04 - val_loss: 2.6804e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.8773e-04 - val_loss: 1.9849e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.8408e-04 - val_loss: 2.2142e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.8595e-04 - val_loss: 2.1095e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 1.8659e-04 - val_loss: 2.0962e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8301e-04 - val_loss: 2.3150e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.8382e-04 - val_loss: 2.6230e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.8143e-04 - val_loss: 3.4169e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 1.9190e-04 - val_loss: 2.3872e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 1.8102e-04 - val_loss: 2.0401e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.8812e-04 - val_loss: 2.1804e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7729e-04 - val_loss: 2.1881e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7912e-04 - val_loss: 2.0323e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 1.8051e-04 - val_loss: 1.8844e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 1.7832e-04 - val_loss: 2.6218e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.8997e-04 - val_loss: 2.0177e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.7573e-04 - val_loss: 2.0865e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7770e-04 - val_loss: 2.2896e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.7751e-04 - val_loss: 1.9032e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 1.7770e-04 - val_loss: 1.9956e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 1.7906e-04 - val_loss: 2.1837e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 1.7462e-04 - val_loss: 3.6816e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.7848e-04 - val_loss: 2.2481e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.7507e-04 - val_loss: 2.1286e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 1.7279e-04 - val_loss: 1.8329e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.8243e-04 - val_loss: 2.0347e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.7650e-04 - val_loss: 2.0945e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step\n",
      "Model 3 Test Set RMSE: 0.148594 AU\n",
      "\n",
      "--- Training MLP Model 4 (Seed: 4) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796us/step - loss: 0.0455 - val_loss: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0173 - val_loss: 0.0296 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.0122 - val_loss: 0.0190 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0084 - val_loss: 0.0132 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0061 - val_loss: 0.0245 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.0045 - val_loss: 0.0097 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0034 - val_loss: 0.0158 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0027 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.0023 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0020 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.0018 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.0015 - val_loss: 0.0070 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.0013 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0012 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.0012 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.0011 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 9.9061e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 9.2153e-04 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 8.0860e-04 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.0010 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 7.3639e-04 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 9.8896e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 7.1075e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 7.1374e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 7.7266e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 7.6729e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 6.0899e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 6.0301e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 6.6481e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 6.3636e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 5.4483e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 6.7889e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.5184e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 7.2055e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.3850e-04 - val_loss: 9.7281e-04 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 4.5285e-04 - val_loss: 6.8275e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 5.3153e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 5.4958e-04 - val_loss: 6.9532e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 5.0952e-04 - val_loss: 7.3042e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 5.6185e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 4.7886e-04 - val_loss: 6.2524e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 4.5327e-04 - val_loss: 7.2261e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 5.8917e-04 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 5.3130e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.7173e-04 - val_loss: 8.5872e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 4.6117e-04 - val_loss: 6.4031e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.5154e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 4.9531e-04 - val_loss: 5.9374e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 4.3445e-04 - val_loss: 8.6115e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 4.3678e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.3314e-04 - val_loss: 4.6948e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 3.0680e-04 - val_loss: 4.4028e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 3.0987e-04 - val_loss: 4.5780e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.1413e-04 - val_loss: 5.3724e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 3.2539e-04 - val_loss: 5.0090e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 3.1782e-04 - val_loss: 4.1575e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 3.0436e-04 - val_loss: 3.8963e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 3.2943e-04 - val_loss: 3.5919e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 3.0851e-04 - val_loss: 3.2496e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.8781e-04 - val_loss: 3.9459e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.9136e-04 - val_loss: 6.9479e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.1338e-04 - val_loss: 4.0801e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.8627e-04 - val_loss: 0.0019 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 3.5501e-04 - val_loss: 3.3179e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.8974e-04 - val_loss: 3.3991e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.7804e-04 - val_loss: 4.0161e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.7898e-04 - val_loss: 3.3907e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.8571e-04 - val_loss: 6.2544e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 2.8122e-04 - val_loss: 4.9956e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.8323e-04 - val_loss: 2.8828e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.7473e-04 - val_loss: 4.5929e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.7168e-04 - val_loss: 0.0014 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.6869e-04 - val_loss: 3.1707e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.6076e-04 - val_loss: 3.3964e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 2.8606e-04 - val_loss: 5.5133e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 2.5518e-04 - val_loss: 5.2886e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.6637e-04 - val_loss: 3.8101e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.6411e-04 - val_loss: 3.3517e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.8047e-04 - val_loss: 3.7583e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.5708e-04 - val_loss: 3.7155e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 2.6938e-04 - val_loss: 5.7373e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.4591e-04 - val_loss: 5.2286e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.6453e-04 - val_loss: 2.5358e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 2.5490e-04 - val_loss: 6.5851e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 2.5742e-04 - val_loss: 2.6489e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.5934e-04 - val_loss: 4.2754e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 2.5650e-04 - val_loss: 4.4012e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 2.4426e-04 - val_loss: 4.0764e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.6729e-04 - val_loss: 7.5046e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.4093e-04 - val_loss: 4.0819e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 2.5579e-04 - val_loss: 3.4049e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 2.4798e-04 - val_loss: 2.5457e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.6131e-04 - val_loss: 5.0000e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.5250e-04 - val_loss: 3.1120e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.2676e-04 - val_loss: 2.7299e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.4840e-04 - val_loss: 2.6454e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.3877e-04 - val_loss: 3.0110e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.4326e-04 - val_loss: 7.0864e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.5029e-04 - val_loss: 2.9006e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 2.5064e-04 - val_loss: 2.6343e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9771e-04 - val_loss: 2.2834e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.9843e-04 - val_loss: 2.5577e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.9801e-04 - val_loss: 2.7615e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0645e-04 - val_loss: 2.3217e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 2.0395e-04 - val_loss: 2.7919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0150e-04 - val_loss: 2.4515e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.0182e-04 - val_loss: 2.5800e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0266e-04 - val_loss: 2.3067e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.0278e-04 - val_loss: 2.5494e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9877e-04 - val_loss: 2.5670e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 2.0217e-04 - val_loss: 2.5321e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0291e-04 - val_loss: 2.5141e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.9619e-04 - val_loss: 2.5732e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.9588e-04 - val_loss: 2.2963e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.9472e-04 - val_loss: 2.5752e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.0015e-04 - val_loss: 2.5866e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.9894e-04 - val_loss: 2.1768e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.8684e-04 - val_loss: 2.6104e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.9404e-04 - val_loss: 2.4633e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.9735e-04 - val_loss: 4.7067e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 1.9177e-04 - val_loss: 2.1773e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.8476e-04 - val_loss: 2.0779e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9144e-04 - val_loss: 2.4356e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.9263e-04 - val_loss: 2.3417e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.8510e-04 - val_loss: 1.9470e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.8709e-04 - val_loss: 2.4944e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.8587e-04 - val_loss: 2.6201e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.8976e-04 - val_loss: 2.0366e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8361e-04 - val_loss: 2.5196e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.8980e-04 - val_loss: 2.2183e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.8592e-04 - val_loss: 2.5868e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.9063e-04 - val_loss: 1.9660e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.7967e-04 - val_loss: 2.3002e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.8538e-04 - val_loss: 1.8209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.8101e-04 - val_loss: 2.0367e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8411e-04 - val_loss: 1.9901e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.8306e-04 - val_loss: 2.1545e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8206e-04 - val_loss: 3.1391e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 1.8346e-04 - val_loss: 3.3475e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.7617e-04 - val_loss: 2.0510e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.8177e-04 - val_loss: 2.4261e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 1.7739e-04 - val_loss: 2.1260e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.7556e-04 - val_loss: 2.3998e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.8384e-04 - val_loss: 1.8186e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.7861e-04 - val_loss: 3.0235e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.7453e-04 - val_loss: 2.7641e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 1.7681e-04 - val_loss: 1.7781e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7904e-04 - val_loss: 1.7606e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.7465e-04 - val_loss: 2.1387e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 1.7162e-04 - val_loss: 1.9658e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step\n",
      "Model 4 Test Set RMSE: 0.137001 AU\n",
      "\n",
      "--- Training MLP Model 5 (Seed: 5) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - loss: 0.0446 - val_loss: 0.0486 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.0162 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0110 - val_loss: 0.0227 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0075 - val_loss: 0.0221 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0054 - val_loss: 0.0204 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.0039 - val_loss: 0.0152 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0032 - val_loss: 0.0245 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0024 - val_loss: 0.0146 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.0020 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0017 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0018 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.0013 - val_loss: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.0014 - val_loss: 0.0128 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.0012 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.0010 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.0011 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 9.7023e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 9.1103e-04 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.0011 - val_loss: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0011 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 6.6053e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 7.8395e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 6.6521e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 7.0686e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 8.1655e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 7.3442e-04 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 7.9967e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.5805e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 7.1951e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 5.3481e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 6.7174e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 6.2648e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 5.1283e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 5.1694e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 7.4326e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 5.1666e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.6923e-04 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 6.0356e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 4.9736e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 5.0169e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 4.5953e-04 - val_loss: 7.8708e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 4.8897e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 6.3299e-04 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 4.7786e-04 - val_loss: 5.1944e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 3.9983e-04 - val_loss: 7.4848e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 4.8606e-04 - val_loss: 9.8577e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 4.3071e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 4.5018e-04 - val_loss: 6.9300e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 4.2266e-04 - val_loss: 8.4057e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 5.5080e-04 - val_loss: 9.2600e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 3.2915e-04 - val_loss: 4.8145e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 3.1253e-04 - val_loss: 5.0059e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 3.0739e-04 - val_loss: 5.6926e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 3.3875e-04 - val_loss: 8.6423e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 3.1517e-04 - val_loss: 4.3781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 3.2102e-04 - val_loss: 5.5493e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 3.2644e-04 - val_loss: 5.5546e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 3.1127e-04 - val_loss: 6.6786e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 3.2582e-04 - val_loss: 4.8800e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.8631e-04 - val_loss: 4.1712e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 3.0096e-04 - val_loss: 3.6731e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 2.8449e-04 - val_loss: 4.0241e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.0817e-04 - val_loss: 3.5359e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.9412e-04 - val_loss: 3.7576e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 3.0275e-04 - val_loss: 4.8278e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 3.0715e-04 - val_loss: 4.9466e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.6917e-04 - val_loss: 4.2191e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.9101e-04 - val_loss: 4.0886e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.8004e-04 - val_loss: 4.2770e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.6625e-04 - val_loss: 3.1088e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.8572e-04 - val_loss: 3.2642e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.6463e-04 - val_loss: 4.1099e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.7212e-04 - val_loss: 5.0334e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.8227e-04 - val_loss: 3.8781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 2.7642e-04 - val_loss: 2.7446e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.6354e-04 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 3.0697e-04 - val_loss: 3.1878e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 2.7357e-04 - val_loss: 3.1817e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 2.5398e-04 - val_loss: 2.8785e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.5713e-04 - val_loss: 2.5885e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.8496e-04 - val_loss: 8.9083e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 3.1455e-04 - val_loss: 2.6615e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 2.4252e-04 - val_loss: 4.1838e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.5791e-04 - val_loss: 3.2193e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 2.6715e-04 - val_loss: 2.5916e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.4786e-04 - val_loss: 3.6944e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.7427e-04 - val_loss: 3.2832e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 2.3782e-04 - val_loss: 3.0827e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.4715e-04 - val_loss: 3.2284e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 2.4836e-04 - val_loss: 7.8483e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 2.5788e-04 - val_loss: 2.5883e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.3595e-04 - val_loss: 4.8899e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 2.5084e-04 - val_loss: 4.4293e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 2.3798e-04 - val_loss: 3.4725e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.5043e-04 - val_loss: 3.2360e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 2.5352e-04 - val_loss: 4.4677e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 2.2854e-04 - val_loss: 4.5063e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.5368e-04 - val_loss: 3.1846e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 2.4083e-04 - val_loss: 3.6971e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 2.2814e-04 - val_loss: 2.2340e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.0107e-04 - val_loss: 2.7192e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.0152e-04 - val_loss: 2.1685e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 2.0095e-04 - val_loss: 2.1851e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 2.1144e-04 - val_loss: 2.1932e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.0173e-04 - val_loss: 2.0942e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.0641e-04 - val_loss: 2.6185e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.0379e-04 - val_loss: 2.1192e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.0601e-04 - val_loss: 2.1679e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0705e-04 - val_loss: 2.3466e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 2.0000e-04 - val_loss: 2.1579e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.9848e-04 - val_loss: 2.8147e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0281e-04 - val_loss: 2.1297e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9945e-04 - val_loss: 2.5905e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.9916e-04 - val_loss: 3.9953e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 2.0395e-04 - val_loss: 3.3941e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.9736e-04 - val_loss: 2.0559e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.8973e-04 - val_loss: 2.8433e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 1.9176e-04 - val_loss: 2.0289e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.0252e-04 - val_loss: 3.0168e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.9140e-04 - val_loss: 2.3850e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 1.8982e-04 - val_loss: 3.0323e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 1.8845e-04 - val_loss: 2.0431e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 1.9574e-04 - val_loss: 3.4478e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 1.8907e-04 - val_loss: 2.0850e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 1.8685e-04 - val_loss: 2.0699e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 1.8690e-04 - val_loss: 2.0942e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 1.8970e-04 - val_loss: 2.2024e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8326e-04 - val_loss: 3.8231e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.8438e-04 - val_loss: 3.2227e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.8640e-04 - val_loss: 2.2613e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.8581e-04 - val_loss: 2.0271e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.8438e-04 - val_loss: 2.0756e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.8339e-04 - val_loss: 1.9439e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 1.8499e-04 - val_loss: 2.0989e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 1.7819e-04 - val_loss: 1.9500e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.8493e-04 - val_loss: 2.0055e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 1.7966e-04 - val_loss: 1.9915e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.7985e-04 - val_loss: 2.8278e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.7917e-04 - val_loss: 1.9167e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.8438e-04 - val_loss: 1.8326e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 1.7694e-04 - val_loss: 1.9355e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 1.7627e-04 - val_loss: 1.9808e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.7742e-04 - val_loss: 2.2143e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 1.8277e-04 - val_loss: 2.2293e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.7476e-04 - val_loss: 2.8392e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 1.7669e-04 - val_loss: 1.9113e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 1.7533e-04 - val_loss: 1.8199e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.7913e-04 - val_loss: 2.5922e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.7507e-04 - val_loss: 2.1978e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 1.7299e-04 - val_loss: 2.0170e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step\n",
      "Model 5 Test Set RMSE: 0.171208 AU\n",
      "\n",
      "--- Training MLP Model 6 (Seed: 6) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 797us/step - loss: 0.0445 - val_loss: 0.0345 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.0171 - val_loss: 0.0252 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.0117 - val_loss: 0.0181 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.0084 - val_loss: 0.0198 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0059 - val_loss: 0.0108 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.0042 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.0033 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.0026 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.0021 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.0020 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.0015 - val_loss: 0.0106 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.0013 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.0014 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.0010 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.0012 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.0011 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 8.3553e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 9.6600e-04 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 9.7402e-04 - val_loss: 0.0098 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 9.6050e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 7.7758e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 7.9880e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 6.5116e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 7.6468e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 6.8704e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 6.6707e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 5.9898e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 6.3620e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 5.7810e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 6.9225e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 4.7156e-04 - val_loss: 7.8836e-04 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 7.0137e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 4.8455e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 5.5373e-04 - val_loss: 8.6294e-04 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 5.3559e-04 - val_loss: 7.7264e-04 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 5.1775e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 5.1447e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 4.8697e-04 - val_loss: 6.6051e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 5.7930e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 5.9028e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 5.1097e-04 - val_loss: 5.0576e-04 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 5.8069e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 4.7374e-04 - val_loss: 6.1923e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 4.0761e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 5.1015e-04 - val_loss: 6.6456e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 5.0401e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 4.6611e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 4.2399e-04 - val_loss: 8.6337e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 4.8072e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 4.4287e-04 - val_loss: 6.8084e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 3.2057e-04 - val_loss: 3.9281e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 3.1335e-04 - val_loss: 3.6198e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 3.0824e-04 - val_loss: 5.2060e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 3.2190e-04 - val_loss: 4.6424e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 3.3749e-04 - val_loss: 7.0702e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 3.0763e-04 - val_loss: 7.2305e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 3.4855e-04 - val_loss: 3.9117e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 2.9753e-04 - val_loss: 3.5187e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 3.0545e-04 - val_loss: 3.8668e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 2.9971e-04 - val_loss: 8.1433e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 3.0285e-04 - val_loss: 3.2660e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 3.0633e-04 - val_loss: 3.1296e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 2.8533e-04 - val_loss: 3.1165e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 2.7519e-04 - val_loss: 3.1736e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 3.3222e-04 - val_loss: 3.4464e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 2.7241e-04 - val_loss: 3.3921e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 2.9426e-04 - val_loss: 3.8031e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 3.0740e-04 - val_loss: 4.0173e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 2.6780e-04 - val_loss: 3.9726e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 2.8172e-04 - val_loss: 4.9214e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 2.7250e-04 - val_loss: 3.4759e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 2.7297e-04 - val_loss: 4.0092e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 2.9328e-04 - val_loss: 3.2428e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 2.6127e-04 - val_loss: 3.1520e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 2.7914e-04 - val_loss: 3.2005e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.7103e-04 - val_loss: 2.9066e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 2.6446e-04 - val_loss: 5.8889e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 2.6150e-04 - val_loss: 2.8894e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 2.7096e-04 - val_loss: 5.2644e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - loss: 2.5093e-04 - val_loss: 3.0296e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 3.0506e-04 - val_loss: 2.5720e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 2.4395e-04 - val_loss: 3.0857e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 2.7857e-04 - val_loss: 3.9450e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 2.5903e-04 - val_loss: 3.6094e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 2.4030e-04 - val_loss: 5.6003e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 3.2104e-04 - val_loss: 3.4632e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 2.3073e-04 - val_loss: 2.9478e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 2.3944e-04 - val_loss: 2.8136e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 2.7558e-04 - val_loss: 2.7847e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 2.4374e-04 - val_loss: 3.4251e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 2.7899e-04 - val_loss: 2.6815e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 2.4845e-04 - val_loss: 2.7513e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 2.4847e-04 - val_loss: 3.9982e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 2.8074e-04 - val_loss: 0.0018 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 2.6145e-04 - val_loss: 3.4858e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 2.3682e-04 - val_loss: 6.8265e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 2.5184e-04 - val_loss: 4.5559e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 2.6872e-04 - val_loss: 0.0013 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 2.5262e-04 - val_loss: 2.2851e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 2.4840e-04 - val_loss: 4.2247e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 2.0496e-04 - val_loss: 2.1243e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 2.0372e-04 - val_loss: 2.4039e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2.0326e-04 - val_loss: 2.3636e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 2.0630e-04 - val_loss: 2.3604e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 2.0795e-04 - val_loss: 2.2229e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 2.0178e-04 - val_loss: 2.4415e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 2.0548e-04 - val_loss: 3.7351e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 2.0565e-04 - val_loss: 2.6666e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 2.0195e-04 - val_loss: 2.7585e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 2.0444e-04 - val_loss: 2.2490e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.0507e-04 - val_loss: 2.5551e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 2.0624e-04 - val_loss: 2.7647e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 1.9727e-04 - val_loss: 2.4925e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 1.9907e-04 - val_loss: 2.0647e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9867e-04 - val_loss: 2.1788e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 1.9890e-04 - val_loss: 2.3504e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 2.0048e-04 - val_loss: 3.2821e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9365e-04 - val_loss: 2.4444e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9631e-04 - val_loss: 3.2421e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9774e-04 - val_loss: 2.5122e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 1.9673e-04 - val_loss: 2.2249e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9371e-04 - val_loss: 2.1524e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 1.9302e-04 - val_loss: 2.2251e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 1.9019e-04 - val_loss: 4.0006e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 1.9285e-04 - val_loss: 3.0134e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9222e-04 - val_loss: 2.6769e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 1.9089e-04 - val_loss: 2.6039e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - loss: 1.9059e-04 - val_loss: 2.6739e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 1.8978e-04 - val_loss: 2.4008e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8947e-04 - val_loss: 2.7331e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 1.8950e-04 - val_loss: 2.5761e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 1.9164e-04 - val_loss: 2.2131e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9365e-04 - val_loss: 3.6408e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 1.9243e-04 - val_loss: 2.0977e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 1.8313e-04 - val_loss: 2.0786e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 1.8243e-04 - val_loss: 3.4754e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.9069e-04 - val_loss: 2.4007e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 1.8568e-04 - val_loss: 2.8781e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 1.8758e-04 - val_loss: 1.9859e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8876e-04 - val_loss: 2.4977e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 1.8397e-04 - val_loss: 2.0618e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 1.8492e-04 - val_loss: 3.9695e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 1.9443e-04 - val_loss: 1.9977e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.7737e-04 - val_loss: 1.8486e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 1.8323e-04 - val_loss: 1.8387e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 1.7539e-04 - val_loss: 2.0457e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8049e-04 - val_loss: 1.8881e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 1.7890e-04 - val_loss: 1.9408e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 1.8456e-04 - val_loss: 2.9990e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8262e-04 - val_loss: 2.2299e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step\n",
      "Model 6 Test Set RMSE: 0.120244 AU\n",
      "\n",
      "--- Training MLP Model 7 (Seed: 7) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0480 - val_loss: 0.0334 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.0184 - val_loss: 0.0212 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 0.0123 - val_loss: 0.0164 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 0.0084 - val_loss: 0.0127 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0058 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 0.0043 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 0.0031 - val_loss: 0.0102 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0025 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 0.0022 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0017 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 0.0015 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.0015 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 0.0012 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 0.0011 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 0.0011 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 9.3614e-04 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 0.0013 - val_loss: 0.0115 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0010 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 7.7751e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 8.8109e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 7.8526e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 8.7633e-04 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 6.7778e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 7.6256e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 6.6229e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 8.4096e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 7.1470e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 5.5756e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 5.6727e-04 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 6.3859e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 6.5027e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 6.2108e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 6.5426e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - loss: 6.1367e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 6.6385e-04 - val_loss: 9.4008e-04 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 4.5654e-04 - val_loss: 7.3045e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 6.0276e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 5.0073e-04 - val_loss: 9.3279e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 5.0760e-04 - val_loss: 9.4170e-04 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 5.2668e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 6.2734e-04 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 4.5802e-04 - val_loss: 7.8770e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 4.9722e-04 - val_loss: 5.0127e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 3.9616e-04 - val_loss: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 6.5995e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 5.2036e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 4.6005e-04 - val_loss: 5.1939e-04 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 4.6094e-04 - val_loss: 7.4552e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 4.7923e-04 - val_loss: 9.6109e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 4.4122e-04 - val_loss: 8.6261e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 3.3675e-04 - val_loss: 4.4194e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 3.1831e-04 - val_loss: 4.0403e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 3.1967e-04 - val_loss: 4.1104e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 3.3337e-04 - val_loss: 8.0840e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 3.2267e-04 - val_loss: 4.0764e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 3.3261e-04 - val_loss: 4.6068e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 3.2937e-04 - val_loss: 3.9223e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 3.0156e-04 - val_loss: 3.8621e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 3.1551e-04 - val_loss: 4.4887e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 3.2686e-04 - val_loss: 3.9666e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 2.9143e-04 - val_loss: 4.0042e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 2.9266e-04 - val_loss: 3.6244e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 3.1340e-04 - val_loss: 6.1594e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 3.0169e-04 - val_loss: 4.0723e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 2.9717e-04 - val_loss: 3.2863e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 2.9547e-04 - val_loss: 3.2724e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 2.7273e-04 - val_loss: 4.0847e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 3.0541e-04 - val_loss: 3.1104e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 2.8127e-04 - val_loss: 3.5698e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 3.0198e-04 - val_loss: 8.2117e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 2.8766e-04 - val_loss: 2.8389e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 2.8646e-04 - val_loss: 4.3669e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 2.6639e-04 - val_loss: 3.7357e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 2.9387e-04 - val_loss: 3.4852e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 2.6305e-04 - val_loss: 2.8318e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 2.8388e-04 - val_loss: 3.9679e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 2.7169e-04 - val_loss: 3.6033e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 2.8169e-04 - val_loss: 3.0316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 3.0074e-04 - val_loss: 4.1312e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 2.6896e-04 - val_loss: 2.7119e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 2.5299e-04 - val_loss: 2.9454e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 2.7004e-04 - val_loss: 7.5316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 2.7791e-04 - val_loss: 5.8738e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 2.7348e-04 - val_loss: 2.5684e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 2.6274e-04 - val_loss: 4.4780e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 2.5500e-04 - val_loss: 2.8013e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 2.7245e-04 - val_loss: 3.0028e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 2.5034e-04 - val_loss: 3.2181e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 2.6991e-04 - val_loss: 5.2367e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 2.5449e-04 - val_loss: 9.1052e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 2.9341e-04 - val_loss: 5.0631e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 2.5677e-04 - val_loss: 2.7103e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 2.5595e-04 - val_loss: 4.1037e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 2.5300e-04 - val_loss: 4.5839e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 2.5621e-04 - val_loss: 2.8674e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2.3353e-04 - val_loss: 2.4871e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 2.4513e-04 - val_loss: 2.5390e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 2.6654e-04 - val_loss: 3.2450e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 2.4108e-04 - val_loss: 2.5595e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 3.3924e-04 - val_loss: 0.0043 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 2.3840e-04 - val_loss: 2.2841e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 2.0629e-04 - val_loss: 2.3992e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 2.0653e-04 - val_loss: 2.2465e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 2.0686e-04 - val_loss: 2.2215e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 2.0738e-04 - val_loss: 2.8153e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 2.1328e-04 - val_loss: 2.4953e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 2.0979e-04 - val_loss: 2.5418e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 2.0888e-04 - val_loss: 2.2777e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 2.1074e-04 - val_loss: 2.2198e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.1364e-04 - val_loss: 2.6356e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 2.0634e-04 - val_loss: 2.7741e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 2.1150e-04 - val_loss: 2.8340e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 2.1285e-04 - val_loss: 2.1438e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 2.0410e-04 - val_loss: 3.0767e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 2.0441e-04 - val_loss: 3.3623e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 2.0433e-04 - val_loss: 2.1947e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 2.0275e-04 - val_loss: 2.2917e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 2.0650e-04 - val_loss: 2.6424e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 2.0475e-04 - val_loss: 2.2032e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 2.0056e-04 - val_loss: 2.5096e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 1.9681e-04 - val_loss: 2.6785e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.0428e-04 - val_loss: 2.2826e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 2.0345e-04 - val_loss: 2.0356e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 1.9439e-04 - val_loss: 2.1709e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 1.9532e-04 - val_loss: 2.4137e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.9777e-04 - val_loss: 2.0645e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 1.9812e-04 - val_loss: 2.3243e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 1.9855e-04 - val_loss: 2.1805e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 1.9262e-04 - val_loss: 2.1849e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 1.9348e-04 - val_loss: 2.3418e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.9239e-04 - val_loss: 2.0262e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 1.9074e-04 - val_loss: 2.0919e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 1.9009e-04 - val_loss: 3.0894e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 1.9353e-04 - val_loss: 2.9314e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 1.9146e-04 - val_loss: 2.3313e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 1.9346e-04 - val_loss: 2.3098e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 1.8888e-04 - val_loss: 2.6047e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.8649e-04 - val_loss: 1.9759e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.8919e-04 - val_loss: 2.2936e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 1.8688e-04 - val_loss: 2.2181e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 1.8968e-04 - val_loss: 3.2717e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 1.9473e-04 - val_loss: 2.2681e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.8499e-04 - val_loss: 2.1288e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 1.8028e-04 - val_loss: 1.9000e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.8334e-04 - val_loss: 2.1943e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 1.8684e-04 - val_loss: 1.9216e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 1.8678e-04 - val_loss: 1.9978e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 1.8121e-04 - val_loss: 2.1078e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.7853e-04 - val_loss: 2.2535e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 1.8286e-04 - val_loss: 2.0624e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step\n",
      "Model 7 Test Set RMSE: 0.105186 AU\n",
      "\n",
      "--- Training MLP Model 8 (Seed: 8) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852us/step - loss: 0.0441 - val_loss: 0.0416 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.0163 - val_loss: 0.0289 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.0111 - val_loss: 0.0339 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.0076 - val_loss: 0.0201 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.0056 - val_loss: 0.0172 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.0040 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 0.0032 - val_loss: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.0027 - val_loss: 0.0127 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 0.0021 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.0019 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.0017 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 0.0014 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.0014 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.0011 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.0013 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.0010 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 9.5922e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 9.4226e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 8.1078e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.0010 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 7.7864e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 7.3408e-04 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 7.6205e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 8.7973e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 6.4035e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 7.8496e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 6.5415e-04 - val_loss: 9.5612e-04 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 5.3707e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 6.3595e-04 - val_loss: 9.4345e-04 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 6.5044e-04 - val_loss: 7.9195e-04 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 5.2035e-04 - val_loss: 7.2001e-04 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 5.6903e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 5.6919e-04 - val_loss: 8.3141e-04 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 4.8104e-04 - val_loss: 9.8152e-04 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 6.7713e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.2912e-04 - val_loss: 9.2612e-04 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 5.1258e-04 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 5.2975e-04 - val_loss: 5.9352e-04 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 4.7853e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 5.2986e-04 - val_loss: 6.2768e-04 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 4.4515e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 4.6857e-04 - val_loss: 7.8988e-04 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 4.0660e-04 - val_loss: 4.3076e-04 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 4.1527e-04 - val_loss: 7.3599e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 4.4571e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 5.2112e-04 - val_loss: 6.1067e-04 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 4.6728e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 3.9594e-04 - val_loss: 4.6016e-04 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 5.0809e-04 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 4.3019e-04 - val_loss: 4.3805e-04 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 3.0109e-04 - val_loss: 3.7797e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 2.9578e-04 - val_loss: 3.6576e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 2.9606e-04 - val_loss: 3.6864e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 3.0097e-04 - val_loss: 5.9727e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 3.2023e-04 - val_loss: 5.1382e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 3.0732e-04 - val_loss: 5.5000e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 3.0109e-04 - val_loss: 3.2227e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.8832e-04 - val_loss: 4.6064e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 3.2739e-04 - val_loss: 3.1096e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 2.8620e-04 - val_loss: 3.0901e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 2.7477e-04 - val_loss: 4.0237e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 2.7518e-04 - val_loss: 4.6219e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.9783e-04 - val_loss: 8.1298e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 2.7281e-04 - val_loss: 3.8316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 999us/step - loss: 2.9550e-04 - val_loss: 2.6760e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.6868e-04 - val_loss: 3.7432e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.6140e-04 - val_loss: 3.3018e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.8846e-04 - val_loss: 3.3316e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 2.6784e-04 - val_loss: 2.9804e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 2.6440e-04 - val_loss: 3.1431e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 2.6358e-04 - val_loss: 3.3013e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 2.8778e-04 - val_loss: 3.6627e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 2.4840e-04 - val_loss: 2.9204e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 2.9229e-04 - val_loss: 2.9239e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 2.4878e-04 - val_loss: 3.8784e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 2.5956e-04 - val_loss: 2.4591e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.5328e-04 - val_loss: 2.8988e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 2.5920e-04 - val_loss: 3.2652e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.3795e-04 - val_loss: 2.8665e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 2.5137e-04 - val_loss: 5.7735e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.5539e-04 - val_loss: 2.6658e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 2.7701e-04 - val_loss: 5.2053e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.5536e-04 - val_loss: 3.8800e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.2521e-04 - val_loss: 2.3446e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 2.4805e-04 - val_loss: 2.5529e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.4322e-04 - val_loss: 4.3839e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 2.5653e-04 - val_loss: 3.1837e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 2.2966e-04 - val_loss: 4.3172e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 2.4288e-04 - val_loss: 2.4620e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 2.4545e-04 - val_loss: 4.4832e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 2.2769e-04 - val_loss: 2.3447e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 2.3848e-04 - val_loss: 4.5552e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 2.3424e-04 - val_loss: 4.9994e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 2.4862e-04 - val_loss: 2.5688e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 2.2368e-04 - val_loss: 2.7569e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 2.2731e-04 - val_loss: 2.9781e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 2.2281e-04 - val_loss: 7.5836e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.3773e-04 - val_loss: 8.0312e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 2.3091e-04 - val_loss: 2.5579e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 2.4082e-04 - val_loss: 2.4931e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 1.9328e-04 - val_loss: 2.2173e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 1.9178e-04 - val_loss: 2.2495e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 1.9467e-04 - val_loss: 2.0544e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 1.9488e-04 - val_loss: 2.2964e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 1.9627e-04 - val_loss: 2.1681e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 1.9931e-04 - val_loss: 2.1064e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 1.9342e-04 - val_loss: 2.4325e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 1.9408e-04 - val_loss: 2.0031e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 2.0171e-04 - val_loss: 2.1045e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 1.9035e-04 - val_loss: 3.0184e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 1.9298e-04 - val_loss: 2.2992e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 1.9028e-04 - val_loss: 2.1145e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 1.9132e-04 - val_loss: 2.2491e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 1.9387e-04 - val_loss: 2.5381e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 1.8715e-04 - val_loss: 2.2136e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 1.8845e-04 - val_loss: 2.1571e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 1.8655e-04 - val_loss: 2.7427e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.8933e-04 - val_loss: 3.4572e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 1.8699e-04 - val_loss: 2.0380e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 1.8462e-04 - val_loss: 2.8801e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 1.8673e-04 - val_loss: 2.4195e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 1.8355e-04 - val_loss: 1.8576e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.8033e-04 - val_loss: 1.9529e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 1.8649e-04 - val_loss: 3.0852e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.8213e-04 - val_loss: 2.1394e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 1.7716e-04 - val_loss: 2.3446e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.7821e-04 - val_loss: 2.1511e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 1.8008e-04 - val_loss: 1.9269e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 1.8074e-04 - val_loss: 1.9213e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.7452e-04 - val_loss: 2.0625e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 1.8202e-04 - val_loss: 2.2649e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 1.7553e-04 - val_loss: 2.4095e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 1.7944e-04 - val_loss: 1.9785e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 1.7146e-04 - val_loss: 1.9758e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 1.8646e-04 - val_loss: 3.4393e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.8028e-04 - val_loss: 1.9809e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 1.7091e-04 - val_loss: 1.8341e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 1.7575e-04 - val_loss: 1.8800e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 1.7697e-04 - val_loss: 1.8660e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 1.7050e-04 - val_loss: 1.8209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 1.7269e-04 - val_loss: 1.8908e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 1.7167e-04 - val_loss: 2.1956e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 1.7945e-04 - val_loss: 1.8520e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 1.6662e-04 - val_loss: 1.6998e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 1.6797e-04 - val_loss: 1.7209e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 1.7025e-04 - val_loss: 2.0405e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 1.7075e-04 - val_loss: 1.8877e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.7025e-04 - val_loss: 1.9792e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 1.6834e-04 - val_loss: 1.8686e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 1.6464e-04 - val_loss: 1.8609e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step\n",
      "Model 8 Test Set RMSE: 0.130518 AU\n",
      "\n",
      "--- Training MLP Model 9 (Seed: 9) ---\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812us/step - loss: 0.0428 - val_loss: 0.0382 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.0176 - val_loss: 0.0275 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.0121 - val_loss: 0.0216 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0087 - val_loss: 0.0199 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.0061 - val_loss: 0.0143 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.0044 - val_loss: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.0036 - val_loss: 0.0103 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.0026 - val_loss: 0.0078 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.0022 - val_loss: 0.0093 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.0020 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.0017 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.0014 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.0013 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 0.0013 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 0.0011 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 0.0012 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 9.6191e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 8.4618e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 9.4274e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 0.0011 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 6.8384e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 7.3224e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 9.5914e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 7.2903e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 7.1192e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 6.5967e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 6.7125e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 5.6647e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 7.1242e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 5.3806e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 6.1576e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 6.0722e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 5.6584e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 5.7587e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 5.7037e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 4.7398e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 5.2182e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 5.6348e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 5.4083e-04 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 5.9672e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 5.2560e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.8233e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 5.4385e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 44/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 4.8665e-04 - val_loss: 6.9777e-04 - learning_rate: 0.0010\n",
      "Epoch 45/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 4.7644e-04 - val_loss: 7.9304e-04 - learning_rate: 0.0010\n",
      "Epoch 46/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 4.5303e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 47/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 4.5348e-04 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 48/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 5.3707e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 49/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 3.7829e-04 - val_loss: 7.4692e-04 - learning_rate: 0.0010\n",
      "Epoch 50/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 4.9466e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 51/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 3.1402e-04 - val_loss: 4.8648e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 3.0978e-04 - val_loss: 4.8527e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 3.1069e-04 - val_loss: 4.4502e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 3.1251e-04 - val_loss: 4.0831e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 3.1767e-04 - val_loss: 4.9604e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 3.2908e-04 - val_loss: 4.0918e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 3.3343e-04 - val_loss: 3.9649e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 2.9451e-04 - val_loss: 5.8165e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 3.1680e-04 - val_loss: 3.3669e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.9148e-04 - val_loss: 3.5593e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 3.0277e-04 - val_loss: 5.4972e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 2.8197e-04 - val_loss: 3.3816e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 3.0724e-04 - val_loss: 3.1472e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.7496e-04 - val_loss: 3.0665e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 65/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 2.9224e-04 - val_loss: 2.8598e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 66/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 2.7165e-04 - val_loss: 3.1026e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 67/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 2.9755e-04 - val_loss: 3.7114e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 68/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 2.7227e-04 - val_loss: 4.2637e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 69/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 2.8420e-04 - val_loss: 5.4160e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 70/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 2.6990e-04 - val_loss: 3.2841e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 71/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 2.6989e-04 - val_loss: 3.9652e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 72/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 2.6693e-04 - val_loss: 4.0568e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 73/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.6710e-04 - val_loss: 2.7920e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 74/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 2.6838e-04 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
      "Epoch 75/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 2.9779e-04 - val_loss: 2.7796e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 76/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 2.9121e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
      "Epoch 77/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.8266e-04 - val_loss: 2.5925e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 78/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.4506e-04 - val_loss: 2.7968e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 79/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.7078e-04 - val_loss: 2.5297e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 80/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 2.5490e-04 - val_loss: 9.6003e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 81/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.7943e-04 - val_loss: 7.3712e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 82/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.6521e-04 - val_loss: 2.8641e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 83/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.6395e-04 - val_loss: 3.3962e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 84/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.7471e-04 - val_loss: 2.8846e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 85/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2.4553e-04 - val_loss: 3.6945e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 86/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 2.3296e-04 - val_loss: 6.6387e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 87/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.4726e-04 - val_loss: 2.3969e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 88/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 2.4655e-04 - val_loss: 5.9328e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 89/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.4852e-04 - val_loss: 3.3150e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 90/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 2.4371e-04 - val_loss: 6.4855e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 91/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 2.5397e-04 - val_loss: 2.8830e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 92/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.5818e-04 - val_loss: 3.1706e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 93/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 2.2847e-04 - val_loss: 2.7281e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 94/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 2.3999e-04 - val_loss: 3.1771e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 95/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.3428e-04 - val_loss: 2.9174e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 96/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 2.4275e-04 - val_loss: 2.4588e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 97/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 2.3735e-04 - val_loss: 4.5365e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 98/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 2.5991e-04 - val_loss: 5.4565e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 99/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 2.5727e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
      "Epoch 100/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 2.3661e-04 - val_loss: 2.6367e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 101/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 1.9752e-04 - val_loss: 2.1933e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 102/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 1.9468e-04 - val_loss: 2.0857e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 103/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.9736e-04 - val_loss: 2.1754e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 104/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 1.9916e-04 - val_loss: 2.3659e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 105/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.0013e-04 - val_loss: 2.7335e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 106/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 1.9972e-04 - val_loss: 2.4758e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 107/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 2.0191e-04 - val_loss: 3.5541e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 108/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 2.0294e-04 - val_loss: 2.3350e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 109/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.9595e-04 - val_loss: 2.0179e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 110/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.9461e-04 - val_loss: 2.0071e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 111/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 1.9298e-04 - val_loss: 2.5066e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 112/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 1.9341e-04 - val_loss: 2.7124e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 113/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9522e-04 - val_loss: 2.1172e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 114/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 1.9648e-04 - val_loss: 2.7776e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 115/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9484e-04 - val_loss: 1.9856e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 116/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 1.9015e-04 - val_loss: 1.9239e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 117/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.9048e-04 - val_loss: 2.0219e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 118/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 1.8895e-04 - val_loss: 1.8927e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 119/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 1.9103e-04 - val_loss: 1.8866e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 120/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 1.8427e-04 - val_loss: 1.9567e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 121/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 1.9235e-04 - val_loss: 2.2327e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 122/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 1.8325e-04 - val_loss: 2.0606e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 123/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 1.8740e-04 - val_loss: 1.7947e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 124/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 1.8077e-04 - val_loss: 1.9976e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 125/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 1.8360e-04 - val_loss: 4.0120e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 126/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 1.8643e-04 - val_loss: 1.9656e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 127/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 1.8360e-04 - val_loss: 4.8416e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 128/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 1.8833e-04 - val_loss: 1.9066e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 129/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 1.8138e-04 - val_loss: 2.4234e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 130/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 1.8408e-04 - val_loss: 2.5000e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 131/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 1.8253e-04 - val_loss: 3.8673e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 132/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.7632e-04 - val_loss: 1.8440e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 133/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 1.8177e-04 - val_loss: 1.8703e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 134/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 1.7915e-04 - val_loss: 1.8756e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 135/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 1.7691e-04 - val_loss: 1.9698e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 136/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 1.8435e-04 - val_loss: 2.1973e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 137/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 1.8110e-04 - val_loss: 1.9076e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 138/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 1.7535e-04 - val_loss: 1.8189e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 139/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 1.7397e-04 - val_loss: 2.1538e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 140/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 1.8164e-04 - val_loss: 1.7776e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 141/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 1.7271e-04 - val_loss: 1.8284e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 142/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 1.8738e-04 - val_loss: 2.3156e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 143/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 1.7016e-04 - val_loss: 1.8853e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 144/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 1.7347e-04 - val_loss: 3.3119e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 145/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 1.7313e-04 - val_loss: 1.9469e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 146/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 1.7030e-04 - val_loss: 2.1229e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 147/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 1.7663e-04 - val_loss: 2.0995e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 148/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 1.6671e-04 - val_loss: 1.6842e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 149/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 1.6929e-04 - val_loss: 2.9857e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 150/5000\n",
      "\u001b[1m503/503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 1.7855e-04 - val_loss: 2.0609e-04 - learning_rate: 2.5000e-04\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step\n",
      "Model 9 Test Set RMSE: 0.125965 AU\n",
      "\n",
      "--- Final Ensemble Evaluation ---\n",
      "Ensemble X-coordinate RMSE: 0.088973 AU\n",
      "Ensemble Y-coordinate RMSE: 0.095098 AU\n",
      "Ensemble Z-coordinate RMSE: 0.078441 AU\n",
      "\n",
      "--- Final Comparison ---\n",
      "Best Single MLP Overall RMSE: 0.003100 AU\n",
      "Multi-MLP Ensemble Overall RMSE: 0.078441 AU\n",
      "\n",
      "--- Best Individual Model Summary ---\n",
      "Best Individual Model (Index 1) achieved RMSE: 0.003779 AU\n",
      "The random seed used for this best model was: 1\n",
      "\n",
      "CONCLUSION: The original single MLP model remains the most accurate.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CRITICAL FIX REMOVED: tf.config.run_functions_eagerly(True) has been removed.\n",
    "# This should revert to fast Graph Execution mode.\n",
    "\n",
    "# --- ASSUME YOUR DATA IS ALREADY LOADED AND PREPROCESSED ---\n",
    "# X_train_scaled, y_train, X_test_scaled, y_test\n",
    "# (Assuming your data variables are correctly loaded in the environment)\n",
    "\n",
    "# --- NEW: OPTIMIZE DATA LOADING FOR PERFORMANCE (FASTEST PATH) ---\n",
    "# We define raw slices once and optimize the pipeline per model inside the loop.\n",
    "BATCH_SIZE = 32 # Standard batch size\n",
    "AUTOTUNE = tf.data.AUTOTUNE \n",
    "\n",
    "# 1. Convert NumPy arrays to raw, unshuffled tf.data.Dataset slices (defined once)\n",
    "raw_train_dataset = Dataset.from_tensor_slices((X_train_scaled, y_train.values.astype(np.float32)))\n",
    "raw_test_dataset = Dataset.from_tensor_slices((X_test_scaled, y_test.values.astype(np.float32)))\n",
    "\n",
    "\n",
    "# --- MODEL CONFIGURATION ---\n",
    "NUM_MODELS = 9 # Assuming you set this to 9 in your environment\n",
    "# Set a high number of epochs since Early Stopping will handle when to stop\n",
    "EPOCHS = 5000   \n",
    "INPUT_SHAPE = X_train_scaled.shape[1] # Number of input features\n",
    "models = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Variables for tracking the best individual model\n",
    "best_individual_rmse = float('inf')\n",
    "best_model_index = -1\n",
    "best_model_seed = -1\n",
    "best_model_predictions = None\n",
    "# Replace the value below with your actual best single MLP RMSE (approx. 0.0031)\n",
    "BEST_SINGLE_MLP_RMSE = 0.003100\n",
    "\n",
    "\n",
    "# --- Define Callbacks for Training ---\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=150,        \n",
    "    restore_best_weights=True,\n",
    "    # REVERTED: Removed min_delta to use the default (0.0), which aligns with the original, best-performing setup.\n",
    ")\n",
    "\n",
    "lr_on_plateau_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5,         \n",
    "    patience=50,        \n",
    "    min_lr=1e-7         \n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, lr_on_plateau_callback]\n",
    "\n",
    "# --- 1. BUILD AND TRAIN THE INDIVIDUAL MLP MODELS ---\n",
    "print(f\"Building and training {NUM_MODELS} diverse MLP models...\")\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    tf.keras.backend.clear_session()\n",
    "    current_seed = i + 1\n",
    "    # Set the seed for model weight initialization\n",
    "    tf.keras.utils.set_random_seed(current_seed)\n",
    "    \n",
    "    # --- CRITICAL FIX: Create unique shuffled datasets for each model ---\n",
    "    # 2. Shuffle, batch, and prefetch for optimal pipeline speed\n",
    "    # We use the current_seed to ensure each model sees the data in a different order.\n",
    "    train_dataset = raw_train_dataset.shuffle(buffer_size=1024, seed=current_seed).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    test_dataset = raw_test_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    \n",
    "    # --- MODEL ARCHITECTURE REVERTED TO ORIGINAL HIGH-PERFORMANCE STRUCTURE ---\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', \n",
    "              kernel_regularizer=l2(0.0001), # Reverted L2 to original 0.0001\n",
    "              input_shape=(INPUT_SHAPE,)), \n",
    "        \n",
    "        Dense(256, activation='relu',\n",
    "              kernel_regularizer=l2(0.0001)), # Reverted L2 to original 0.0001\n",
    "        \n",
    "        Dense(128, activation='relu', \n",
    "              kernel_regularizer=l2(0.0001)), # Reverted L2 to original 0.0001\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "\n",
    "        Dense(3, activation='linear') \n",
    "    ])\n",
    "    # --- END REVERT ---\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"\\n--- Training MLP Model {i+1} (Seed: {current_seed}) ---\")\n",
    "    \n",
    "    # Use the optimized datasets for fastest training\n",
    "    model.fit(\n",
    "        train_dataset, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=test_dataset, \n",
    "        callbacks=callbacks, \n",
    "        verbose=1 \n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    # Generate predictions using the raw NumPy array (this usually runs fine)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    # Calculate and print the individual model's RMSE for comparison\n",
    "    rmse_x = np.sqrt(mean_squared_error(y_test.values[:, 0], y_pred[:, 0]))\n",
    "    rmse_y = np.sqrt(mean_squared_error(y_test.values[:, 1], y_pred[:, 1]))\n",
    "    rmse_z = np.sqrt(mean_squared_error(y_test.values[:, 2], y_pred[:, 2]))\n",
    "    overall_rmse = np.mean([rmse_x, rmse_y, rmse_z])\n",
    "    \n",
    "    print(f\"Model {i+1} Test Set RMSE: {overall_rmse:.6f} AU\")\n",
    "    \n",
    "    if overall_rmse < best_individual_rmse:\n",
    "        best_individual_rmse = overall_rmse\n",
    "        best_model_index = i\n",
    "        best_model_seed = current_seed\n",
    "        best_model_predictions = y_pred.copy()\n",
    "\n",
    "# --- 2. CREATE THE ENSEMBLE PREDICTION ---\n",
    "y_ensemble_pred_au = np.mean(y_pred_list, axis=0)\n",
    "\n",
    "# --- 3. EVALUATE THE ENSEMBLE PERFORMANCE ---\n",
    "print(\"\\n--- Final Ensemble Evaluation ---\")\n",
    "rmse_x_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 0], y_ensemble_pred_au[:, 0]))\n",
    "rmse_y_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 1], y_ensemble_pred_au[:, 1]))\n",
    "rmse_z_ensemble = np.sqrt(mean_squared_error(y_test.values[:, 2], y_ensemble_pred_au[:, 2]))\n",
    "overall_ensemble_rmse = np.mean([rmse_x_ensemble, rmse_y_ensemble, rmse_z_ensemble])\n",
    "\n",
    "print(f\"Ensemble X-coordinate RMSE: {rmse_x_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Y-coordinate RMSE: {rmse_y_ensemble:.6f} AU\")\n",
    "print(f\"Ensemble Z-coordinate RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "# --- 4. Final Comparison and Extraction of Best Model ---\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "print(f\"Best Single MLP Overall RMSE: {BEST_SINGLE_MLP_RMSE:.6f} AU\")\n",
    "print(f\"Multi-MLP Ensemble Overall RMSE: {overall_ensemble_rmse:.6f} AU\")\n",
    "\n",
    "print(f\"\\n--- Best Individual Model Summary ---\")\n",
    "print(f\"Best Individual Model (Index {best_model_index+1}) achieved RMSE: {best_individual_rmse:.6f} AU\")\n",
    "print(f\"The random seed used for this best model was: {best_model_seed}\")\n",
    "\n",
    "# --- 5. SAVING THE BEST MODEL ---\n",
    "if overall_ensemble_rmse < best_individual_rmse and overall_ensemble_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(\"\\nCONCLUSION: The Multi-MLP Ensemble is the most accurate predictor.\")\n",
    "    MODEL_SAVE_NAME = \"final_best_ensemble_predictions.npy\"\n",
    "    np.save(MODEL_SAVE_NAME, y_ensemble_pred_au)\n",
    "    print(f\"Saved FINAL ENSEMBLE PREDICTIONS to: {MODEL_SAVE_NAME}\")\n",
    "    \n",
    "elif best_individual_rmse < overall_ensemble_rmse and best_individual_rmse < BEST_SINGLE_MLP_RMSE:\n",
    "    print(f\"\\nCONCLUSION: A new individual MLP model (Index {best_model_index+1}) is the most accurate predictor.\")\n",
    "    \n",
    "    final_best_model = models[best_model_index]\n",
    "    MODEL_SAVE_NAME = f\"best_mlp_model_seed_{best_model_seed}.h5\" \n",
    "    \n",
    "    # Check if file exists before saving\n",
    "    if os.path.exists(MODEL_SAVE_NAME):\n",
    "        print(f\"Warning: File {MODEL_SAVE_NAME} already exists. Skipping save to prevent overwrite.\")\n",
    "    else:\n",
    "        final_best_model.save(MODEL_SAVE_NAME)\n",
    "        print(f\"Saved FINAL BEST INDIVIDUAL MODEL to: {MODEL_SAVE_NAME}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nCONCLUSION: The original single MLP model remains the most accurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'models'\n",
    "MODELS_LIS = ['mm0','mm1','mm2']\n",
    "\n",
    "models = stars_utils.models_loader(MODEL_DIR,MODELS_LIS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
