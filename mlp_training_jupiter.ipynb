{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Multi-Layer Perceptron (MLP) Ensemble Training Notebook\n",
"\n",
"This notebook handles data generation, feature scaling, model architecture definition, and training for the multi-model ensemble. It is designed to be run sequentially."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# --- 1. IMPORTS AND SETUP ---\n",
"import numpy as np\n",
"import pandas as pd\n",
"import tensorflow as tf\n",
"from tensorflow.keras.models import Sequential\n",
"from tensorflow.keras.layers import Dense\n",
"from tensorflow.keras.regularizers import l2\n",
"from sklearn.model_selection import train_test_split\n",
"from sklearn.preprocessing import StandardScaler\n",
"from sklearn.metrics import mean_squared_error\n",
"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
"from datetime import datetime, timedelta\n",
"import os\n",
"\n",
"# Ensure your stars_utils.py file is in the same directory\n",
"import stars_utils\n",
"\n",
"# Set seed globally for reproducibility\n",
"RANDOM_SEED = 42\n",
"tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
"os.makedirs('models', exist_ok=True)\n",
"print('Setup complete. TensorFlow version: ' + tf.version)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# --- 2. CONFIGURATION ---\n",
"TARGET_PLANET = 'mars'\n",
"NUM_MODELS = 3 \n",
"EPOCHS = 5000   \n",
"TEST_SIZE = 0.2  \n",
"BATCH_SIZE = 32\n",
"\n",
"# Time Range for Data Generation (Training Data)\n",
"START_DATE = datetime(1970, 1, 1)\n",
"END_DATE = datetime(2025, 1, 1)\n",
"TIME_STEP = timedelta(days=7) # Weekly data\n",
"\n",
"# File Path for Scaler Persistence\n",
"MODEL_DIR = 'models'\n",
"SCALER_FILEPATH = os.path.join(MODEL_DIR, 'feature_scaler.pkl')\n",
"\n",
"print('Configuration loaded.')"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# --- 3. DATA GENERATION, SPLIT, AND SCALING ---\n",
"\n",
"print('1. Generating data for ' + TARGET_PLANET.capitalize() + '...')\n",
"\n",
"# Use the utility function to get the raw ephemeris data\n",
"df_raw = stars_utils.generate_planetary_ephemeris_df(\n",
"    target_planet=TARGET_PLANET, \n",
"    start_date=START_DATE, \n",
"    end_date=END_DATE, \n",
"    time_step=TIME_STEP\n",
")\n",
"\n",
"X = df_raw[['Julian_Date']] \n",
"y = df_raw[['RA_deg', 'Dec_deg', 'Distance_AU']] \n",
"\n",
"# Chronological Train-Test Split\n",
"split_index = int(len(df_raw) * (1 - TEST_SIZE))\n",
"X_train, X_test = X[:split_index], X[split_index:]\n",
"y_train, y_test = y[:split_index], y[split_index:]\n",
"\n",
"# Scaling the features (Julian Date)\n",
"scaler = StandardScaler()\n",
"X_train_scaled = scaler.fit_transform(X_train)\n",
"X_test_scaled = scaler.transform(X_test)\n",
"INPUT_SHAPE = X_train_scaled.shape[1] \n",
"\n",
"# Save the fitted scaler for later use in prediction/analysis notebooks\n",
"stars_utils.save_scaler(scaler, SCALER_FILEPATH)\n",
"\n",
"# Create robust, optimized TensorFlow Datasets\n",
"AUTOTUNE = tf.data.AUTOTUNE\n",
"raw_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train.values))\n",
"raw_test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test.values))\n",
"test_dataset = raw_test_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
"\n",
"print('Data split and scaled. Scaler saved.')"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# --- 4. MODEL CONFIGURATION AND CALLBACKS ---\n",
"models = []\n",
"y_pred_list = []\n",
"\n",
"early_stopping_callback = EarlyStopping(\n",
"    monitor='val_loss', \n",
"    patience=150,\n",
"    restore_best_weights=True \n",
")\n",
"\n",
"lr_on_plateau_callback = ReduceLROnPlateau(\n",
"    monitor='val_loss', \n",
"    factor=0.5,\n",
"    patience=50,\n",
"    min_lr=1e-7\n",
")\n",
"\n",
"callbacks = [early_stopping_callback, lr_on_plateau_callback]\n",
"best_ensemble_rmse = float('inf')\n",
"\n",
"print('Callbacks defined.')"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# --- 5. BUILD AND TRAIN THE INDIVIDUAL MLP MODELS ---\n",
"print('\n2. Building and training ' + str(NUM_MODELS) + ' diverse MLP models...')\n",
"\n",
"for i in range(NUM_MODELS):\n",
"    # CRITICAL: Clear session to ensure fresh start for each model\n",
"    tf.keras.backend.clear_session()\n",
"    \n",
"    # Use a different random seed and shuffle seed for diversity\n",
"    current_seed = RANDOM_SEED + i\n",
"    tf.keras.utils.set_random_seed(current_seed)\n",
"\n",
"    # Dataset with unique shuffle for this model\n",
"    train_dataset = raw_train_dataset.shuffle(\n",
"        buffer_size=1024, \n",
"        seed=current_seed  # Unique shuffle for diversity\n",
"    ).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
"\n",
"    # --- Model Architecture ---\n",
"    model = Sequential([\n",
"        Dense(128, activation='relu', kernel_regularizer=l2(0.0001), \n",
"              input_shape=(INPUT_SHAPE,)), \n",
"        \n",
"        Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
"        \n",
"        Dense(128, activation='relu', kernel_regularizer=l2(0.0001)), \n",
"        \n",
"        Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
"\n",
"        Dense(3, activation='linear') # Output for RA, Dec, Distance\n",
"    ])\n",
"    \n",
"    model.compile(optimizer='adam', loss='mean_squared_error')\n",
"    \n",
"    print('\n--- Training MLP Model ' + str(i+1) + ' (Seed: ' + str(current_seed) + ') ---')\n",
"    model.fit(\n",
"        train_dataset, \n",
"        epochs=EPOCHS, \n",
"        validation_data=test_dataset,\n",
"        callbacks=callbacks,\n",
"        verbose=1\n",
"    )\n",
"    \n",
"    models.append(model)\n",
"    \n",
"    # Generate predictions for the current model\n",
"    y_pred = model.predict(X_test_scaled)\n",
"    y_pred_list.append(y_pred)\n",
"    \n",
"    # Calculate and print the individual model's RMSE\n",
"    overall_rmse = np.sqrt(mean_squared_error(y_test.values, y_pred))\n",
"    \n",
"    print('Model ' + str(i+1) + ' Test Set RMSE: ' + '{:.6f}'.format(overall_rmse) + ' AU')\n",
"    \n",
"    # Save the individual model (using the modern .keras format)\n",
"    model_save_name = os.path.join(MODEL_DIR, 'mars_position_predictor_mm' + str(i) + '.keras')\n",
"    model.save(model_save_name)\n",
"    print('Saved model to: ' + model_save_name)\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# --- 6. ENSEMBLE PREDICTION AND EVALUATION ---\n",
"print('\n3. Final Ensemble Evaluation...')\n",
"\n",
"# The ensemble prediction is the simple average of all individual model predictions.\n",
"y_ensemble_pred_au = np.mean(y_pred_list, axis=0)\n",
"\n",
"# Evaluate the ensemble performance\n",
"overall_ensemble_rmse = np.sqrt(mean_squared_error(y_test.values, y_ensemble_pred_au))\n",
"\n",
"print('Overall Ensemble RMSE: ' + '{:.6f}'.format(overall_ensemble_rmse) + ' AU')\n",
"\n",
"print('\nTraining and evaluation complete. Models and scaler saved to the 'models' directory.')"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.10.12"
}
},
"nbformat": 4,
"nbformat_minor": 5
}
